[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub tutorials",
    "section": "",
    "text": "1 Welcome\nWelcome to my Quarto book! This is the introduction.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>My Quarto Book</span>"
    ]
  },
  {
    "objectID": "chapters/00_intro.html",
    "href": "chapters/00_intro.html",
    "title": "2  A world without skrub",
    "section": "",
    "text": "2.1 Strategizing\nLet’s begin the lesson by imagining a world without skrub, where we can use only Pandas and scikit-learn to clean data and prepare a machine learning model.\nLet’s take a look at the target::\nThis is a numerical column, and our task is predicting the value of current_annual_salary.\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000\nWe need to:\nOnce we have processed the data, we can train a machine learning model. For the sake of the example, we will use a linear model (Ridge), which means that we need to scale numerical features, besides imputing missing values.\nFinally, we want to evaluate the performance of the method across multiple cross-validation splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "chapters/00_intro.html#strategizing",
    "href": "chapters/00_intro.html#strategizing",
    "title": "2  A world without skrub",
    "section": "",
    "text": "Impute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "chapters/00_intro.html#building-a-traditional-pipeline",
    "href": "chapters/00_intro.html#building-a-traditional-pipeline",
    "title": "2  A world without skrub",
    "section": "2.2 Building a traditional pipeline",
    "text": "2.2 Building a traditional pipeline\nLet’s build a traditional predictive pipeline following the steps we just discussed.\n\n2.2.1 Step 1: Convert date features to numerical\nExtract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['years_since_hired'] = (pd.Timestamp.now() - X_processed['date_first_hired']).dt.days / 365.25\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 10)\n\n\n\n\n2.2.2 Step 2: Encode categorical features\nEncode only the non-numerical categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1219)\n\n\n\n\n2.2.3 Step 3: Impute missing values\nWe’ll impute missing values in the gender column using the most frequent strategy.\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)\n\n\n\n2.2.4 Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)\n\n\n\n2.2.5 Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10366.9520 (+/- 1403.5225)\n\n\n\n\n2.2.6 “Just ask an agent to write the code”\nIt’s what I did. Here are some of the issues I noticed:\n\nOperations in the wrong order.\nTrying to impute categorical features without encoding them as numerical values.\nThe datetime feature was encoded as a categorical (i.e, with dummmies).\nToo many print statements.\nCells could not be executed in order without proper debugging and re-prompting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "chapters/00_intro.html#waking-up-from-a-nightmare",
    "href": "chapters/00_intro.html#waking-up-from-a-nightmare",
    "title": "2  A world without skrub",
    "section": "2.3 Waking up from a nightmare",
    "text": "2.3 Waking up from a nightmare\nThankfully, we live in a world where we can import skrub. Let’s see what we can get if we use skrub.tabular_pipeline.\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.9085 (+/- 0.0153)\nMean test RMSE: 8795.1710 (+/- 1003.4363)\n\n\nAll the code from before, the tokens and the debugging are replaced by a single import that gives better results.\nThroughout the tutorial, we will see how each step can be simplified, replaced, or improved using skrub features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html",
    "href": "chapters/01_exploring_data.html",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "3.1 Exploring data with Pandas tools\nIn this notebook, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table.\nFirst, let’s import the necessary libraries and load the dataset.\nNow that we have a dataframe we can work with, we need to do some exploratory analysis to get an idea of the characteristics of the data. Among others, we would like to find out:\nFor the sake of the example, let’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   age             48842 non-null  int64   \n 1   workclass       46043 non-null  category\n 2   fnlwgt          48842 non-null  int64   \n 3   education       48842 non-null  category\n 4   education-num   48842 non-null  int64   \n 5   marital-status  48842 non-null  category\n 6   occupation      46033 non-null  category\n 7   relationship    48842 non-null  category\n 8   race            48842 non-null  category\n 9   sex             48842 non-null  category\n 10  capital-gain    48842 non-null  int64   \n 11  capital-loss    48842 non-null  int64   \n 12  hours-per-week  48842 non-null  int64   \n 13  native-country  47985 non-null  category\ndtypes: category(8), int64(6)\nmemory usage: 2.6 MB\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "href": "chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.2 Exploring data with the TableReport",
    "text": "3.2 Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nTableReport(data)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n3.2.1 Default view of the TableReport\nThe TableReport gives us a comprehensive overview of the dataset. The default view shows all the columns in the dataset, and allows to select and copy the content of the cells shown in the preview.\nThe TableReport is intended to show a preview of the data, so it does not contain all the rows in the dataset, rather it shows only the first and last few rows by default.\n\n\n3.2.2 The “Stats” tab\nThe “Stats” tab provides a variety of descriptive statistics for each column in the dataset. This includes: - The column name - The detected data type of the column - Whether the column is sorted or not - The number of null values in the column, as well as the percentage - The number of unique values in the column\nFor numerical columns, additional statistics are provided: - Mean - Standard deviation - Minimum and maximum values - Median\n\n\n3.2.3 The “Distributions” tab\nThe “Distributions” tab provides visualizations of the distributions of values in each column. This includes histograms for numerical columns and bar plots for categorical columns.\nThe “Distributions” tab helps with detecting potential issues in the data, such as: - Skewed distributions - Outliers - Unexpected value frequencies\nFor example, in this dataset we can see that some columns are heavily skewed, such as “workclass”, “race”, and “native-country”: this is important information to keep track of, because these columns may require special handling during data preprocessing or modeling.\nAdditionally, the “Distributions” tab allows so select columns manually, so that they can be added to a script and selected for further analysis or modeling.\n\n\n3.2.4 The “Associations” tab\nThe “Associations” tab provides insights into the relationships between different columns in the dataset. It shows Pearson’s correlation coefficients for numerical columns, as well as Cramér’s V for all columns.\nWhile this is a somewhat rough measure of association, it can help identify potential relationships worth exploring further during the analysis, and highlights highly correlated columns: depending on the modeling technique used, these may need to be handled specially to avoid issues with multicollinearity.\nIn this example, we can see that “education-num” and “education” have perfect correlation, which means that one of the two columns can be dropped without losing information.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html#exploring-the-target-variable",
    "href": "chapters/01_exploring_data.html#exploring-the-target-variable",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.3 Exploring the target variable",
    "text": "3.3 Exploring the target variable\nLet’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "href": "chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.4 Configuring and saving the TableReport",
    "text": "3.4 Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console.\nIt is possible to configure various parameters using the skrub global config. For example, it is possible to replace the default Pandas or Polars dataframe display with the TableReport as follows:\n\nfrom skrub import set_config\nset_config(use_table_report=True)\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html#working-with-big-tables",
    "href": "chapters/01_exploring_data.html#working-with-big-tables",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.5 Working with big tables",
    "text": "3.5 Working with big tables\nPlotting and measuring the column correlations are expensive operations, so when the dataframe under study is large it may be more convenient to skip them, as generating the Distributions and Associations tab may take a long time.\nThe max_plot_columns and max_association_columns parameters allow to set a threshold on the number of columns: the TableReport will skip the respective task if the number of colums in the dataframe is larger than the threshold:\n\nTableReport(data, max_association_columns=3, max_plot_columns=3)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhen the number of columns is too large, an error message is shown in the respective tab instead of the plots or correlations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/01_exploring_data.html#exercise-exploring-a-new-table",
    "href": "chapters/01_exploring_data.html#exercise-exploring-a-new-table",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.6 Exercise: exploring a new table",
    "text": "3.6 Exercise: exploring a new table\nFor this exercise, we will use the employee_salaries dataframe to answer some questions.\nRun the following code to import the dataframe:\n\nfrom skrub.datasets import fetch_employee_salaries\n\nemployee_salaries = fetch_employee_salaries()\ndata = employee_salaries.X\n\nNow use the skrub TableReport and answer the following questions:\n\nQuestionsSolution\n\n\n\nWhat’s the size of the dataframe? (columns and rows)\nHow many columns have object/numerical/datetime\nAre there columns with a large number of missing values?\nAre there columns that have a high cardinality (&gt;40 unique values)?\nWere datetime columns parsed correctly?\nWhich columns have outliers?\nWhich columns have an imbalanced distribution?\nWhich columns are strongly correlated with each other?\n\n\n\n\nWhat’s the size of the dataframe? (columns and rows)\n\n9228 rows × 8 columns\n\nHow many columns have object/numerical/datetime\n\nNo datetime columns, one integer column (year_first_hired), all other columns are objects.\n\nAre there columns with a large number of missing values?\n\nNo, only the gender column contains a small fraction (0.2%) of missing values.\n\nAre there columns that have a high cardinality?\n\nYes, division, employee_position_title, date_first_hired have a cardinality larger than 40.\n\nWere datetime columns parsed correctly?\n\nNo, the date_first_hired column has dtype Object.\n\nWhich columns have outliers?\n\nNo columns seem to include outliers.\n\nWhich columns have an imbalanced distribution?\n\nassignment_category has an unbalanced distribution.\n\nWhich columns are strongly correlated with each other?\n\ndepartment and department_name have a Cramer’s V of 1, so they are very strongly correlated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/02_cleaning_data.html",
    "href": "chapters/02_cleaning_data.html",
    "title": "4  Preprocessing data with skrub",
    "section": "",
    "text": "4.1 Cleaning data with Pandas\nIn this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner, and compare it to traditional methods using pandas.\nfrom sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=42074)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "href": "chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "title": "4  Preprocessing data with skrub",
    "section": "4.2 Using the skrub Cleaner",
    "text": "4.2 Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column: 1. It replaces common strings used to represent missing values (e.g., NULL, ?) with NA markers. 2. It uses the DropUninformative transformer to decide whether a column is “uninformative”, that is, it is not likely to bring information useful to train a ML model. For example, empty columns are uninformative. 3. It tries to parse datetime columns using common formats, or a user-provided datetime_format. 4. It processes categorical columns to ensure consistent typing depending on the dataframe library in use. 5. It converts columns to string, unless they have a data type that carries more information, such as numerical, datetime, and categorial columns. 6. Finally, it can convert numerical columns to np.float32 dtype. This ensures a consistent representation of numbers and missing values, and helps reducing the memory footprint. This is useful if the Cleaner is used as the first step in a machine learning pipeline.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "href": "chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "title": "4  Preprocessing data with skrub",
    "section": "4.3 Under the hood: DropUninformative",
    "text": "4.3 Under the hood: DropUninformative\nWhen the cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that could be dropped as they do not bring useful information for training a ML model.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction. By default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nIt contains only one value, and no missing values. This is controlled by the drop_if_constant flag, which is False by default..\nAll values in the column are distinct. This may be the case if the column contains UIDs, but it can also happen when the column contains text. This check is off by default and can be turned on by setting drop_if_unique to True.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/02_cleaning_data.html#exercise",
    "href": "chapters/02_cleaning_data.html#exercise",
    "title": "4  Preprocessing data with skrub",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\nGiven the following dataframe, use skrub’s Cleaner to clean the data so that: - Constant columns are removed - All columns with more than 50% missing values are removed\n\nimport pandas as pd\ndf = pd.read_csv(\"../data/synthetic_data.csv\")\n\nLet’s first examine the dataset before cleaning:\n\nfrom skrub import TableReport\nTableReport(df)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nNow, let’s use the Cleaner to clean the data:\n\nfrom skrub import Cleaner\n\n# Configure the Cleaner to:\n# - Remove constant columns (drop_if_constant=True)\n# - Remove columns with more than 50% missing values (drop_null_fraction=0.5)\ncleaner = Cleaner(drop_if_constant=True, drop_null_fraction=0.5)\n\n# Apply the cleaning\ndf_cleaned = cleaner.fit_transform(df)\n\n# Display the cleaned dataframe\nTableReport(df_cleaned)\n\nProcessing column   1 / 10Processing column   2 / 10Processing column   3 / 10Processing column   4 / 10Processing column   5 / 10Processing column   6 / 10Processing column   7 / 10Processing column   8 / 10Processing column   9 / 10Processing column  10 / 10\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWe can inspect which columns were dropped and what transformations were applied:\n\nprint(f\"Original shape: {df.shape}\")\nprint(f\"Cleaned shape: {df_cleaned.shape}\")\nprint(\n    f\"\\nColumns dropped: {[col for col in df.columns if col not in cleaner.all_outputs_]}\"\n)\n\nOriginal shape: (10000, 14)\nCleaned shape: (10000, 10)\n\nColumns dropped: ['with_nulls_1', 'with_nulls_2', 'constant_1', 'constant_2']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html",
    "href": "chapters/03_feat_eng_apply.html",
    "title": "5  Applying transformers to columns",
    "section": "",
    "text": "5.1 Introduction\nOften, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception.\nScikit-learn provides the ColumnTransformer to deal with this:\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "href": "chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "title": "5  Applying transformers to columns",
    "section": "5.2 ApplyToCols and ApplyToFrame",
    "text": "5.2 ApplyToCols and ApplyToFrame\n\n5.2.1 Applying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n5.2.2 Applying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, it may be beneficial to apply the same transformer to a subset of columns in a dataframe.\nThis example dataframe contains some patient information, and some (random) metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "5  Applying transformers to columns",
    "section": "5.3 Selection operations in a scikit-learn pipeline",
    "text": "5.3 Selection operations in a scikit-learn pipeline\nIn some situations, it may be necessary to select or remove specific columns from a dataframe: unlike ApplyToCols and ApplyToFrame, this means removing some features from the original table. This can be done with SelectCols and DropCols, which work as their name suggests, and can take a cols parameter to choose which columns to select or drop respectively.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#exercise-putting-everything-together-in-a-scikit-learn-pipeline",
    "href": "chapters/03_feat_eng_apply.html#exercise-putting-everything-together-in-a-scikit-learn-pipeline",
    "title": "5  Applying transformers to columns",
    "section": "5.4 Exercise: putting everything together in a scikit-learn pipeline",
    "text": "5.4 Exercise: putting everything together in a scikit-learn pipeline",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "chapters/04_selectors.html",
    "href": "chapters/04_selectors.html",
    "title": "6  Choose your column: selectors",
    "section": "",
    "text": "6.1 Skrub selectors\nVery often, column selection is more complex than simply passing a list of column names to a transformer: it may be necessary to select all columns that have a specific data type, or based on some other characteristic (presence of nulls, column cardinality etc.).\nThe skrub selectors implement a number of selection strategies that can be combined in various ways to build complex filtering conditions that can then be employed by ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors are available from the skrub.selectors namespace:\nimport skrub.selectors as s\nSelectors allow to filter columns by data type: - .float: floating-point columns - .integer: integer columns - .any_date: date or datetime columns - .boolean: boolean columns - .string: columns with a String data type - .categorical: columns with a Categorical data type - .numeric: numeric (either integer or float) columns\nAdditional conditions include: - .all: select all columns - .cardinality_below: select all columns with a number of unique values lower than the given threshold - .has_nulls: select all columns that include at least one null value\nVarious selectors allow to choose columns based on their name: - .cols: choose the provided column name (or list of names) - .glob: use Unix shell style glob to select column names - .regex: select columns using regular expressions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "chapters/04_selectors.html#combining-selectors",
    "href": "chapters/04_selectors.html#combining-selectors",
    "title": "6  Choose your column: selectors",
    "section": "6.2 Combining selectors",
    "text": "6.2 Combining selectors\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "chapters/04_selectors.html#designing-custom-filters",
    "href": "chapters/04_selectors.html#designing-custom-filters",
    "title": "6  Choose your column: selectors",
    "section": "6.3 Designing custom filters",
    "text": "6.3 Designing custom filters\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function is True. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "chapters/04_selectors.html#exercise-using-selectors-together-with-applytocols",
    "href": "chapters/04_selectors.html#exercise-using-selectors-together-with-applytocols",
    "title": "6  Choose your column: selectors",
    "section": "6.4 Exercise: using selectors together with ApplyToCols",
    "text": "6.4 Exercise: using selectors together with ApplyToCols\n\ndefine dataframe\n\nsome numeric metrics\na numerical id column\na string id column\nsome text column\n\napply standardscaler to specific numerical columns, onehotencoder to specific (not all) string columns\ndrop the id columns",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html",
    "href": "chapters/05_feat_eng_numerical.html",
    "title": "7  Scaling numerical features safely",
    "section": "",
    "text": "Numerical features with outliers\nRegular scalers\nSquashingScaler",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html",
    "href": "chapters/06_feat_eng_datetimes.html",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "With pandas/polars\nSpecifying the datetime format\nDatetimeEncoder default\nDatetimeEncoder additional parameters\nPeriodic encoders",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html",
    "href": "chapters/07_feat_eng_categorical.html",
    "title": "9  Mixed data: dealing with categories",
    "section": "",
    "text": "Low cardinality: OneHotEncoder and OrdinalEncoder\nHigh cardinality default: StringEncoder\nTextEncoder\nOther encoders: GapEncoder, MinHashEncoder",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "chapters/08_feat_eng_table_vect.html",
    "href": "chapters/08_feat_eng_table_vect.html",
    "title": "10  All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "10.1 Exercise\nReplicate the behavior of a TableVectorizer using ApplyToCols, the skrub selectors, and the given transformers.\nfrom skrub import Cleaner, ApplyToCols, StringEncoder, DatetimeEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\nNotes on the implementation: - In the first step, the TableVectorizer cleans the data to parse datetimes and other dtypes. - Numeric features are left untouched, i.e., they use a Passthrough transformer. - String and categorical feature are split into high and low cardinality features. - For this exercise, set the the cardinality threshold to 4. - High cardinality features are transformed with a StringEncoder. In this exercise, set n_components to 2. - Low cardinality features are transformed with a OneHotEncoder, and the first category in binary features is dropped (hint: check the docs of the OneHotEncoder for the drop parameter). Set sparse_output=True. - Remember cardinality_below is one of the skrub selectors. - Datetimes are transformed by a default DatetimeEncoder. - Everything should be wrapped in a scikit-learn Pipeline.\nUse the following dataframe to test the result.\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", \"private\", \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"datetime-col\": [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n    ]\n    + [None],\n}\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\n2020-02-03T12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\n2021-03-15T00:37:15\n\n\n2\n63\n6.20\nprivate\nlawyer\nTrue\n2022-02-13T17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\n2023-05-22T08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nNone\nUse the following PassThrough transformer where needed.\nfrom skrub._apply_to_cols import SingleColumnTransformer\nclass PassThrough(SingleColumnTransformer):\n    def fit_transform(self, column, y=None):\n        return column\n\n    def transform(self, column):\n        return column\n# Write your code here\n#\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n#\ncleaner = ApplyToCols(Cleaner())\nhigh_cardinality = ApplyToCols(\n    StringEncoder(n_components=2), cols=~s.cardinality_below(4) & (s.string())\n)\nlow_cardinality = ApplyToCols(\n    OneHotEncoder(sparse_output=False, drop=\"if_binary\"),\n    cols=s.cardinality_below(4) & s.string(),\n)\nnumeric = ApplyToCols(PassThrough(), cols=s.numeric())\ndatetime = ApplyToCols(DatetimeEncoder(), cols=s.any_date())\n\nmy_table_vectorizer = make_pipeline(\n    cleaner, numeric, high_cardinality, low_cardinality, datetime\n)\n\nmy_table_vectorizer.fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1_public\nstr2_0\nstr2_1\nbool_True\ndatetime-col_year\ndatetime-col_month\ndatetime-col_day\ndatetime-col_hour\ndatetime-col_total_seconds\n\n\n\n\n0\n15\n5.20\n1.0\n0.820965\n-0.926898\n1.0\n2020.0\n2.0\n3.0\n12.0\n1.580733e+09\n\n\n1\n56\n2.40\n0.0\n0.820965\n-0.926894\n0.0\n2021.0\n3.0\n15.0\n0.0\n1.615769e+09\n\n\n2\n63\n6.20\n0.0\n0.862896\n-0.936518\n1.0\n2022.0\n2.0\n13.0\n17.0\n1.644772e+09\n\n\n3\n12\n10.45\n0.0\n1.029686\n1.353007\n0.0\n2023.0\n5.0\n22.0\n8.0\n1.684745e+09\n\n\n4\n44\n9.00\n1.0\n1.419117\n0.660160\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(\n    high_cardinality=StringEncoder(n_components=2), cardinality_threshold=4\n)\ntv.fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1_public\nstr2_0\nstr2_1\nbool\ndatetime-col_year\ndatetime-col_month\ndatetime-col_day\ndatetime-col_hour\ndatetime-col_total_seconds\n\n\n\n\n0\n15.0\n5.20\n1.0\n0.820966\n-0.926898\n1.0\n2020.0\n2.0\n3.0\n12.0\n1.580733e+09\n\n\n1\n56.0\n2.40\n0.0\n0.820972\n-0.926881\n0.0\n2021.0\n3.0\n15.0\n0.0\n1.615769e+09\n\n\n2\n63.0\n6.20\n0.0\n0.862892\n-0.936527\n1.0\n2022.0\n2.0\n13.0\n17.0\n1.644772e+09\n\n\n3\n12.0\n10.45\n0.0\n1.029680\n1.353003\n0.0\n2023.0\n5.0\n22.0\n8.0\n1.684745e+09\n\n\n4\n44.0\n9.00\n1.0\n1.419120\n0.660169\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html",
    "href": "chapters/09_tabular_pipeline.html",
    "title": "11  Building a tabular pipeline",
    "section": "",
    "text": "pipeline with TableVectorizer\npipeline with tabular_pipeline",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  }
]