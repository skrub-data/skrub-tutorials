[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inria Academy - Skrub like a pro",
    "section": "",
    "text": "1 Introduction to the course\nThis is the website for the Inria Academy course on the skrub package: it contains all the material used for the course, including the datasets and exercises used during the session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "Inria Academy - Skrub like a pro",
    "section": "1.1 Structure of the course",
    "text": "1.1 Structure of the course\nThe course covers the main features of skrub, from data exploration to pipeline construction. Note that the skrub Data Ops are not covered in this course.\nEach chapter includes a section that describes how a specific feature may assist in building a machine learning pipeline, along with practical code examples.\nSome chapters include exercises for the participants to work on with the assistance of the instructure. Exercises can be run in the provided Jupyter notebooks.\nThe content of the book is split in sections, and each section includes a “final quiz” that covers the subjects covered up to that point.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "index.html#setting-up-a-local-environment",
    "href": "index.html#setting-up-a-local-environment",
    "title": "Inria Academy - Skrub like a pro",
    "section": "2.1 Setting up a local environment",
    "text": "2.1 Setting up a local environment\n\n\n\n\n\n\nImportantNavigating the repository\n\n\n\nDepending on how you launch the instance of Jupyter lab, you might start it in the root folder.\nAll notebooks used in the course are found in content/notebooks.\n\n\n\n2.1.1 Using pixi\nThe easiest way to set up the environment is by installing and using pixi. Follow the platform-specific instructions in the link to install pixi, then open a terminal window.\nRun\npixi install\nto create the environment, followed by\npixi run lab\nto start a Jupyter lab instance.\n\n\n2.1.2 Using conda\nAn environment.yaml file is provided to create a conda environment.\nCreate and activate the environment with\nconda env create -f environment.yaml\nconda activate skrub-tutorial\nThen, start a jupyter lab instance:\njupyter lab\n\n\n2.1.3 Using uv\nCreate the environment using pyproject.toml as the requirement file.\nuv venv \nuv pip install -r pyproject.toml\nActivate the environment that was created in the folder.\nsource .venv/bin/activate\nStart the Jupyter lab instance:\njupyter lab\n\n\n2.1.4 Using pip\nCreate the and activate the environment:\npython -m venv skrub-tutorial\nsource skrub-tutorial/bin/activate\nInstall the required dependencies using the requirements.txt file:\npip install -r requirements.txt\nStart the Jupyter lab instance:\njupyter lab",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html",
    "href": "content/chapters/00_intro.html",
    "title": "2  A world without skrub",
    "section": "",
    "text": "2.1 Strategizing\nLet’s begin the lesson by imagining a world without skrub, where we can use only Pandas and scikit-learn to clean data and prepare a machine learning model.\nLet’s take a look at the target::\nThis is a numerical column, and our task is predicting the value of current_annual_salary.\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000\nWe need to:\nOnce we have processed the data, we can train a machine learning model. For the sake of the example, we will use a linear model (Ridge), which means that we need to scale numerical features and impute missing values.\nFinally, we want to evaluate the performance of the method across multiple cross-validation splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#strategizing",
    "href": "content/chapters/00_intro.html#strategizing",
    "title": "2  A world without skrub",
    "section": "",
    "text": "Impute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "href": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "title": "2  A world without skrub",
    "section": "2.2 Building a traditional pipeline",
    "text": "2.2 Building a traditional pipeline\nLet’s build a traditional predictive pipeline following the steps we just discussed.\n\n2.2.1 Step 1: Convert date features to numerical\nExtract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 9)\n\n\n\n\n2.2.2 Step 2: Encode categorical features\nEncode only the non-numerical categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1218)\n\n\n\n\n2.2.3 Step 3: Impute missing values\nWe’ll impute missing values in the gender column using the most frequent strategy.\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)\n\n\n\n2.2.4 Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)\n\n\n\n2.2.5 Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(\n    ridge,\n    X_scaled,\n    y,\n    cv=5,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n)\n\n# Convert MSE to RMSE\ntest_rmse = np.sqrt(-cv_results[\"test_neg_mean_squared_error\"])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(\n    f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\"\n)\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10367.1206 (+/- 1403.4322)\n\n\n\n\n2.2.6 “Just ask an agent to write the code”\nIt’s what I did. Here are some of the issues I noticed:\n\nOperations in the wrong order.\nTrying to impute categorical features without encoding them as numerical values.\nThe datetime feature was encoded as a categorical (i.e, with dummmies).\nCells could not be executed in order without proper debugging and re-prompting.\npd.get_dummies was executed on the full dataframe, rather than only on the training split, leading to data leakage.\n\nThis means that I had to spend time re-prompting the model to get it to run, and that’s (intentionally) without removing the leakage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "href": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "title": "2  A world without skrub",
    "section": "2.3 Waking up from a nightmare",
    "text": "2.3 Waking up from a nightmare\nThankfully, we live in a world where we can import skrub. Let’s see what we can get if we use skrub.tabular_pipeline.\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\n/Users/rcap/work/skrub-tutorials/.pixi/envs/dev/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/dev/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/dev/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/dev/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/dev/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nCross-Validation Results:\nMean test R²: 0.9083 (+/- 0.0162)\nMean test RMSE: 8804.3443 (+/- 1054.0937)\n\n\nAll the code from before, the tokens and the debugging are replaced by a single import that gives better results.\nThroughout the tutorial, we will see how each step can be simplified, replaced, or improved using skrub features, going through the various features until we get to the tabular_pipeline.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#roadmap-for-the-course",
    "href": "content/chapters/00_intro.html#roadmap-for-the-course",
    "title": "2  A world without skrub",
    "section": "2.4 Roadmap for the course",
    "text": "2.4 Roadmap for the course\nWe are going to build what could be a typicial pre-processing pipeline:\n\nWe will explore the data to identify possible problems and figure out what needs to be cleaned.\nWe will then sanitize the data to address some common problems.\nThere will be an intermission on various skrub features that simplify.\nThen, we will show how to perform feature engineering using various skrub encoders.\nFinally, we will show how we can put everything together.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html",
    "href": "content/chapters/01_exploring_data.html",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "3.1 Introduction\nIn this chapter, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table, and perform some exploratory analysis to learn about the characteristics of the data.\nFirst, let’s import the necessary libraries and load the dataset.\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\n# Load the Adult Census dataset\ndata =  pd.read_csv(\"../data/adult_census/data.csv\")\ntarget =  pd.read_csv(\"../data/adult_census/target.csv\")\nNow that we have a dataframe we can work with, we would like to find out:",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#introduction",
    "href": "content/chapters/01_exploring_data.html#introduction",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "The size of the dataset.\nThe data types and names of the columns.\nThe distribution of values in the columns.\nWhether null values are present, in what measure and where.\nDiscrete/categorical features, and their cardinality.\nColumns strongly correlated with each other.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "href": "content/chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.2 Exploring data with Pandas tools",
    "text": "3.2 Exploring data with Pandas tools\nFor the sake of the example, let’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\n\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n\n\n\n\n\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   age             48842 non-null  int64 \n 1   workclass       46043 non-null  object\n 2   fnlwgt          48842 non-null  int64 \n 3   education       48842 non-null  object\n 4   education-num   48842 non-null  int64 \n 5   marital-status  48842 non-null  object\n 6   occupation      46033 non-null  object\n 7   relationship    48842 non-null  object\n 8   race            48842 non-null  object\n 9   sex             48842 non-null  object\n 10  capital-gain    48842 non-null  int64 \n 11  capital-loss    48842 non-null  int64 \n 12  hours-per-week  48842 non-null  int64 \n 13  native-country  47985 non-null  object\ndtypes: int64(6), object(8)\nmemory usage: 5.2+ MB\n\n\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\n\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\n\n\n\n\n\n\n\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.3 Exploring data with the TableReport",
    "text": "3.3 Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nfrom skrub import TableReport\nTableReport(data)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n3.3.1 Default view of the TableReport\nThe TableReport gives us a comprehensive overview of the dataset. The default view shows all the columns in the dataset, and allows to select and copy the content of the cells shown in the preview.\nThe TableReport is intended to show a preview of the data, so it does not contain all the rows in the dataset, rather it shows only the first and last few rows by default.\n\n\n3.3.2 The “Stats” tab\n\nTableReport(data, open_tab=\"stats\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Stats” tab provides a variety of descriptive statistics for each column in the dataset. This includes:\n\nThe column name\nThe detected data type of the column\nWhether the column is sorted or not\nThe number of null values in the column, as well as the percentage\nThe number of unique values in the column\n\nFor numerical columns, additional statistics are provided:\n\nMean\nStandard deviation\nMinimum and maximum values\nMedian\n\n\n\n3.3.3 The “Distributions” tab\n\nTableReport(data, open_tab=\"distributions\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Distributions” tab provides visualizations of the distributions of values in each column. This includes histograms for numerical columns and bar plots for categorical columns.\nThe “Distributions” tab helps with detecting potential issues in the data, such as:\n\nSkewed distributions\nOutliers\nUnexpected value frequencies\n\nFor example, in this dataset we can see that some columns are heavily skewed, such as “workclass”, “race”, and “native-country”: this is important information to keep track of, because these columns may require special handling during data preprocessing or modeling.\nAdditionally, the “Distributions” tab allows so select columns manually, so that they can be added to a script and selected for further analysis or modeling.\n\n\n\n\n\n\nCaution\n\n\n\nThe TableReport detects outliers using a simple interquartile test, marking as outliers all values that are beyond the IQR. This is a simple heuristic, and should not be treated as perfect. If your problem requires reliable outlier detection, you should not rely exclusively on what the TableReport shows.\n\n\n\n\n3.3.4 The “Associations” tab\n\nTableReport(data, open_tab=\"associations\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Associations” tab provides insights into the relationships between different columns in the dataset. It shows Pearson’s correlation coefficient for numerical columns, as well as Cramér’s V for all columns.\nWhile this is a somewhat rough measure of association, it can help identify potential relationships worth exploring further during the analysis, and highlights highly correlated columns: depending on the modeling technique used, these may need to be handled specially to avoid issues with multicollinearity.\nIn this example, we can see that “education-num” and “education” have perfect correlation, which means that one of the two columns can be dropped without losing information.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "href": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.4 Exploring the target variable",
    "text": "3.4 Exploring the target variable\nLet’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nBesides handling dataframes, the TableReport handles series and bidimensional numpy arrays.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.5 Configuring and saving the TableReport",
    "text": "3.5 Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console.\nIt is possible to configure various parameters using the skrub global config. For example, it is possible to replace the default Pandas or Polars dataframe display with the TableReport by using patch_display (and unpatch_display):\n\nfrom skrub import patch_display, unpatch_display\n\n# replace the default pandas repr \npatch_display()\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nTo disable, use unpatch_display:\n\nunpatch_display()\ndata\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n\n\n\n\n48842 rows × 14 columns\n\n\n\nThis can also be done using the skrub global configuration as follows:\n\nfrom skrub import set_config\n\n# replace the default pandas repr \nset_config(use_table_report=True)\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nTo disable, then switch off the same flag.\n\nset_config(use_table_report=False)\ndata\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n\n\n\n\n48842 rows × 14 columns\n\n\n\nMore detail on the skrub configuration is reported in the User Guide.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "href": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.6 Working with big tables",
    "text": "3.6 Working with big tables\nPlotting and measuring the column correlations are expensive operations and may take a long time, so when the dataframe under study is large it may be more convenient to skip them.\nThe max_plot_columns and max_association_columns parameters allow to set a threshold on the number of columns: the TableReport will skip the respective task if the number of colums in the dataframe is larger than the threshold:\n\nTableReport(\n    data, max_association_columns=3, max_plot_columns=3, open_tab=\"distributions\"\n)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhen the number of columns is too large, an information message is shown in the respective tab instead of the plots or correlations.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#questions",
    "href": "content/chapters/01_exploring_data.html#questions",
    "title": "3  Exploring dataframes with skrub",
    "section": "4.1 Questions",
    "text": "4.1 Questions\n\nWhat’s the size of the dataframe? (columns and rows)\nHow many columns have object/numerical/datetime\nAre there columns with a large number of missing values?\nAre there columns that have a high cardinality (&gt;40 unique values)?\nWere datetime columns parsed correctly?\nWhich columns have outliers?\nWhich columns have an imbalanced distribution?\nWhich columns are strongly correlated with each other?\n\n# PLACEHOLDER\n#\n#\n#\n#\n#\n#\n#\n#\n#",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#answers",
    "href": "content/chapters/01_exploring_data.html#answers",
    "title": "3  Exploring dataframes with skrub",
    "section": "4.2 Answers",
    "text": "4.2 Answers\n\nWhat’s the size of the dataframe? (columns and rows)\n\n9228 rows × 8 columns\n\nHow many columns have object/numerical/datetime\n\nNo datetime columns, one integer column (year_first_hired), all other columns are objects.\n\nAre there columns with a large number of missing values?\n\nNo, only the gender column contains a small fraction (0.2%) of missing values.\n\nAre there columns that have a high cardinality?\n\nYes, division, employee_position_title, date_first_hired have a cardinality larger than 40.\n\nWere datetime columns parsed correctly?\n\nNo, the date_first_hired column has dtype Object.\n\nWhich columns have outliers?\n\nNo columns seem to include outliers.\n\nWhich columns have an imbalanced distribution?\n\nassignment_category has an unbalanced distribution.\n\nWhich columns are strongly correlated with each other?\n\ndepartment and department_name have a Cramer’s V of 1, so they are very strongly correlated.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html",
    "href": "content/chapters/02_cleaning_data.html",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "",
    "text": "4.1 Introduction\nIn this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner.\nWe first load the wine dataset from the local repository. This dataset is a downsampled version of the OpenML dataset (id=42074).\nfrom sklearn.datasets import fetch_openml\nfrom skrub import TableReport\nimport pandas as pd\n\nwine = pd.read_csv(\"../data/wine/data.csv\")\nWe can explore it using the TableReport:\nTableReport(wine)\n\nProcessing column   1 / 10Processing column   2 / 10Processing column   3 / 10Processing column   4 / 10Processing column   5 / 10Processing column   6 / 10Processing column   7 / 10Processing column   8 / 10Processing column   9 / 10Processing column  10 / 10\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\nWe can notice that there are a few columns that contain a sizable amount of missing values (“region_2” and “designation”). If we want to remove these columns programmatically using pandas, we have to do something like this:\nwine.loc[:, wine.isnull().mean() &lt;= 0.3]\n\n\n\n\n\n\n\n\ncountry\ndescription\npoints\nprice\nprovince\nregion_1\nvariety\nwinery\n\n\n\n\n0\nNew Zealand\nAn obvious, unsubtle Chardonnay that flashes p...\n85\n12.0\nGisborne\nNaN\nChardonnay\nBrancott\n\n\n1\nItaly\nCoffee bean, leather and tobacco tones are sur...\n87\nNaN\nPiedmont\nRoero\nNebbiolo\nDeltetto\n\n\n2\nPortugal\nBottled in June 2009, nearly four years after ...\n91\n25.0\nDouro\nNaN\nPortuguese Red\nMário Braga\n\n\n3\nChile\nDusty, mild red fruit aromas bring hints of fl...\n84\n10.0\nColchagua Valley\nNaN\nMerlot\nSanta Carolina\n\n\n4\nUS\nThe most massive, dense and ageworthy of all t...\n92\n56.0\nWashington\nRed Mountain\nCabernet Sauvignon\nSparkman\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nSpain\nUnusual in that this cava hails from the Riber...\n85\n16.0\nCatalonia\nCava\nSparkling Blend\nFinca Torremilanos\n\n\n9996\nUS\nIn this newest vintage of Oriana, the Riesling...\n90\n24.0\nWashington\nColumbia Valley (WA)\nWhite Blend\nBrian Carter Cellars\n\n\n9997\nItaly\nFounded in 1918 by one of the grandfathers and...\n95\nNaN\nPiedmont\nBarolo\nNebbiolo\nCantina Bartolo Mascarello\n\n\n9998\nItaly\nPerticaia delivers a gorgeous Trebbiano with r...\n89\nNaN\nCentral Italy\nUmbria\nTrebbiano\nPerticaia\n\n\n9999\nFrance\nFloral wine, light in structure with some fres...\n87\nNaN\nBordeaux\nPessac-Léognan\nBordeaux-style Red Blend\nChâteau Les Carmes Haut-Brion\n\n\n\n\n10000 rows × 8 columns\nIt may also be beneficial to convert numerical features to float32, to reduce the computational cost:\nwine.astype({col: \"float32\" for col in wine.select_dtypes(include=\"number\").columns})\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n0\nNew Zealand\nAn obvious, unsubtle Chardonnay that flashes p...\nUnoaked\n85.0\n12.0\nGisborne\nNaN\nNaN\nChardonnay\nBrancott\n\n\n1\nItaly\nCoffee bean, leather and tobacco tones are sur...\nBraja Riserva\n87.0\nNaN\nPiedmont\nRoero\nNaN\nNebbiolo\nDeltetto\n\n\n2\nPortugal\nBottled in June 2009, nearly four years after ...\nQuinta do Mourão Rio Bom Colheita\n91.0\n25.0\nDouro\nNaN\nNaN\nPortuguese Red\nMário Braga\n\n\n3\nChile\nDusty, mild red fruit aromas bring hints of fl...\nReserva\n84.0\n10.0\nColchagua Valley\nNaN\nNaN\nMerlot\nSanta Carolina\n\n\n4\nUS\nThe most massive, dense and ageworthy of all t...\nKingpin\n92.0\n56.0\nWashington\nRed Mountain\nColumbia Valley\nCabernet Sauvignon\nSparkman\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nSpain\nUnusual in that this cava hails from the Riber...\nPeñalba López Brut Nature\n85.0\n16.0\nCatalonia\nCava\nNaN\nSparkling Blend\nFinca Torremilanos\n\n\n9996\nUS\nIn this newest vintage of Oriana, the Riesling...\nOriana White\n90.0\n24.0\nWashington\nColumbia Valley (WA)\nColumbia Valley\nWhite Blend\nBrian Carter Cellars\n\n\n9997\nItaly\nFounded in 1918 by one of the grandfathers and...\nNaN\n95.0\nNaN\nPiedmont\nBarolo\nNaN\nNebbiolo\nCantina Bartolo Mascarello\n\n\n9998\nItaly\nPerticaia delivers a gorgeous Trebbiano with r...\nNaN\n89.0\nNaN\nCentral Italy\nUmbria\nNaN\nTrebbiano\nPerticaia\n\n\n9999\nFrance\nFloral wine, light in structure with some fres...\nNaN\n87.0\nNaN\nBordeaux\nPessac-Léognan\nNaN\nBordeaux-style Red Blend\nChâteau Les Carmes Haut-Brion\n\n\n\n\n10000 rows × 10 columns\nThese operations are quite common in most cases (although the parameters and requirements may vary by project), so writing the code that addresses them may become repetitive.\nA simpler way of dealing with this preliminary preparation is to use the skrub Cleaner.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "href": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "4.2 Using the skrub Cleaner",
    "text": "4.2 Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column:\nConsider this example dataframe:\n\ndf = pd.DataFrame(\n    {\n        \"numerical_1\": [1, 2, 3, 4, 5],\n        \"numerical_2\": [10.5, 20.3, None, 40.1, 50.2],\n        \"string_column\": [\"apple\", \"?\", \"banana\", \"cherry\", \"?\"],\n        \"datetime_column\": [\n            \"03 Jan 2020\",\n            \"04 Jan 2020\",\n            \"05 Jan 2020\",\n            \"06 Jan 2020\",\n            \"07 Jan 2020\",\n        ],\n        \"all_none\": [None, None, None, None, None],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\nall_none\n\n\n\n\n0\n1\n10.5\napple\n03 Jan 2020\nNone\n\n\n1\n2\n20.3\n?\n04 Jan 2020\nNone\n\n\n2\n3\nNaN\nbanana\n05 Jan 2020\nNone\n\n\n3\n4\n40.1\ncherry\n06 Jan 2020\nNone\n\n\n4\n5\n50.2\n?\n07 Jan 2020\nNone\n\n\n\n\n\n\n\nThis dataframe has mixed type columns, with some of the missing values denoted as None and some \"?\". The datetime column has a non-standard format and has been parsed as a string column. Finally, one of the columns is completely empty.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   numerical_1      5 non-null      int64  \n 1   numerical_2      4 non-null      float64\n 2   string_column    5 non-null      object \n 3   datetime_column  5 non-null      object \n 4   all_none         0 non-null      object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 332.0+ bytes\n\n\n\n4.2.1 Using plain pandas\nCleaning this dataset using plain pandas may require writing code like this:\n\n# Parse the datetime strings with a specific format\ndf['datetime_column'] = pd.to_datetime(df['datetime_column'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_clean = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_clean = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_clean.columns[df_clean.eq('').all()]\n    df_clean = df_clean.drop(columns=empty_string_cols)\n    return df_clean\n\n# Apply the function to the DataFrame\ndf_clean = drop_empty_columns(df_clean)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\n?\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\n?\n2020-01-07\n\n\n\n\n\n\n\n\n\n4.2.2 The alternative: skrub.Cleaner\nBy default, the Cleaner applies various transformations that can sanitize many common use cases:\n\nfrom skrub import Cleaner\ndf_clean = Cleaner().fit_transform(df)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNone\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNone\n2020-01-07\n\n\n\n\n\n\n\nWe can see that the cleaned version of the dataframe is now marking missing values correctly, and that the datetime column has been parsed accordingly:\n\ndf_clean.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   numerical_1      5 non-null      int64         \n 1   numerical_2      4 non-null      float64       \n 2   string_column    3 non-null      object        \n 3   datetime_column  5 non-null      datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\n\n\n4.2.3 Cleaning steps performed by the Cleaner\nIn more detail, the Cleaner executes the following steps in order:\n\nIt replaces common strings used to represent missing values (e.g., NULL, ?) with NA markers.\nIt uses the DropUninformative transformer to decide whether a column is “uninformative”, that is, it is not likely to bring information useful to train a ML model. For example, empty columns are uninformative.\nIt tries to parse datetime columns using common formats, or a user-provided datetime_format.\nIt processes categorical columns to ensure consistent typing depending on the dataframe library in use.\nIt converts columns to string, unless they have a data type that carries more information, such as numerical, datetime, and categorial columns.\nFinally, it can convert numerical columns to np.float32 dtype if called with the parameter numeric_dtype=\"float32\". This ensures a consistent representation of numbers and missing values, and helps reducing the memory footprint.\n\nWe can look back at the “wine” dataframe and clean it with a suitably configured Cleaner:\n\ncleaner = Cleaner(drop_null_fraction=0.3, numeric_dtype=\"float32\")\n\ncleaner.fit_transform(wine)\n\n\n\n\n\n\n\n\ncountry\ndescription\npoints\nprice\nprovince\nregion_1\nvariety\nwinery\n\n\n\n\n0\nNew Zealand\nAn obvious, unsubtle Chardonnay that flashes p...\n85.0\n12.0\nGisborne\nNaN\nChardonnay\nBrancott\n\n\n1\nItaly\nCoffee bean, leather and tobacco tones are sur...\n87.0\nNaN\nPiedmont\nRoero\nNebbiolo\nDeltetto\n\n\n2\nPortugal\nBottled in June 2009, nearly four years after ...\n91.0\n25.0\nDouro\nNaN\nPortuguese Red\nMário Braga\n\n\n3\nChile\nDusty, mild red fruit aromas bring hints of fl...\n84.0\n10.0\nColchagua Valley\nNaN\nMerlot\nSanta Carolina\n\n\n4\nUS\nThe most massive, dense and ageworthy of all t...\n92.0\n56.0\nWashington\nRed Mountain\nCabernet Sauvignon\nSparkman\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nSpain\nUnusual in that this cava hails from the Riber...\n85.0\n16.0\nCatalonia\nCava\nSparkling Blend\nFinca Torremilanos\n\n\n9996\nUS\nIn this newest vintage of Oriana, the Riesling...\n90.0\n24.0\nWashington\nColumbia Valley (WA)\nWhite Blend\nBrian Carter Cellars\n\n\n9997\nItaly\nFounded in 1918 by one of the grandfathers and...\n95.0\nNaN\nPiedmont\nBarolo\nNebbiolo\nCantina Bartolo Mascarello\n\n\n9998\nItaly\nPerticaia delivers a gorgeous Trebbiano with r...\n89.0\nNaN\nCentral Italy\nUmbria\nTrebbiano\nPerticaia\n\n\n9999\nFrance\nFloral wine, light in structure with some fres...\n87.0\nNaN\nBordeaux\nPessac-Léognan\nBordeaux-style Red Blend\nChâteau Les Carmes Haut-Brion\n\n\n\n\n10000 rows × 8 columns",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "href": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "4.3 Under the hood: DropUninformative",
    "text": "4.3 Under the hood: DropUninformative\nWhen the Cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that do not bring useful information for training a ML model, and should therefore be dropped.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction.\n\nBy default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nSetting the threshold to None will disable this check and therefore retain empty columns.\n\nIt contains only one value, and no missing values.\n\nThis is controlled by the drop_if_constant flag, which is False by default.\n\nAll values in the column are distinct.\n\nThis may be the case if the column contains UIDs, but it can also happen when the column contains text.\nThis check is off by default and can be turned on by setting drop_if_unique to True.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html",
    "href": "content/chapters/quiz_01.html",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "",
    "text": "5.1 Question 1",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html#question-1",
    "href": "content/chapters/quiz_01.html#question-1",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "",
    "text": "What do I need to open a saved TableReport?\n\nA python console\nAn internet browser\nA Jupyter notebook\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAn internet browser is enough.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html#question-2",
    "href": "content/chapters/quiz_01.html#question-2",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "5.2 Question 2",
    "text": "5.2 Question 2\n\n\n\n\n\n\nConsider this dataframe and TableReport, then answer the question.\n\nimport pandas as pd\nfrom skrub import TableReport\n\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n    'Salary': [70000, 80000, 90000, 100000, 110000],\n    'Department': ['HR', 'Finance', 'IT', 'Marketing', 'Sales']\n})\n\nTableReport(df, max_plot_columns=5, max_association_columns=3)\n\nProcessing column   1 / 5Processing column   2 / 5Processing column   3 / 5Processing column   4 / 5Processing column   5 / 5\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhat does the “Distributions” tab show? What about the “Associations” tab?\n\nBoth tabs work as normal.\nThe “Distribution” tab shows the plots, “Associations” are not shown.\nBoth tabs contain a message explaining their operation was skipped.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nThe “Distribution” contains the usual distribution plots, while the computation of the associations was skipped because the number of columns in the dataframe was larger than max_association_columns.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html#question-3",
    "href": "content/chapters/quiz_01.html#question-3",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "5.3 Question 3",
    "text": "5.3 Question 3\n\n\n\n\n\n\nDoes the TableReport parse datetimes or other data types?\n\nYes, the TableReport automatically converts datetime strings to datetime objects and strings that contain numbers into floats.\nNo, the TableReport does not perform any conversion.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: No, the TableReport is generated on the basis of the datatypes found in the supplied dataframe. Any datatype parsing must be done before generating the report, e.g., by using the Cleaner.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html#question-4",
    "href": "content/chapters/quiz_01.html#question-4",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "5.4 Question 4",
    "text": "5.4 Question 4\n\n\n\n\n\n\nWhich of these transformations is executed by default when the Cleaner is fitted on a dataframe?\n\nA) Dropping constant columns\nB) Dropping columns that contain only missing values\nC) Dropping columns that contain more than 90% of missing values\nD) Dropping columns where all values are distinct\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nColumns that contain only missing values, i.e., where the fraction of missing values is 1.0, are dropped. This is controlled by the drop_null_fraction parameter.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html#question-5",
    "href": "content/chapters/quiz_01.html#question-5",
    "title": "5  Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "5.5 Question 5",
    "text": "5.5 Question 5\n\n\n\n\n\n\nConsider the following dataframe.\n\nimport pandas as pd\nmedical_df = pd.DataFrame({\n    'Patient_ID': ['P001', 'P002', 'P003', 'P004', 'P005'],\n    'Visit_Date': ['10 Jan 2023', '15 Feb 2023', '20 Mar 2023', '25 Apr 2023', None],\n    'Blood_Pressure': [120.5, 130.2, 125.8, 140.0, 135.6],\n    'Diagnosis': ['Hypertension', '?', '?', 'Hypertension', 'Diabetes'],\n})\n\nmedical_df\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\n130.2\n?\n\n\n2\nP003\n20 Mar 2023\n125.8\n?\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\nWhat is the output of this cleaner?\n\nfrom skrub import Cleaner\ncleaner = Cleaner()\ndf_clean = cleaner.fit_transform(medical_df)\n\n\nA)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n2023-01-10\n120.5\nHypertension\n\n\n1\nP002\n2023-02-15\n130.2\nNone\n\n\n2\nP003\n2023-03-20\n125.8\nNone\n\n\n3\nP004\n2023-04-25\n140.0\nHypertension\n\n\n4\nP005\nNaT\n135.6\nDiabetes\n\n\n\n\n\n\n\n\nB)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\nNaN\n?\n\n\n2\nP003\n20 Mar 2023\n125.8\n?\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\n\nC)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\nNaN\nNone\n\n\n2\nP003\n20 Mar 2023\n125.8\nNone\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: A)\nThe Cleaner replaces strings that are commonly used to denote missing values (such as “?”), and guesses most common datetime formats from their strings.\nNo empty columns are present, so no further transformations are made.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Quiz: Exploring and sanitizing dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html",
    "href": "content/chapters/03_feat_eng_apply.html",
    "title": "6  Applying transformers to columns",
    "section": "",
    "text": "6.1 Introduction\nOften, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception.\nScikit-learn provides the ColumnTransformer to deal with this:\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "6  Applying transformers to columns",
    "section": "6.2 Selection operations in a scikit-learn pipeline",
    "text": "6.2 Selection operations in a scikit-learn pipeline\nIn some situations, it may be necessary to select or remove specific columns from a dataframe: this means removing some features from the original table. This can be done with SelectCols and DropCols, which work as their name suggests, and can take a cols parameter to choose which columns to select or drop respectively.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nWe can selectively choose or drop columns based on names, or more complex rules (see the next chapter).\n\nfrom skrub import SelectCols\nSelectCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\ndate\n\n\n\n\n0\n03 January 2023\n\n\n1\n04 February 2023\n\n\n2\n05 March 2023\n\n\n\n\n\n\n\n\nfrom skrub import DropCols\nDropCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\nvalues\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "href": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "title": "6  Applying transformers to columns",
    "section": "6.3 ApplyToCols and ApplyToFrame",
    "text": "6.3 ApplyToCols and ApplyToFrame\n\n6.3.1 Applying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ndate_03 January 2023\ndate_04 February 2023\ndate_05 March 2023\nvalues\n\n\n\n\n0\n1.0\n0.0\n0.0\n-1.224745\n\n\n1\n0.0\n1.0\n0.0\n0.000000\n\n\n2\n0.0\n0.0\n1.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n6.3.2 Applying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, it may be beneficial to apply the same transformer to a subset of columns in a dataframe.\nThis example dataframe contains some patient information, and some (random) metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541\n\n\n\n\n\n\n\n\n\n6.3.3 The allow_reject parameter\nWhen ApplyToCols or ApplyToFrame are using a skrub transformer, they can use the allow_reject parameter for more flexibility. By setting allow_reject to True, columns that cannot be treated by the current transformer will be ignored rather than raising an exception.\nConsider this example. By default, ToDatetime raises a RejectColumn exception when it finds a column it cannot convert to datetime.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nBy setting allow_reject=True, the datetime column is converted properly and the other column is passed through without issues.\n\nwith_reject = ApplyToCols(ToDatetime(), allow_reject=True)\nwith_reject.fit_transform(df)\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n2023-01-03\n10\n\n\n1\n2023-02-04\n20\n\n\n2\n2023-03-05\n30",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#concatenating-the-skrub-column-transformers",
    "href": "content/chapters/03_feat_eng_apply.html#concatenating-the-skrub-column-transformers",
    "title": "6  Applying transformers to columns",
    "section": "6.4 Concatenating the skrub column transformers",
    "text": "6.4 Concatenating the skrub column transformers\nSkrub column transformers can be concatenated by using scikit-learn pipelines. In the following example, we first select only the column patiend_id, then encode it using OneHotEncoder and finally use PCA to reduce the number of dimensions.\nThis is done by wrapping the latter two steps in ApplyToCols and ApplyToFrame respectively, and then putting all transformers in order in a scikit-learn pipeline using make_pipeline.\n\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols\n\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nselect = SelectCols(\"patient_id\")\nencode = ApplyToCols(OneHotEncoder(sparse_output=False))\nreduce = ApplyToFrame(PCA(n_components=2))\n\ntransform = make_pipeline(select, encode, reduce)\ndft= transform.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npca0\npca1\n\n\n\n\n0\n1.451188e-17\n9.393890e-18\n\n\n1\n-2.405452e-02\n9.397337e-01\n\n\n2\n-2.305851e-01\n9.374222e-03\n\n\n3\n-5.287468e-02\n9.374222e-03\n\n\n4\n-7.954573e-02\n-6.807817e-03\n\n\n\n\n\n\n\n\n6.4.1 The order of column transformations is important\nSome care must be taken when concatenating columnn transformers, in particular when selection is done on datatypes. Consider this case:\n\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\nIn the first case, we encode and then scale, in the second case we instead scale first and then encode.\n\ntransform_1 = make_pipeline(encode, scale)\ndft = transform_1.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.301570\n0.904534\n-0.904534\n\n\n1\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.709947\n0.904534\n-0.904534\n\n\n2\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n0.059162\n-1.105542\n1.105542\n\n\n3\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.479057\n-1.105542\n1.105542\n\n\n4\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.473298\n0.904534\n-0.904534\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ntransform_2 = make_pipeline(scale, encode)\ndft = transform_2.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.301570\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.709947\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.059162\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.479057\n0.0\n1.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.473298\n1.0\n0.0\n\n\n\n\n5 rows × 23 columns\n\n\n\nThe result of transform_1 is that the features that have been generated by the OneHotEncoder are then scaled by the StandardScaler, because the new features are numeric and are therefore selected in the next step.\nIn many cases, this behavior is not desired: while some model types may not be affected by the different ordering (such as tree-based models), linear models and NN-based models may produce worse results.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html",
    "href": "content/chapters/04_selectors.html",
    "title": "7  Choose your column: selectors",
    "section": "",
    "text": "8 Introduction\nVery often, column selection is more complex than simply passing a list of column names to a transformer: it may be necessary to select all columns that have a specific data type, or based on some other characteristic (presence of nulls, column cardinality, etc.).\nThe skrub selectors implement a number of selection strategies that can be combined in various ways to build complex filtering conditions that can then be employed by ApplyToCols, ApplyToFrame, SelectCols and DropCols.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#skrub-selectors",
    "href": "content/chapters/04_selectors.html#skrub-selectors",
    "title": "7  Choose your column: selectors",
    "section": "8.1 Skrub selectors",
    "text": "8.1 Skrub selectors\nSelectors are available from the skrub.selectors namespace:\n\nimport skrub.selectors as s\n\nWe will use this example dataframe to test some of the selectors:\n\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\n\n\n\n\n\n\n\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\n\n.float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\n\n\n\n\n\n\n\nAdditional conditions include:\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\n\n\n\n\n\n\n\nVarious selectors allow to choose columns based on their name:\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions\n\n\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#combining-selectors",
    "href": "content/chapters/04_selectors.html#combining-selectors",
    "title": "7  Choose your column: selectors",
    "section": "8.2 Combining selectors",
    "text": "8.2 Combining selectors\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -; for example to select all columns except for “datetime-col”, one would write:\n\nSelectCols(cols=s.all() - \"datetime-col\").fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\n\n\n\n\n\n\n\nTo select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#extracting-selected-columns",
    "href": "content/chapters/04_selectors.html#extracting-selected-columns",
    "title": "7  Choose your column: selectors",
    "section": "8.3 Extracting selected columns",
    "text": "8.3 Extracting selected columns\nSelectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#designing-custom-filters",
    "href": "content/chapters/04_selectors.html#designing-custom-filters",
    "title": "7  Choose your column: selectors",
    "section": "8.4 Designing custom filters",
    "text": "8.4 Designing custom filters\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function on the given column is True. It is also possible to pass arguments to the function to further tweak the conditions. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html",
    "href": "content/chapters/quiz_02.html",
    "title": "8  Quiz: Column-level transformations",
    "section": "",
    "text": "9 Column transformers",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html#question-1",
    "href": "content/chapters/quiz_02.html#question-1",
    "title": "8  Quiz: Column-level transformations",
    "section": "9.1 Question 1",
    "text": "9.1 Question 1\n\n\n\n\n\n\nConsider this diagram. Which column transformer can replicate this behavior if it wraps a OneHotEncoder?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: A) ApplyToCols takes a transformer, then clones it and applies it separately to each column under selection (in this case, Name and Desc). Columns that were not selected are left unchanged.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html#question-2",
    "href": "content/chapters/quiz_02.html#question-2",
    "title": "8  Quiz: Column-level transformations",
    "section": "9.2 Question 2",
    "text": "9.2 Question 2\n\n\n\n\n\n\nConsider this diagram. Which column transformer can replicate this behavior if it wraps a PCA?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B) ApplyToFrame takes a transformer and a list of columns (usually, a subset of the columns in the dataframe), then applies the transformer to all the selected columns at once, replacing them with the output of the transfromer. Columns that were not selected are left unchanged.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html#question-3",
    "href": "content/chapters/quiz_02.html#question-3",
    "title": "8  Quiz: Column-level transformations",
    "section": "9.3 Question 3",
    "text": "9.3 Question 3\n\n\n\n\n\n\nIs the following statement true or false?\n\nThe output of these two snippets is the same:\n\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\ncase_1 = make_pipeline(encode, scale)\ncase_1.fit_transform(df)\ncase_2 = make_pipeline(scale, encode)\ncase_2.fit_transform(df)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: False.\nThe order of the operations matters, and a different order leads to different results.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols, ApplyToCols, ApplyToFrame\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport skrub.selectors as s \n\nn_patients = 5\ndf = pd.DataFrame(\n    {\n        \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n        \"age\": np.random.randint(18, 80, size=n_patients),\n        \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n    }\n)\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\n\ncase_1 = make_pipeline(encode, scale)\ndf_1 = case_1.fit_transform(df)\ndf_1.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n2.0\n-0.5\n-0.5\n-0.5\n-0.5\n-0.368230\n0.5\n-0.5\n\n\n1\n-0.5\n2.0\n-0.5\n-0.5\n-0.5\n-1.657034\n0.5\n-0.5\n\n\n2\n-0.5\n-0.5\n2.0\n-0.5\n-0.5\n0.368230\n-2.0\n2.0\n\n\n3\n-0.5\n-0.5\n-0.5\n2.0\n-0.5\n1.380862\n0.5\n-0.5\n\n\n4\n-0.5\n-0.5\n-0.5\n-0.5\n2.0\n0.276172\n0.5\n-0.5\n\n\n\n\n\n\n\n\ncase_2 = make_pipeline(scale, encode)\ndf_2 = case_2.fit_transform(df)\ndf_2.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n-0.368230\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n-1.657034\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.368230\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n1.380862\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.276172\n1.0\n0.0",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html#question-4",
    "href": "content/chapters/quiz_02.html#question-4",
    "title": "8  Quiz: Column-level transformations",
    "section": "10.1 Question 4",
    "text": "10.1 Question 4\n\n\n\n\n\n\nWhat does this selector do?\n\nfrom skrub import SelectCols\nimport skrub.selectors as s\n\ndef fun(col):\n    mean = col.mean()\n    return mean &gt; 40000\n\nsel = s.numeric() & s.filter(fun) \nt = SelectCols(cols=sel)\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\n\n\n\n\n0\n45000.0\n\n\n1\n52000.0\n\n\n2\n61000.0\n\n\n3\nNaN\n\n\n4\n48000.0\n\n\n\n\n\n\n\n\nA) It drops only numerical columns with mean &lt; 40000\nB) It selects only numerical column with mean &gt; 40000\nC) It selects non-numeric columns or numeric columns that have mean &gt; 40000\nD) It drops only numeric columns unless their mean is &lt; 40000\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\n\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\n\n\n\n\n0\n45000.0\n\n\n1\n52000.0\n\n\n2\n61000.0\n\n\n3\nNaN\n\n\n4\n48000.0",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_02.html#question-5",
    "href": "content/chapters/quiz_02.html#question-5",
    "title": "8  Quiz: Column-level transformations",
    "section": "10.2 Question 5",
    "text": "10.2 Question 5\n\n\n\n\n\n\nWhat does this selector do?\n\nsel = s.cols(\"salary\") | s.filter_names(lambda name: name.endswith(\"_title\"))\n\nt = SelectCols(cols=sel)\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\njob_title\ndepartment_title\n\n\n\n\n0\n45000.0\nengineer\nIT\n\n\n1\n52000.0\nanalyst\nFinance\n\n\n2\n61000.0\nconsultant\nConsulting\n\n\n3\nNaN\ndesigner\nDesign\n\n\n4\n48000.0\ndeveloper\nDevelopment\n\n\n\n\n\n\n\n\nA) It selects the column “salary”, numerical columns, and columns that end in “_title”\nB) It drops all columns (including “salary”), except the columns that end in “_title”\nC) It selects the column “salary”, and columns whose name end in “_title”\nD) It drops the column “salary”, and columns whose name end in “_title”\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: C)\n\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\njob_title\ndepartment_title\n\n\n\n\n0\n45000.0\nengineer\nIT\n\n\n1\n52000.0\nanalyst\nFinance\n\n\n2\n61000.0\nconsultant\nConsulting\n\n\n3\nNaN\ndesigner\nDesign\n\n\n4\n48000.0\ndeveloper\nDevelopment",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html",
    "href": "content/chapters/05_feat_eng_numerical.html",
    "title": "9  Scaling numerical features safely",
    "section": "",
    "text": "9.1 Introduction\nNow that we can transform any column we want thanks to ApplyToCols, ApplyToFrame and the selectors, we can start covering the feature engineering part of our pipeline, beginning from numerical features.\nSpecifically, we will find out how to safely scale numerical features with the skrub SquashingScaler.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "href": "content/chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "title": "9  Scaling numerical features safely",
    "section": "9.2 Numerical features with outliers",
    "text": "9.2 Numerical features with outliers\nWhen dealing with numerical features that contain outliers (including infinite values), standard scaling methods can be problematic. Outliers can dramatically affect the centering and scaling of the entire dataset, causing the scaled inliers to be compressed into a narrow range.\nConsider this example:\n\nfrom helpers import (\n    generate_data_with_outliers,\n    plot_feature_with_outliers\n)\n\nvalues = generate_data_with_outliers()\n\nplot_feature_with_outliers(values)\n\n\n\n\n\n\n\n\nIn this case, most of the values are in the range [-2, 2], but there are some large outliers in the range [-40, 40] that can cause issues when the feature needs to be scaled.\n\n9.2.1 Regular scalers and their limitations\nThe StandardScaler computes mean and standard deviation across all values. With outliers present, these statistics become unreliable, and the scaling factor can become too small, squashing inlier values.\nThe RobustScaler uses quantiles (typically the 25th and 75th percentiles) instead of mean/std, which makes it more resistant to outliers. However, it doesn’t bound the output values, so extreme outliers can still have very large scaled values.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html#squashingscaler-a-robust-solution",
    "href": "content/chapters/05_feat_eng_numerical.html#squashingscaler-a-robust-solution",
    "title": "9  Scaling numerical features safely",
    "section": "9.3 SquashingScaler: A robust solution",
    "text": "9.3 SquashingScaler: A robust solution\nThe SquashingScaler combines robust centering with smooth clipping to handle outliers effectively.\nIt works as following:\n\nIt centers the median to 0, then it scales values using quantile-based statistics.\nIt fills constant columns with 0s.\nIt applies a smooth squashing function: \\(x_{\\text{out}} = \\frac{z}{\\sqrt{1 + (z/B)^2}}\\)\nIt constrains all values to the range \\([-\\texttt{max\\_absolute\\_value}, \\texttt{max\\_absolute\\_value}]\\) (default: 3)\nInfinite values are mapped to the corresponding boundaries.\nNaN values are kept unchanged.\n\n\n9.3.1 Advantages and disadvantages of SquashingScaler\nThe SquashingScaler has various advantages over traditional scalers:\n\nIt is outlier-resistant: Outliers don’t affect inlier scaling, unlike the StandardScaler.\nIt has bounded output: All values stay in a predictable range, ideal for neural networks and linear models.\nIt handles edge cases: The scaler works with infinite values and constant columns.\nIt preserves missing data: NaN values are kept unchanged.\n\nA disadvantage of the SquashingScaler is that it is non-invertible: The soft clipping function is smooth but cannot be exactly inverted.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html#conclusion",
    "href": "content/chapters/05_feat_eng_numerical.html#conclusion",
    "title": "9  Scaling numerical features safely",
    "section": "9.4 Conclusion",
    "text": "9.4 Conclusion\nWhen compared on data with outliers:\n\nStandardScaler compresses inliers due to large scaling factors\nRobustScaler preserves relative scales but allows extreme outlier values\nSquashingScaler keeps inliers in a reasonable range while smoothly bounding all values\n\nIf we plot the impact of each scaler on the result, this is what we can see:\n\nfrom helpers import scale_feature_and_plot\nscale_feature_and_plot(values)",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html",
    "href": "content/chapters/06_feat_eng_datetimes.html",
    "title": "10  Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "10.1 Introduction to Datetime Features\nDatetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Correct parsing of these and more exotic formats is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "href": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "title": "10  Encoding datetime features with DatetimeEncoder",
    "section": "10.2 Converting datetime strings to datetime objects",
    "text": "10.2 Converting datetime strings to datetime objects\nOften, the first operation that must be done to work with datetime objects is converting the datetimes from a string representation to a proper datetime object. This is beneficial because using datetimes gives access to datetime-specific features, and allows to access the different parts of the datetime.\nSkrub provides different objects to deal with the conversion problem.\nToDatetime is a single column transformer that tries to conver the given column to datetime either by relying on a user-provided format, or by guessing common formats. Since this transformer must be applied to single columns (rather than dataframes), it is typically better to use it in conjunction with ApplyToCols. Additionally, the allow_reject parameter of ApplyToCols should be set to True to avoid raising exceptions for non-datetime columns:\n\nfrom skrub import ApplyToCols, ToDatetime\n\nimport pandas as pd\n\ndata = {\n    \"dates\": [\n        \"2023-01-03\",\n        \"2023-02-15\",\n        \"2023-03-27\",\n        \"2023-04-10\",\n    ]\n}\ndf = pd.DataFrame(data)\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\ndf_enc.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   dates   4 non-null      datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 164.0 bytes\n\n\nto_datetime works similarly to pd.to_datetime, or the example shown above with ApplyToCols.\n\n\n\n\n\n\nWarning\n\n\n\nto_datetime is a stateless function, so it should not be used in a pipeline, because it does not guarantee consistency between fit_transform and successive transform. ApplyToCols(ToDatetime(), allow_reject=True) is a better solution for pipelines.\n\n\nFinally, the standard Cleaner can be used for parsing datetimes, as it uses ToDatetime under the hood, and can take the datetime_format. As the Cleaner is a transformer, it guarantees consistency between fit_transform and transform.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "title": "10  Encoding datetime features with DatetimeEncoder",
    "section": "10.3 Encoding datetime features",
    "text": "10.3 Encoding datetime features\nDatetimes cannot be used “as-is” for training ML models, and must instead be converted to numerical features. Typically, this is done by “splitting” the datetime parts (year, month, day etc.) into separate columns, so that each column contains only one number.\nAdditional features may also be of interest, such as the number of seconds since epoch (which increases monotonically and gives an indication of the order of entries), whether a date is a weekday or weekend, or the day of the year.\nTo achieve this with standard dataframe libraries, the code looks like this:\n\ndf_enc[\"year\"] = df_enc[\"dates\"].dt.year\ndf_enc[\"month\"] = df_enc[\"dates\"].dt.month\ndf_enc[\"day\"] = df_enc[\"dates\"].dt.day\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"total_seconds\"] = (\n    df_enc[\"dates\"] - pd.Timestamp(\"1970-01-01\")\n) // pd.Timedelta(seconds=1)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nyear\nmonth\nday\nweekday\nday_of_year\ntotal_seconds\n\n\n\n\n0\n2023-01-03\n2023\n1\n3\n1\n3\n1672704000\n\n\n1\n2023-02-15\n2023\n2\n15\n2\n46\n1676419200\n\n\n2\n2023-03-27\n2023\n3\n27\n0\n86\n1679875200\n\n\n3\n2023-04-10\n2023\n4\n10\n0\n100\n1681084800\n\n\n\n\n\n\n\nSkrub’s DatetimeEncoder allows to add the same features with a simpler interface. As the DatetimeEncoder is a single column transformer, we use again ApplyToCols.\nThe DatetimeEncoder includes various parameters to add more features to the transformed dataframe: - add_total_seconds adds the number of seconds since Epoch (1970-01-01) - add_weekday adds the day in the week (to highlight weekends, for example) - add_day_of_year adds the day in year of the datetime\n\nfrom skrub import DatetimeEncoder\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nde = DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True)\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_month\ndates_day\ndates_total_seconds\ndates_weekday\ndates_day_of_year\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "title": "10  Encoding datetime features with DatetimeEncoder",
    "section": "10.4 Periodic features",
    "text": "10.4 Periodic features\nPeriodic features are useful for training machine learning models because they capture the cyclical nature of certain data patterns. For example, time-related features such as hours in a day, days in a week, or months in a year often exhibit periodic behavior. By encoding these features periodically, models can better understand and predict patterns that repeat over time, such as daily traffic trends, or seasonal variations. This ensures that the model treats the start and end of a cycle as close neighbors, improving its ability to generalize and make accurate predictions.\nThis can be done manually with dataframe libraries. For example, circular encoding (a.k.a., trigonometric or sin/cos encoding) can be implemented with Pandas like so:\n\nimport numpy as np \n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_enc[\"day_of_year\"] / 365)\ndf_enc[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_enc[\"day_of_year\"] / 365)\n\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"weekday_sin\"] = np.sin(2 * np.pi * df_enc[\"weekday\"] / 7)\ndf_enc[\"weekday_cos\"] = np.cos(2 * np.pi * df_enc[\"weekday\"] / 7)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023-01-03\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023-02-15\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023-03-27\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023-04-10\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000\n\n\n\n\n\n\n\nAlternatively, the DatetimeEncoder can add periodic features using either circular or spline encoding through the periodic_encoding parameter:\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_total_seconds\ndates_month_circular_0\ndates_month_circular_1\ndates_day_circular_0\ndates_day_circular_1\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023.0\n1.672704e+09\n0.500000\n8.660254e-01\n5.877853e-01\n0.809017\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023.0\n1.676419e+09\n0.866025\n5.000000e-01\n1.224647e-16\n-1.000000\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023.0\n1.679875e+09\n1.000000\n6.123234e-17\n-5.877853e-01\n0.809017\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023.0\n1.681085e+09\n0.866025\n-5.000000e-01\n8.660254e-01\n-0.500000\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "href": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "title": "10  Encoding datetime features with DatetimeEncoder",
    "section": "10.5 Conclusions",
    "text": "10.5 Conclusions\nIn this chapter, we explored the importance and challenges of working with datetime features. We covered how to convert string representations of dates to datetime objects using skrub’s ToDatetime transformer and the Cleaner, both of which can be integrated into pipelines for robust preprocessing.\nWe also discussed the need to encode datetime features into numerical representations suitable for machine learning models. The DatetimeEncoder provides a convenient way to extract useful components such as year, month, day, weekday, day of year, and total seconds since epoch. Additionally, we saw how periodic (circular) encoding can be used to capture cyclical patterns in time-based data.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html",
    "href": "content/chapters/07_feat_eng_categorical.html",
    "title": "11  Mixed data: dealing with categories",
    "section": "",
    "text": "11.1 Introduction\nReal-world datasets rarely contain only numeric values. We frequently encounter categorical features—values that belong to discrete categories, such as names, occupations, geographic locations, or clothing sizes. Text data also falls into this category, since each unique string can be considered a categorical value.\nThe challenge is that machine learning models require numeric input. How do we convert these categorical values into numeric features that preserve their information and enable our models to make good predictions?\nThis chapter explores the various strategies and tools available in skrub to encode categorical features, with an eye on choosing the best method for our specific use case.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "href": "content/chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "title": "11  Mixed data: dealing with categories",
    "section": "11.2 Why Categorical Encoders Matter",
    "text": "11.2 Why Categorical Encoders Matter\nThe way we encode categorical features significantly impacts our machine learning pipeline:\n\nPerformance: The encoding choice directly affects how well our model learns from categorical information\nEfficiency: Some encodings create many features (potentially thousands), which increases computation time and memory usage\nInterpretability: Different encoders provide varying levels of transparency in what features represent\nScalability: Not all methods scale well to high-cardinality features (those with many unique values)\n\nUsing the appropriate encoder ensures we’re making the best use of categorical information while keeping our model efficient and interpretable.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "href": "content/chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "title": "11  Mixed data: dealing with categories",
    "section": "11.3 Categorical Encoders: Pros and Cons",
    "text": "11.3 Categorical Encoders: Pros and Cons\n\n11.3.1 One-Hot Encoding and Ordinal Encoding (scikit-learn)\nOneHotEncoder: Creates a binary indicator column for each unique category, where 1 denotes the presence of the category and 0 its absence.\nPros:\n\nStraightforward and intuitive\nWorks well for low-cardinality features (few unique values)\nProduces sparse matrices that can save memory\n\nCons:\n\nBecomes impractical with high-cardinality features (creates hundreds or thousands of columns)\nResults in mostly zero-valued sparse matrices when dense, which is the situation when working with dataframes\nIncreases overfitting risk and computational overhead\n\nThe OneHotEncoder is used by default by the skrub TableVectorizer for categorical features with fewer than 40 unique values.\nOrdinalEncoder: Assigns each category a numerical value (0, 1, 2, …).\nPros:\n\nVery memory-efficient\nCreates only one output column per input column\nFast to compute\n\nCons:\n\nIntroduces artificial ordering among categories that may not exist in reality\nCan mislead models into thinking some categories are “greater than” others\n\n\n\n11.3.2 Categorical encoders in skrub\nAll the categorical encoders in skrub are designed to encode any number of unique values using a fixed number of components: this number is controlled by the parameter n_components in each transformer.\n\n\n11.3.3 StringEncoder\nApproach: Applies term frequency-inverse document frequency (tf-idf) vectorization to character n-grams, followed by truncated singular value decomposition (SVD) for dimensionality reduction. This method is also known as Latent Semantic Analysis.\nPros:\n\nThe best all-rounder: Performs well on both categorical and text data\nFast training time\nRobust and generalizes well across different datasets\nNo artificial ordering introduced\n\nCons:\n\nLess interpretable than one-hot encoding or ordinal encoding\nMay not capture semantic relationships as well as language model-based approaches\nPerformance depends on the nature of the categorical data\n\n\n\n11.3.4 TextEncoder\nApproach: Uses pretrained language models from HuggingFace Hub to generate dense vector representations of text.\nPros:\n\nExceptional performance on free-flowing text and natural language\nCaptures semantic meaning and context\nLeverages knowledge from large-scale language model pretraining\nCan excel on datasets where domain-specific information aligns with pretraining data\n\nCons:\n\nVery computationally expensive: Significantly slower than other methods\nRequires heavy dependencies (PyTorch, transformers)\nModels are large and require downloading\nImpractical for CPU-only environments\nPerformance on traditional categorical data (non-text, such as IDs) is not much better than simpler methods\n\n\n\n11.3.5 MinHashEncoder\nApproach: Decomposes strings into n-grams and applies the MinHash algorithm for quick dimension reduction.\nPros:\n\nVery fast training time\nSimple and lightweight\nMinimal memory overhead\nGood for quick prototyping or very large-scale datasets\n\nCons:\n\nPerformance generally lags behind StringEncoder and TextEncoder\nLess nuanced feature representation\nLess robust across different types of data\n\n\n\n11.3.6 GapEncoder\nApproach: Estimates latent categories by finding common n-gram patterns across values, then encodes these patterns as numeric features.\nPros:\n\nInterpretable: Column names reflect the estimated categories\nCan group similar strings intelligently\nReasonable performance across datasets\n\nCons:\n\nSlower training time compared to StringEncoder and MinHashEncoder\nPerformance is on par with or slightly worse than the faster StringEncoder\nInterpretability comes at the cost of training speed\nMay require more computational resources for large datasets",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#conclusion",
    "href": "content/chapters/07_feat_eng_categorical.html#conclusion",
    "title": "11  Mixed data: dealing with categories",
    "section": "11.4 Conclusion",
    "text": "11.4 Conclusion\nEncoding categorical features is a critical step in preparing data for machine learning. The skrub library provides multiple encoders to handle different scenarios:\n\nStart with StringEncoder as a default for high-cardinality categorical features. It offers the best balance of speed, performance, and robustness across diverse datasets.\nUse OneHotEncoder for low-cardinality features (&lt; 40 unique values) to keep the feature space manageable.\nChoose TextEncoder if you’re working with true textual data (reviews, comments, descriptions) and have sufficient computational resources.\nConsider GapEncoder when interpretability is important and the additional training time can be dealt with.\nUse MinHashEncoder when you need maximum speed and are working with very large datasets.\n\nThe TableVectorizer integrates these encoders automatically, dispatching columns to the appropriate encoder based on their data type and cardinality. This automation makes it easy to process mixed-type datasets efficiently while still allowing fine-grained control when needed. By default, the TableVectorizer uses the OneHotEncoder for categorical features with cardinality &lt;= 40, and StringEncoder for categorical features with cardinality &gt; 40.\nFor a comprehensive empirical comparison of these methods, refer to the categorical encoders benchmark.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_03.html",
    "href": "content/chapters/quiz_03.html",
    "title": "12  Quiz: Column-level transformations",
    "section": "",
    "text": "13 Feature engineering",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_03.html#question-1",
    "href": "content/chapters/quiz_03.html#question-1",
    "title": "12  Quiz: Column-level transformations",
    "section": "13.1 Question 1",
    "text": "13.1 Question 1\n\n\n\n\n\n\nConsider this data with outliers. \nIn this plot, which line shows numerical features scaled by the SquashingScaler? \n\nA) The solid blue line\nB) The dashed green line\nC) The dotted purple line\nD) The dash-dotted red line\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer A)\nThe solid blue line",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_03.html#question-2",
    "href": "content/chapters/quiz_03.html#question-2",
    "title": "12  Quiz: Column-level transformations",
    "section": "13.2 Question 2",
    "text": "13.2 Question 2\n\n\n\n\n\n\nWhich of these is not a feature that can be added by the DatetimeEncoder?\n\nA) is_holiday: a boolean flag that checks whether a date is a holiday or not\nB) periodic_encoding: a flag that can be None, \"circular\" or \"spline\" and that adds the respective periodic encoding\nC) add_weekday: a boolean flag that indicates the day of the week\nD) add_total_seconds: a boolean flag that adds the number of seconds since epoch\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nAll other parameters are available to the DatetimeEncoder.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_03.html#question-3",
    "href": "content/chapters/quiz_03.html#question-3",
    "title": "12  Quiz: Column-level transformations",
    "section": "13.3 Question 3",
    "text": "13.3 Question 3\n\n\n\n\n\n\nWhich categorical encoder is the most balanced in both runtime and downstream performance?\n\nA) The StringEncoder\nB) The MinHashEncoder\nC) The TextEncoder\nD) The GapEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nThe StringEncoder is always one of the fastest encoders to fit, and produces high quality encodings. The MinHashEncoder is faster, but it produces worse encodings. The TextEncoder can produce the best encodings for certain types of categorical data (mainly text), but is very slow to run.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_03.html#question-4",
    "href": "content/chapters/quiz_03.html#question-4",
    "title": "12  Quiz: Column-level transformations",
    "section": "13.4 Question 4",
    "text": "13.4 Question 4\n\n\n\n\n\n\nYou need to do feature engineering on a dataset that includes user reviews as part of its features. You are working in an environment that has access to good computational resources, including a GPU. Which of the following encoders would be the best choice in such a scenario?\n\nA) OneHotEncoder\nB) TextEncoder\nC) OrdinalEncoder\nD) MinHashEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nThe TextEncoder works best when it is applied to free-flowing text, or columns whose content may find additional context in the training set of the language model used by the encoder. User reviews fall in this category.\nThe OneHotEncoder would likely generate a very large number of uninformative features, and the features prepared by the OrdinalEncoder are unlikely to be informative.\nThe MinHashEncoder would generate a fixed number of encoded features, but the quality of the embeddings is lower than that of the TextEncoder. In a situation where computational resources are limited, it may be beneficial, but even in that case the StringEncoder may produce better results.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html",
    "href": "content/chapters/08_feat_eng_table_vect.html",
    "title": "13  All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "13.1 Introduction\nMachine learning models typically require numeric input features. When working with real-world datasets, we often have a mix of data types: numbers, text, dates, and categorical values. The TableVectorizer automates the entire process of converting a heterogeneous dataframe into a matrix of numeric features ready for machine learning.\nInstead of manually specifying how to handle each column, the TableVectorizer automatically detects the data type of each column and applies the appropriate transformation to encode the column using numerical features.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#how-does-the-tablevectorizer-work",
    "href": "content/chapters/08_feat_eng_table_vect.html#how-does-the-tablevectorizer-work",
    "title": "13  All the pre-processing in one place: TableVectorizer",
    "section": "13.2 How does the TableVectorizer work?",
    "text": "13.2 How does the TableVectorizer work?\nThe TableVectorizer operates in two phases:\n\n13.2.1 Phase 1: Data Cleaning and Type Detection\nFirst, it runs a Cleaner on the input data to:\n\nDetect and parse datetime columns (possibly, with custom datetime formats)\nHandle missing values represented as strings (e.g., “N/A”)\nClean up categorical columns to have consistent typing\nRemove uninformative columns (those with only nulls, constant values, or all unique values)\nFinally, convert all numerical features to float32 to reduce the computational cost.\n\nThis ensures that each column has the correct data type before encoding.\n\n\n13.2.2 Phase 2: Column Dispatch and Encoding\nAfter cleaning, the TableVectorizer categorizes columns and dispatches them to the appropriate transformer based on their data type and cardinality. Categorical columns with a cardinality (i.e., number of unique values) larger than 40 (by default)) are considered “high cardinality”, while all other categorical columns are “low cardinality”.\nThe TableVectorizer uses the following default transformers for each column type:\n\nNumeric columns: Left untouched (passthrough) - they’re already in the right format\nDatetime columns: Transformed by DatetimeEncoder to extract meaningful temporal features\nLow-cardinality categorical/string columns: Transformed with OneHotEncoder to create binary indicator variables\nHigh-cardinality categorical/string columns: Transformed with StringEncoder to create dense numeric representations",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#key-parameters",
    "href": "content/chapters/08_feat_eng_table_vect.html#key-parameters",
    "title": "13  All the pre-processing in one place: TableVectorizer",
    "section": "13.3 Key Parameters",
    "text": "13.3 Key Parameters\n\n13.3.1 Cardinality Threshold\nThe cardinality threshold that splits columns in “high” and “low” cardinality can be changed by setting the relative parameter:\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(cardinality_threshold=10)  # Adjust the threshold\n\n\n\n13.3.2 Data Cleaning Parameters\nThe TableVectorizer forwards several parameters to the internal Cleaner, which behave in the same way:\n\ndrop_null_fraction: Fraction of nulls above which a column is dropped (default: 1.0)\ndrop_if_constant: Drop columns with only one unique value (default: False)\ndrop_if_unique: Drop string/categorical columns where all values are unique (default: False)\ndatetime_format: Format string for parsing dates\n\n\ntv = TableVectorizer(\n    drop_null_fraction=0.9,  # Drop columns that are 90% null\n    drop_if_constant=True,\n    datetime_format=\"%Y-%m-%d\"\n)\n\n\n\n13.3.3 Specifying custom transformers\nThe TableVectorizer applies whatever transformer is provided to each of the numeric, datetime, high_cardinality, and low_cardinality paramters. To tweak the default parameters of the transformers a new transformer should be provided:\n\nfrom skrub import TableVectorizer, DatetimeEncoder, StringEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Create custom transformers\ndatetime_enc = DatetimeEncoder(periodic_encoding=\"circular\")\nstring_enc = StringEncoder(n_components=10)\n\n# Pass them to TableVectorizer\ntv = TableVectorizer(\n    datetime=datetime_enc,\n    high_cardinality=string_enc,\n)\n\nThis allows to, for example, change the number of parameters in the StringEncoder, provide a custom datetime format for the DatetimeEncoder, or use a completely different encoder such as the TextEncoder.\n\n\n13.3.4 Applying the TableVectorizer only to a subset of columns\nBy default, the TableVectorizer is applied to all the columns in the given dataframe. In some cases, it may be important to keep specific columns “as is”, so that they are not modified by the transformer.\nThis can be done by wrapping the vectorizer into an ApplyToCols object.\nFor example, in this case we might want to avoid modifying the two *_id columns.\n\nimport pandas as pd\nfrom skrub import ApplyToCols\nimport skrub.selectors as s\n\ndf = pd.DataFrame(\n    {\n        \"metric_1\": [10.5, 20.3, 30.1, 40.2],\n        \"metric_2\": [5.1, 15.6, None, 35.8],\n        \"metric_3\": [1.1, 3.3, 2.6, .8],\n        \"num_id\": [101, 102, 103, 104],\n        \"str_id\": [\"A101\", \"A102\", \"A103\", \"A104\"],\n        \"description\": [\"apple\", None, \"cherry\", \"date\"],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\ndescription\nname\n\n\n\n\n0\n10.5\n5.1\n1.1\n101\nA101\napple\nAlice\n\n\n1\n20.3\n15.6\n3.3\n102\nA102\nNone\nBob\n\n\n2\n30.1\nNaN\n2.6\n103\nA103\ncherry\nCharlie\n\n\n3\n40.2\n35.8\n0.8\n104\nA104\ndate\nDavid\n\n\n\n\n\n\n\nWe can use ApplyToCols and the skrub selectors as follows:\n\ntv = ApplyToCols(TableVectorizer(), cols=s.all()-s.glob(\"*_id\"))\ndf_enc = tv.fit_transform(df)\n\nprint(\"Original\")\nprint(df[[\"num_id\", \"str_id\"]])\nprint(\"\\nEncoded\")\nprint(df_enc[[\"num_id\", \"str_id\"]])\n\nOriginal\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\nEncoded\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\n\nThe id strings are the same, while all other columns have been encoded as expected.\n\n\n13.3.5 Using specific_transformers for more low-level control\nFor fine-grained control, we can specify transformers for specific columns using the specific_transformers parameter. This is useful when we want to override the default behavior for particular columns:\n\nfrom sklearn.preprocessing import OrdinalEncoder\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"occupation\": [\"engineer\", \"teacher\", \"doctor\"],\n    \"salary\": [100000, 50000, 150000]\n})\n\n# Create a custom transformer for the 'occupation' column\nspecific_transformers = [(OrdinalEncoder(), [\"occupation\"])]\n\ntv = TableVectorizer(specific_transformers=specific_transformers)\nresult = tv.fit_transform(df)\n\nImportant notes about specific_transformers:\n\nColumns specified here bypass the default categorization logic\nThe transformer receives the column as-is, without any preprocessing\nThe transformer must be able to handle the column’s current data type and values\nFor more complex transformations, consider using ApplyToCols and the selectors API (explained in the previous chapters), or the skrub Data Ops.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#conclusions",
    "href": "content/chapters/08_feat_eng_table_vect.html#conclusions",
    "title": "13  All the pre-processing in one place: TableVectorizer",
    "section": "13.4 Conclusions",
    "text": "13.4 Conclusions\nThe TableVectorizer is a self-contained feature engineering engine that\n\nCleans your data to have consistent representation of data types and null values, and\nEncodes all columns depending on their data type and characteristics using good defaults.\n\nThe idea behind the TableVectorizer is that you should be able to provide any dataframe, and get a good feature matrix based on that dataframe as a result.\nThe TableVectorizer makes use of most of the objects that have been explained so far, and is an important part of the tabular_pipeline explained in the next chapter.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html",
    "href": "content/chapters/09_tabular_pipeline.html",
    "title": "14  Building a tabular pipeline",
    "section": "",
    "text": "14.1 Introduction\nUp until now we have covered how to clean data with the Cleaner, extract features from different column types, and handle categorical features with specialized encoders. In this section we will show how we can combine all these preprocessing techniques into a complete machine learning pipeline.\nA pipeline ensures that:\nIn this chapter, we explore two approaches: building custom pipelines with TableVectorizer, and using the tabular_pipeline function for quick, well-tuned baselines.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#introduction",
    "href": "content/chapters/09_tabular_pipeline.html#introduction",
    "title": "14  Building a tabular pipeline",
    "section": "",
    "text": "Data transformations are applied consistently across training and test sets\nData leakage is avoided by fitting transformers only on training data\nThe workflow is reproducible and deployable\nPreprocessing steps are properly chained together",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "href": "content/chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "title": "14  Building a tabular pipeline",
    "section": "14.2 Manual pipeline construction with TableVectorizer",
    "text": "14.2 Manual pipeline construction with TableVectorizer\nThe TableVectorizer can be the foundation of a custom scikit-learn pipeline, where cleaning and feature engineering are dealt with by a single object. Scaling and imputation are not required by all models, so they are not in the TableVectorizer’s scope.\nWe combine it with other preprocessing steps and a final estimator:\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skrub import TableVectorizer\n\nmodel = make_pipeline(\n    TableVectorizer(),           # Feature engineering\n    SimpleImputer(),             # Handle missing values\n    StandardScaler(),            # Normalize features\n    LogisticRegression()         # Final estimator\n)\nThis approach gives complete control over which preprocessing steps to use and in what order. We can customize the TableVectorizer parameters (cardinality threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\nIn the case of the example we used LogisticRegression as our estimator, but if we used a different estimator, such as the HistogramGradientBoostingClassifier, the scaling and imputation steps could have been avoided.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#the-tabular_pipeline",
    "href": "content/chapters/09_tabular_pipeline.html#the-tabular_pipeline",
    "title": "14  Building a tabular pipeline",
    "section": "14.3 The tabular_pipeline",
    "text": "14.3 The tabular_pipeline\nFor many common use cases, we can skip the manual pipeline construction and use the tabular_pipeline function. This function automatically creates an appropriate pipeline based on the estimator we provide:\nfrom skrub import tabular_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a complete pipeline for a specific estimator\nmodel = tabular_pipeline(LogisticRegression())\nOr, we can use a string to get a pre-configured pipeline with a default estimator:\n# Classification with HistGradientBoostingClassifier\nmodel = tabular_pipeline('classification')\n\n# Regression with HistGradientBoostingRegressor\nmodel = tabular_pipeline('regression')",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "href": "content/chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "title": "14  Building a tabular pipeline",
    "section": "14.4 How tabular_pipeline adapts to different estimators",
    "text": "14.4 How tabular_pipeline adapts to different estimators\nThe tabular_pipeline function intelligently configures the preprocessing pipeline based on the estimator type:\n\n14.4.1 For linear models (e.g., LogisticRegression, Ridge)\n\nTableVectorizer: Uses the default configuration, except for the addition of spline-encoded datetime features by the DatetimeEncoder\nSimpleImputer: Added because linear models cannot handle missing values\nSquashingScaler: Normalizes numeric features to improve convergence and performance\nEstimator: The provided linear model\n\nThis configuration ensures numeric features are properly scaled and missing values are handled appropriately.\n\n\n14.4.2 For tree-based ensemble models (RandomForest, HistGradientBoosting)\n\nTableVectorizer: Configured specifically for tree models\n\nLow-cardinality categorical features: Either kept as categorical (HistGradientBoosting) or ordinal encoded (RandomForest)\nHigh-cardinality features: StringEncoder for robust feature extraction\nDatetime features: No spline encoding (unnecessary for trees)\n\nScaler: Not added (unnecessary for tree-based models)\nEstimator: The provided tree-based estimator\n\nThis configuration leverages the native capabilities of tree models while still providing effective feature engineering through the StringEncoder.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#conclusions-why-the-tabular_pipeline-is-useful",
    "href": "content/chapters/09_tabular_pipeline.html#conclusions-why-the-tabular_pipeline-is-useful",
    "title": "14  Building a tabular pipeline",
    "section": "14.5 Conclusions: why the tabular_pipeline is useful",
    "text": "14.5 Conclusions: why the tabular_pipeline is useful\n\nSmart Configuration: Automatically selects preprocessing parameters appropriate for the estimator\nSimplicity: One-line creation of a complete, well-tuned baseline\nRobustness: Handles edge cases like missing values and mixed data types automatically\n\n\nUse tabular_pipeline when you want a quick, well-tuned baseline to benchmark against or as a starting point\nBuild manual pipelines when you need fine-grained control over preprocessing steps or want to experiment with custom transformers\n\nBoth approaches produce scikit-learn compatible pipelines that can be used with cross-validation, hyperparameter tuning, and other standard workflows.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_04.html",
    "href": "content/chapters/quiz_04.html",
    "title": "15  Quiz: Column-level transformations",
    "section": "",
    "text": "16 TableVectorizer",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_04.html#question-1",
    "href": "content/chapters/quiz_04.html#question-1",
    "title": "15  Quiz: Column-level transformations",
    "section": "16.1 Question 1",
    "text": "16.1 Question 1\n\n\n\n\n\n\nWhat is the proper way of setting the number of components used by the high cardinality encoder in the TableVectorizer?\n\nA) By passing n_components as kwargs:\n\nTableVectorizer(n_components=10)\n\nB) By creating a new encoder with the specified parameter:\n\nnew_encoder = StringEncoder(n_components=10)\nTableVectorizer(high_cardinality=new_encoder)\n\nC) Both A) and B) are correct\nD) None of the above\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_04.html#question-2",
    "href": "content/chapters/quiz_04.html#question-2",
    "title": "15  Quiz: Column-level transformations",
    "section": "16.2 Question 2",
    "text": "16.2 Question 2\n\n\n\n\n\n\nThe TableVectorizer categorizes columns based on their characteristics. What are the types of columns that it handles?\n\nA) Numeric, categorical, datetimes\nB) Numeric, datetimes, high cardinality categorical, low cardinality categorical\nC) Integer, float, string, categorical, datetimes\nD) Numeric, datetimes, high cardinality categorical, low cardinality categorical, and complex objects such as structs or lists\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nBy default, the TableVectorizer converts complex objects to their string representation, before checking their cardinality. No special transformation is provided for columns that contain structs or lists.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_04.html#question-3",
    "href": "content/chapters/quiz_04.html#question-3",
    "title": "15  Quiz: Column-level transformations",
    "section": "17.1 Question 3",
    "text": "17.1 Question 3\n\n\n\n\n\n\nWhat are the default estimators used by the tabular_pipeline when it is called with \"regression\" and \"classification\" respectively?\ntabular_pipeline(\"regression\")\ntabular_pipeline(\"classification\")\n\nA) RidgeCV and ExtraTreeClassifier\nB) HistGradientBoostingRegressor and LogisticRegression\nC) RandomForestRegressor and RandomForestClassifier\nD) HistGradientBoostingRegressor and HistGradientBoosting\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: D)",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_04.html#question-4",
    "href": "content/chapters/quiz_04.html#question-4",
    "title": "15  Quiz: Column-level transformations",
    "section": "17.2 Question 4",
    "text": "17.2 Question 4\n\n\n\n\n\n\nHow are linear models (Ridge, LogisticRegression) handled by the tabular_pipeline?\n\nA) The TableVectorizer uses default parameters, no other steps are added to the pipeline.\nB) The TableVectorizer uses default parameters, SimpleImputer and StandardScaler are added before the encoder.\nC) The TableVectorizer uses a datetime encoder that periodic encodings with splines, SimpleImputer and StandardScaler are added before the encoder.\nD) The TableVectorizer uses a datetime encoder that periodic encodings with splines, no other steps are added to the pipeline.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: C)",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quiz: Column-level transformations</span>"
    ]
  }
]