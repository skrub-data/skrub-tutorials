[
  {
    "objectID": "content/notebooks/04_ex_table_vec.html",
    "href": "content/notebooks/04_ex_table_vec.html",
    "title": "Exercise: implementing a TableVectorizer from its components",
    "section": "",
    "text": "Replicate the behavior of a TableVectorizer using ApplyToCols, the skrub selectors, and the given transformers.\n\nfrom skrub import Cleaner, ApplyToCols, StringEncoder, DatetimeEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\nNotes on the implementation:\n\nIn the first step, the TableVectorizer cleans the data to parse datetimes and other dtypes.\nNumeric features are left untouched, i.e., they use a Passthrough transformer.\nString and categorical feature are split into high and low cardinality features.\nFor this exercise, set the the cardinality threshold to 4.\nHigh cardinality features are transformed with a StringEncoder. In this exercise, set n_components to 2.\nLow cardinality features are transformed with a OneHotEncoder, and the first category in binary features is dropped (hint: check the docs of the OneHotEncoder for the drop parameter). Set sparse_output=True.\nRemember cardinality_below is one of the skrub selectors.\nDatetimes are transformed by a default DatetimeEncoder.\nEverything should be wrapped in a scikit-learn Pipeline.\n\nUse the following dataframe to test the result.\n\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", \"private\", \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"datetime-col\": [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n    ]\n    + [None],\n}\ndf = pd.DataFrame(data)\ndf\n\nUse the following PassThrough transformer where needed.\n\nfrom skrub._apply_to_cols import SingleColumnTransformer\nclass PassThrough(SingleColumnTransformer):\n    def fit_transform(self, column, y=None):\n        return column\n\n    def transform(self, column):\n        return column\n\nYou can test the correctness of your solution by comparing it with the equivalent TableVectorizer:\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(\n    high_cardinality=StringEncoder(n_components=2), cardinality_threshold=4\n)\ntv.fit_transform(df)\n\n\n# Write your code here\n#\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n\n\n# Solution\ncleaner = ApplyToCols(Cleaner(numeric_dtype=\"float32\"))\nhigh_cardinality = ApplyToCols(\n    StringEncoder(n_components=2), cols=~s.cardinality_below(4) & (s.string())\n)\nlow_cardinality = ApplyToCols(\n    OneHotEncoder(sparse_output=False, drop=\"if_binary\"),\n    cols=s.cardinality_below(4) & s.string(),\n)\nnumeric = ApplyToCols(PassThrough(), cols=s.numeric())\ndatetime = ApplyToCols(DatetimeEncoder(), cols=s.any_date())\n\nmy_table_vectorizer = make_pipeline(\n    cleaner, numeric, high_cardinality, low_cardinality, datetime\n)\n\nmy_table_vectorizer.fit_transform(df)"
  },
  {
    "objectID": "content/notebooks/02_ex_selectors.html",
    "href": "content/notebooks/02_ex_selectors.html",
    "title": "Exercise: using selectors together with ApplyToCols",
    "section": "",
    "text": "Consider this example dataframe:\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"metric_1\": [10.5, 20.3, 30.1, 40.2],\n        \"metric_2\": [5.1, 15.6, None, 35.8],\n        \"metric_3\": [1.1, 3.3, 2.6, .8],\n        \"num_id\": [101, 102, 103, 104],\n        \"str_id\": [\"A101\", \"A102\", \"A103\", \"A104\"],\n        \"description\": [\"apple\", None, \"cherry\", \"date\"],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    }\n)\ndf\n\nUsing the skrub selectors and ApplyToCols:\n\nApply the StandardScaler to numeric columns, except \"num_id\".\nApply a OneHotEncoder with sparse_output=False on all string columns except \"str_id\".\n\n\nimport skrub.selectors as s\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom skrub import ApplyToCols\nfrom sklearn.pipeline import make_pipeline\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n\n\nimport skrub.selectors as s\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom skrub import ApplyToCols\nfrom sklearn.pipeline import make_pipeline\n\nscaler = ApplyToCols(StandardScaler(), cols=s.numeric() - \"num_id\")\none_hot = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string() - \"str_id\")\n\ntransformer = make_pipeline(scaler, one_hot)\n\ntransformer.fit_transform(df)\n\nGiven the same dataframe and using selectors, drop only string columns that contain nulls.\n\nfrom skrub import DropCols\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n\n\nfrom skrub import DropCols\n\nDropCols(cols=s.has_nulls() & s.string()).fit_transform(df)\n\nNow write a custom function that selects columns where all values are lower than 10.0.\n\nfrom skrub import SelectCols\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n\n\nfrom skrub import SelectCols\n\ndef lower_than(col):\n    return all(col &lt; 10.0)\n\nSelectCols(cols=s.numeric() & s.filter(lower_than)).fit_transform(df)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inria Academy - skrub like a pro",
    "section": "",
    "text": "This is the website for the Inria Academy course on the skrub package: it contains all the material used for the course, including the datasets and exercises used during the session.\nChoose your learning format:"
  },
  {
    "objectID": "index.html#book-format",
    "href": "index.html#book-format",
    "title": "Inria Academy - skrub like a pro",
    "section": "üìñ Book Format",
    "text": "üìñ Book Format\nComplete course material in book format with detailed explanations.\nAccess the Book ‚Üí"
  },
  {
    "objectID": "index.html#presentation-slides",
    "href": "index.html#presentation-slides",
    "title": "Inria Academy - skrub like a pro",
    "section": "üéØ Presentation Slides",
    "text": "üéØ Presentation Slides\nSlide-based presentation for instructor-led sessions.\nView Slides ‚Üí"
  },
  {
    "objectID": "index.html#interactive-exercises",
    "href": "index.html#interactive-exercises",
    "title": "Inria Academy - skrub like a pro",
    "section": "üíª Interactive Exercises",
    "text": "üíª Interactive Exercises\nTry the exercises in your browser with JupyterLite (no installation required).\nLaunch JupyterLite ‚Üí"
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "Inria Academy - skrub like a pro",
    "section": "Structure of the course",
    "text": "Structure of the course\nThe course covers the main features of skrub, from data exploration to pipeline construction, with the notable exclusion of the Data Ops.\nEach chapter includes a section that describes how a specific feature may assist in building a machine learning pipeline, along with practical code examples.\nSome chapters include exercises for participants to work with the explained features. These exercises are made available in content/exercises, as well as at the end of the respective lesson in content/notebooks.\nThe content of the book is split in sections, and each section includes a ‚Äúfinal quiz‚Äù that covers the subjects covered up to that point."
  },
  {
    "objectID": "index.html#setting-up-a-local-environment",
    "href": "index.html#setting-up-a-local-environment",
    "title": "Inria Academy - skrub like a pro",
    "section": "Setting up a local environment",
    "text": "Setting up a local environment\n\n\n\n\n\n\nImportantNavigating the repository\n\n\n\nDepending on how you launch the instance of Jupyter lab, you might start it in the root folder.\nAll notebooks used in the course are found in content/notebooks, while the exercises are in content/exercises.\n\n\n\nUsing pixi\nThe easiest way to set up the environment is by installing and using pixi. Follow the platform-specific instructions in the link to install pixi, then open a terminal window in the folder of the repository you cloned.\nRun\npixi install\nto create the environment, followed by\npixi run lab\nto start a Jupyter lab instance.\n\n\nUsing pip\nCreate the and activate the environment:\npython -m venv skrub-tutorial\nsource skrub-tutorial/bin/activate\nInstall the required dependencies using the requirements.txt file:\npip install -r requirements.txt\nStart the Jupyter lab instance:\njupyter lab\n\n\nUsing conda\nAn environment.yaml file is provided to create a conda environment.\nCreate and activate the environment with\nconda env create -f environment.yaml\nconda activate skrub-tutorial\nThen, start a jupyter lab instance:\njupyter lab\n\n\nUsing uv\nCreate the environment using pyproject.toml as the requirement file.\nuv venv \nuv pip install -r pyproject.toml\nActivate the environment that was created in the folder.\nsource .venv/bin/activate\nStart the Jupyter lab instance:\njupyter lab"
  },
  {
    "objectID": "content/notebooks/01_ex_explore_clean.html",
    "href": "content/notebooks/01_ex_explore_clean.html",
    "title": "Exercise: exploring a new table",
    "section": "",
    "text": "For this exercise, we will use the employee_salaries dataframe to answer some questions.\nRun the following code to import the dataframe:\nimport pandas as pd\ndata = pd.read_csv(\"../data/employee_salaries/data.csv\")\nNow use the skrub TableReport and answer the following questions:\nfrom skrub import TableReport\nTableReport(data)"
  },
  {
    "objectID": "content/notebooks/01_ex_explore_clean.html#questions",
    "href": "content/notebooks/01_ex_explore_clean.html#questions",
    "title": "Exercise: exploring a new table",
    "section": "Questions",
    "text": "Questions\n\nWhat‚Äôs the size of the dataframe? (columns and rows)\nHow many columns have object/numerical/datetime\nAre there columns with a large number of missing values?\nAre there columns that have a high cardinality (&gt;40 unique values)?\nWere datetime columns parsed correctly?\nWhich columns have outliers?\nWhich columns have an imbalanced distribution?\nWhich columns are strongly correlated with each other?\n\n# PLACEHOLDER\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "content/notebooks/01_ex_explore_clean.html#answers",
    "href": "content/notebooks/01_ex_explore_clean.html#answers",
    "title": "Exercise: exploring a new table",
    "section": "Answers",
    "text": "Answers\n\nWhat‚Äôs the size of the dataframe? (columns and rows)\n\n9228 rows √ó 8 columns\n\nHow many columns have object/numerical/datetime\n\nNo datetime columns, one integer column (year_first_hired), all other columns are objects.\n\nAre there columns with a large number of missing values?\n\nNo, only the gender column contains a small fraction (0.2%) of missing values.\n\nAre there columns that have a high cardinality?\n\nYes, division, employee_position_title, date_first_hired have a cardinality larger than 40.\n\nWere datetime columns parsed correctly?\n\nNo, the date_first_hired column has dtype Object.\n\nWhich columns have outliers?\n\nNo columns seem to include outliers.\n\nWhich columns have an imbalanced distribution?\n\nassignment_category has an unbalanced distribution.\n\nWhich columns are strongly correlated with each other?\n\ndepartment and department_name have a Cramer‚Äôs V of 1, so they are very strongly correlated."
  },
  {
    "objectID": "content/notebooks/03_ex_feat_eng.html",
    "href": "content/notebooks/03_ex_feat_eng.html",
    "title": "Exercise",
    "section": "",
    "text": "Use one of the methods explained so far (Cleaner/ApplyToCols) to convert the provided dataframe to datetime dtype, then extract the following features: - All parts of the datetime - The number of seconds from epoch - The day in the week - The day of the year\nHint: use the format \"%d %B %Y\" for the datetime.\n\nimport pandas as pd\n\ndata = {\n    \"admission_dates\": [\n        \"03 January 2023\",\n        \"15 February 2023\",\n        \"27 March 2023\",\n        \"10 April 2023\",\n    ],\n    \"patient_ids\": [101, 102, 103, 104],\n    \"age\": [25, 34, 45, 52],\n    \"outcome\": [\"Recovered\", \"Under Treatment\", \"Recovered\", \"Deceased\"],\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n\n\n# Solution with ApplyToCols and ToDatetime\nfrom skrub import ApplyToCols, ToDatetime, DatetimeEncoder\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\nto_datetime_encoder = ApplyToCols(ToDatetime(format=\"%d %B %Y\"), cols=\"admission_dates\")\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(to_datetime_encoder, datetime_encoder)\nencoder.fit_transform(df)\n\n\n# Solution with Cleaner\nfrom skrub import Cleaner\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(Cleaner(datetime_format=\"%d %B %Y\"), datetime_encoder)\nencoder.fit_transform(df)\n\nModify the script so that the DatetimeEncoder adds periodic encoding with sine and cosine (aka circular encoding):\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n\nNow modify the script above to add spline features (periodic_encoding=\"spline\").\n\n# Solution\nfrom skrub import Cleaner\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(\n        periodic_encoding=\"spline\",\n        add_total_seconds=True,\n        add_weekday=True,\n        add_day_of_year=True,\n    ),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(Cleaner(datetime_format=\"%d %B %Y\"), datetime_encoder)\nencoder.fit_transform(df)"
  }
]