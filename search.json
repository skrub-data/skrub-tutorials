[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub tutorials",
    "section": "",
    "text": "0.1 Prepration and setup\nThis website contains the material used for the Inria Academy course on the skrub package.\nEach chapter of the book includes a theoretical section that describes a specific aspect of the library, and, if appropriate, an exercise to evaluate the theory.\nQuiz sections split the major sections of the course.\nThe easiest way to work with the notebooks provided here is by installing and using pixi.\nThen, it is possible to run\nto create the environment, followed by\nto start a jupyter lab instance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "index.html#prepration-and-setup",
    "href": "index.html#prepration-and-setup",
    "title": "Skrub tutorials",
    "section": "",
    "text": "pixi install\n\npixi run jupyter lab\n\n\n0.1.1 With conda\nAn environment.yaml file is provided to create a conda environment.\nCreate and activate the environment with\nconda env create -f environment.yaml\nconda activate skrub-tutorial\nThen, start a jupyter lab instance:\njupyter lab\n\n\n0.1.2 With uv\nuv venv \nuv pip install -r pyproject.toml\nsource .venv/bin/activate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html",
    "href": "content/chapters/00_intro.html",
    "title": "2  A world without skrub",
    "section": "",
    "text": "2.1 Strategizing\nLet’s begin the lesson by imagining a world without skrub, where we can use only Pandas and scikit-learn to clean data and prepare a machine learning model.\nLet’s take a look at the target::\nThis is a numerical column, and our task is predicting the value of current_annual_salary.\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000\nWe need to:\nOnce we have processed the data, we can train a machine learning model. For the sake of the example, we will use a linear model (Ridge), which means that we need to scale numerical features, besides imputing missing values.\nFinally, we want to evaluate the performance of the method across multiple cross-validation splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#strategizing",
    "href": "content/chapters/00_intro.html#strategizing",
    "title": "2  A world without skrub",
    "section": "",
    "text": "Impute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "href": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "title": "2  A world without skrub",
    "section": "2.2 Building a traditional pipeline",
    "text": "2.2 Building a traditional pipeline\nLet’s build a traditional predictive pipeline following the steps we just discussed.\n\n2.2.1 Step 1: Convert date features to numerical\nExtract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['years_since_hired'] = (pd.Timestamp.now() - X_processed['date_first_hired']).dt.days / 365.25\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 10)\n\n\n\n\n2.2.2 Step 2: Encode categorical features\nEncode only the non-numerical categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1219)\n\n\n\n\n2.2.3 Step 3: Impute missing values\nWe’ll impute missing values in the gender column using the most frequent strategy.\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)\n\n\n\n2.2.4 Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)\n\n\n\n2.2.5 Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10366.9520 (+/- 1403.5225)\n\n\n\n\n2.2.6 “Just ask an agent to write the code”\nIt’s what I did. Here are some of the issues I noticed:\n\nOperations in the wrong order.\nTrying to impute categorical features without encoding them as numerical values.\nThe datetime feature was encoded as a categorical (i.e, with dummmies).\nToo many print statements.\nCells could not be executed in order without proper debugging and re-prompting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "href": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "title": "2  A world without skrub",
    "section": "2.3 Waking up from a nightmare",
    "text": "2.3 Waking up from a nightmare\nThankfully, we live in a world where we can import skrub. Let’s see what we can get if we use skrub.tabular_pipeline.\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.9085 (+/- 0.0149)\nMean test RMSE: 8793.9844 (+/- 994.1213)\n\n\nAll the code from before, the tokens and the debugging are replaced by a single import that gives better results.\nThroughout the tutorial, we will see how each step can be simplified, replaced, or improved using skrub features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html",
    "href": "content/chapters/01_exploring_data.html",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "3.1 Introduction\nIn this notebook, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table, and perform some exploratory analysis to learn about the characteristics of the data.\nFirst, let’s import the necessary libraries and load the dataset.\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom skrub import TableReport\nfrom sklearn.datasets import fetch_openml\n# Load the Adult Census dataset from OpenML\nadult = fetch_openml(name='adult', version=2, as_frame=True)\ndata = adult.data\ntarget = adult.target\nNow that we have a dataframe we can work with, we would like to find out:",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#introduction",
    "href": "content/chapters/01_exploring_data.html#introduction",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "The size of the dataset.\nThe data types and names of the columns.\nThe distribution of values in the columns.\nWhether null values are present, in what measure and where.\nDiscrete/categorical features, and their cardinality.\nColumns strongly correlated with each other.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "href": "content/chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.2 Exploring data with Pandas tools",
    "text": "3.2 Exploring data with Pandas tools\nFor the sake of the example, let’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\n\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n\n\n\n\n\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   age             48842 non-null  int64   \n 1   workclass       46043 non-null  category\n 2   fnlwgt          48842 non-null  int64   \n 3   education       48842 non-null  category\n 4   education-num   48842 non-null  int64   \n 5   marital-status  48842 non-null  category\n 6   occupation      46033 non-null  category\n 7   relationship    48842 non-null  category\n 8   race            48842 non-null  category\n 9   sex             48842 non-null  category\n 10  capital-gain    48842 non-null  int64   \n 11  capital-loss    48842 non-null  int64   \n 12  hours-per-week  48842 non-null  int64   \n 13  native-country  47985 non-null  category\ndtypes: category(8), int64(6)\nmemory usage: 2.6 MB\n\n\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\n\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\n\n\n\n\n\n\n\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.3 Exploring data with the TableReport",
    "text": "3.3 Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nTableReport(data)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n3.3.1 Default view of the TableReport\nThe TableReport gives us a comprehensive overview of the dataset. The default view shows all the columns in the dataset, and allows to select and copy the content of the cells shown in the preview.\nThe TableReport is intended to show a preview of the data, so it does not contain all the rows in the dataset, rather it shows only the first and last few rows by default.\n\n\n3.3.2 The “Stats” tab\n\nTableReport(data, open_tab=\"stats\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Stats” tab provides a variety of descriptive statistics for each column in the dataset. This includes:\n\nThe column name\nThe detected data type of the column\nWhether the column is sorted or not\nThe number of null values in the column, as well as the percentage\nThe number of unique values in the column\n\nFor numerical columns, additional statistics are provided:\n\nMean\nStandard deviation\nMinimum and maximum values\nMedian\n\n\n\n3.3.3 The “Distributions” tab\n\nTableReport(data, open_tab=\"distributions\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Distributions” tab provides visualizations of the distributions of values in each column. This includes histograms for numerical columns and bar plots for categorical columns.\nThe “Distributions” tab helps with detecting potential issues in the data, such as:\n\nSkewed distributions\nOutliers\nUnexpected value frequencies\n\nFor example, in this dataset we can see that some columns are heavily skewed, such as “workclass”, “race”, and “native-country”: this is important information to keep track of, because these columns may require special handling during data preprocessing or modeling.\nAdditionally, the “Distributions” tab allows so select columns manually, so that they can be added to a script and selected for further analysis or modeling.\n\n\n\n\n\n\nCaution\n\n\n\nThe TableReport detects outliers using a simple interquartile test, marking as outliers all values that are beyond the IQR. This is a simple heuristic, and should not be treated as perfect. If your problem requires reliable outlier detection, you should not rely exclusively on what the TableReport shows.\n\n\n\n\n3.3.4 The “Associations” tab\n\nTableReport(data, open_tab=\"associations\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Associations” tab provides insights into the relationships between different columns in the dataset. It shows Pearson’s correlation coefficients for numerical columns, as well as Cramér’s V for all columns.\nWhile this is a somewhat rough measure of association, it can help identify potential relationships worth exploring further during the analysis, and highlights highly correlated columns: depending on the modeling technique used, these may need to be handled specially to avoid issues with multicollinearity.\nIn this example, we can see that “education-num” and “education” have perfect correlation, which means that one of the two columns can be dropped without losing information.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "href": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.4 Exploring the target variable",
    "text": "3.4 Exploring the target variable\nLet’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.5 Configuring and saving the TableReport",
    "text": "3.5 Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console.\nIt is possible to configure various parameters using the skrub global config. For example, it is possible to replace the default Pandas or Polars dataframe display with the TableReport as follows:\n\nfrom skrub import set_config\nset_config(use_table_report=True)\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "href": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.6 Working with big tables",
    "text": "3.6 Working with big tables\nPlotting and measuring the column correlations are expensive operations, so when the dataframe under study is large it may be more convenient to skip them, as generating the Distributions and Associations tab may take a long time.\nThe max_plot_columns and max_association_columns parameters allow to set a threshold on the number of columns: the TableReport will skip the respective task if the number of colums in the dataframe is larger than the threshold:\n\nTableReport(data, max_association_columns=3, max_plot_columns=3)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhen the number of columns is too large, an error message is shown in the respective tab instead of the plots or correlations.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#questions",
    "href": "content/chapters/01_exploring_data.html#questions",
    "title": "3  Exploring dataframes with skrub",
    "section": "4.1 Questions",
    "text": "4.1 Questions\n\nWhat’s the size of the dataframe? (columns and rows)\nHow many columns have object/numerical/datetime\nAre there columns with a large number of missing values?\nAre there columns that have a high cardinality (&gt;40 unique values)?\nWere datetime columns parsed correctly?\nWhich columns have outliers?\nWhich columns have an imbalanced distribution?\nWhich columns are strongly correlated with each other?\n\n\n4.1.1 Answers\n\nWhat’s the size of the dataframe? (columns and rows)\n\n9228 rows × 8 columns\n\nHow many columns have object/numerical/datetime\n\nNo datetime columns, one integer column (year_first_hired), all other columns are objects.\n\nAre there columns with a large number of missing values?\n\nNo, only the gender column contains a small fraction (0.2%) of missing values.\n\nAre there columns that have a high cardinality?\n\nYes, division, employee_position_title, date_first_hired have a cardinality larger than 40.\n\nWere datetime columns parsed correctly?\n\nNo, the date_first_hired column has dtype Object.\n\nWhich columns have outliers?\n\nNo columns seem to include outliers.\n\nWhich columns have an imbalanced distribution?\n\nassignment_category has an unbalanced distribution.\n\nWhich columns are strongly correlated with each other?\n\ndepartment and department_name have a Cramer’s V of 1, so they are very strongly correlated.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html",
    "href": "content/chapters/02_cleaning_data.html",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "",
    "text": "4.1 Introduction\nIn this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner.\nWe first load the wine datasets from OpenML.\nfrom sklearn.datasets import fetch_openml\nfrom skrub import TableReport\nimport pandas as pd\n\ndata = fetch_openml(data_id=42074)\nwine = data.data\nWe can explore it using the TableReport:\nTableReport(wine)\n\nProcessing column   1 / 10Processing column   2 / 10Processing column   3 / 10Processing column   4 / 10Processing column   5 / 10Processing column   6 / 10Processing column   7 / 10Processing column   8 / 10Processing column   9 / 10Processing column  10 / 10\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\nWe can notice that there are a few columns that contain a sizable amount of missing values (“region_2” and “designation”). If we want to remove these columns programmatically using pandas, we have to do something like this:\nwine.loc[:, wine.isnull().mean() &lt;= 0.3]\n\n\n\n\n\n\n\n\ncountry\ndescription\npoints\nprice\nprovince\nregion_1\nvariety\nwinery\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\n96\n235.0\nCalifornia\nNapa Valley\nCabernet Sauvignon\nHeitz\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\n96\n110.0\nNorthern Spain\nToro\nTinta de Toro\nBodega Carmen Rodríguez\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\n96\n90.0\nCalifornia\nKnights Valley\nSauvignon Blanc\nMacauley\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\n96\n65.0\nOregon\nWillamette Valley\nPinot Noir\nPonzi\n\n\n4\nFrance\nThis is the top wine from La Bégude, named aft...\n95\n66.0\nProvence\nBandol\nProvence red blend\nDomaine de la Bégude\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n150925\nItaly\nMany people feel Fiano represents southern Ita...\n91\n20.0\nSouthern Italy\nFiano di Avellino\nWhite Blend\nFeudi di San Gregorio\n\n\n150926\nFrance\nOffers an intriguing nose with ginger, lime an...\n91\n27.0\nChampagne\nChampagne\nChampagne Blend\nH.Germain\n\n\n150927\nItaly\nThis classic example comes from a cru vineyard...\n91\n20.0\nSouthern Italy\nFiano di Avellino\nWhite Blend\nTerredora\n\n\n150928\nFrance\nA perfect salmon shade, with scents of peaches...\n90\n52.0\nChampagne\nChampagne\nChampagne Blend\nGosset\n\n\n150929\nItaly\nMore Pinot Grigios should taste like this. A r...\n90\n15.0\nNortheastern Italy\nAlto Adige\nPinot Grigio\nAlois Lageder\n\n\n\n\n150930 rows × 8 columns\nIt may also be beneficial to convert numerical features to float32, to reduce the computational cost:\nwine.astype({col: \"float32\" for col in wine.select_dtypes(include=\"number\").columns})\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96.0\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\nCarodorum Selección Especial Reserva\n96.0\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\nSpecial Selected Late Harvest\n96.0\n90.0\nCalifornia\nKnights Valley\nSonoma\nSauvignon Blanc\nMacauley\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\nReserve\n96.0\n65.0\nOregon\nWillamette Valley\nWillamette Valley\nPinot Noir\nPonzi\n\n\n4\nFrance\nThis is the top wine from La Bégude, named aft...\nLa Brûlade\n95.0\n66.0\nProvence\nBandol\nNaN\nProvence red blend\nDomaine de la Bégude\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n150925\nItaly\nMany people feel Fiano represents southern Ita...\nNaN\n91.0\n20.0\nSouthern Italy\nFiano di Avellino\nNaN\nWhite Blend\nFeudi di San Gregorio\n\n\n150926\nFrance\nOffers an intriguing nose with ginger, lime an...\nCuvée Prestige\n91.0\n27.0\nChampagne\nChampagne\nNaN\nChampagne Blend\nH.Germain\n\n\n150927\nItaly\nThis classic example comes from a cru vineyard...\nTerre di Dora\n91.0\n20.0\nSouthern Italy\nFiano di Avellino\nNaN\nWhite Blend\nTerredora\n\n\n150928\nFrance\nA perfect salmon shade, with scents of peaches...\nGrand Brut Rosé\n90.0\n52.0\nChampagne\nChampagne\nNaN\nChampagne Blend\nGosset\n\n\n150929\nItaly\nMore Pinot Grigios should taste like this. A r...\nNaN\n90.0\n15.0\nNortheastern Italy\nAlto Adige\nNaN\nPinot Grigio\nAlois Lageder\n\n\n\n\n150930 rows × 10 columns\nThese operations are quite common in most cases (although the parameters and requirements may vary by project), so writing the code that addresses them may become repetitive.\nA simpler way of dealing with this preliminary preparation is to use the skrub Cleaner.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "href": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "4.2 Using the skrub Cleaner",
    "text": "4.2 Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column:\nConsider this example dataframe:\n\ndf = pd.DataFrame(\n    {\n        \"numerical_1\": [1, 2, 3, 4, 5],\n        \"numerical_2\": [10.5, 20.3, None, 40.1, 50.2],\n        \"string_column\": [\"apple\", \"?\", \"banana\", \"cherry\", \"?\"],\n        \"datetime_column\": [\n            \"03 Jan 2020\",\n            \"04 Jan 2020\",\n            \"05 Jan 2020\",\n            \"06 Jan 2020\",\n            \"07 Jan 2020\",\n        ],\n        \"all_none\": [None, None, None, None, None],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\nall_none\n\n\n\n\n0\n1\n10.5\napple\n03 Jan 2020\nNone\n\n\n1\n2\n20.3\n?\n04 Jan 2020\nNone\n\n\n2\n3\nNaN\nbanana\n05 Jan 2020\nNone\n\n\n3\n4\n40.1\ncherry\n06 Jan 2020\nNone\n\n\n4\n5\n50.2\n?\n07 Jan 2020\nNone\n\n\n\n\n\n\n\nThis dataframe has mixed type columns, with some of the missing values denoted as None and some \"?\". The datetime column has a non-standard format and has been parsed as a string column. Finally, one of the columns is completely empty.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   numerical_1      5 non-null      int64  \n 1   numerical_2      4 non-null      float64\n 2   string_column    5 non-null      object \n 3   datetime_column  5 non-null      object \n 4   all_none         0 non-null      object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 332.0+ bytes\n\n\nBy default, the Cleaner applies various transformations that can sanitize many common use cases:\n\nfrom skrub import Cleaner\ndf_clean = Cleaner().fit_transform(df)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNone\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNone\n2020-01-07\n\n\n\n\n\n\n\nWe can see that the cleaned version of the dataframe is now marking missing values correctly, and that the datetime column has been parsed accordingly:\n\ndf_clean.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   numerical_1      5 non-null      int64         \n 1   numerical_2      4 non-null      float64       \n 2   string_column    3 non-null      object        \n 3   datetime_column  5 non-null      datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\n\n4.2.1 Cleaning steps performed by the Cleaner\nIn more detail, the Cleaner executes the following steps in order:\n\nIt replaces common strings used to represent missing values (e.g., NULL, ?) with NA markers.\nIt uses the DropUninformative transformer to decide whether a column is “uninformative”, that is, it is not likely to bring information useful to train a ML model. For example, empty columns are uninformative.\nIt tries to parse datetime columns using common formats, or a user-provided datetime_format.\nIt processes categorical columns to ensure consistent typing depending on the dataframe library in use.\nIt converts columns to string, unless they have a data type that carries more information, such as numerical, datetime, and categorial columns.\nFinally, it can convert numerical columns to np.float32 dtype. This ensures a consistent representation of numbers and missing values, and helps reducing the memory footprint.\n\nWe can look back at the “wine” dataframe and clean it with a suitably configured Cleaner:\n\ncleaner = Cleaner(drop_null_fraction=0.3, numeric_dtype=\"float32\")\n\ncleaner.fit_transform(wine)\n\n\n\n\n\n\n\n\ncountry\ndescription\npoints\nprice\nprovince\nregion_1\nvariety\nwinery\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\n96.0\n235.0\nCalifornia\nNapa Valley\nCabernet Sauvignon\nHeitz\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\n96.0\n110.0\nNorthern Spain\nToro\nTinta de Toro\nBodega Carmen Rodríguez\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\n96.0\n90.0\nCalifornia\nKnights Valley\nSauvignon Blanc\nMacauley\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\n96.0\n65.0\nOregon\nWillamette Valley\nPinot Noir\nPonzi\n\n\n4\nFrance\nThis is the top wine from La Bégude, named aft...\n95.0\n66.0\nProvence\nBandol\nProvence red blend\nDomaine de la Bégude\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n150925\nItaly\nMany people feel Fiano represents southern Ita...\n91.0\n20.0\nSouthern Italy\nFiano di Avellino\nWhite Blend\nFeudi di San Gregorio\n\n\n150926\nFrance\nOffers an intriguing nose with ginger, lime an...\n91.0\n27.0\nChampagne\nChampagne\nChampagne Blend\nH.Germain\n\n\n150927\nItaly\nThis classic example comes from a cru vineyard...\n91.0\n20.0\nSouthern Italy\nFiano di Avellino\nWhite Blend\nTerredora\n\n\n150928\nFrance\nA perfect salmon shade, with scents of peaches...\n90.0\n52.0\nChampagne\nChampagne\nChampagne Blend\nGosset\n\n\n150929\nItaly\nMore Pinot Grigios should taste like this. A r...\n90.0\n15.0\nNortheastern Italy\nAlto Adige\nPinot Grigio\nAlois Lageder\n\n\n\n\n150930 rows × 8 columns",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "href": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "title": "4  Preprocessing data with the skrub Cleaner",
    "section": "4.3 Under the hood: DropUninformative",
    "text": "4.3 Under the hood: DropUninformative\nWhen the Cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that do not bring useful information for training a ML model, and should therefore be dropped.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction.\n\nBy default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nSetting the threshold to None will disable this check and therefore retain empty columns.\n\nIt contains only one value, and no missing values.\n\nThis is controlled by the drop_if_constant flag, which is False by default.\n\nAll values in the column are distinct.\n\nThis may be the case if the column contains UIDs, but it can also happen when the column contains text.\nThis check is off by default and can be turned on by setting drop_if_unique to True.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with the skrub `Cleaner`</span>"
    ]
  },
  {
    "objectID": "content/chapters/quiz_01.html",
    "href": "content/chapters/quiz_01.html",
    "title": "5  Exploring dataframes with skrub",
    "section": "",
    "text": "What do I need to open a saved TableReport?\n\nA python console\nAn internet browser\nA Jupyter notebook\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAn internet browser is enough.",
    "crumbs": [
      "Exploring and sanitizing data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html",
    "href": "content/chapters/03_feat_eng_apply.html",
    "title": "6  Applying transformers to columns",
    "section": "",
    "text": "6.1 Introduction\nOften, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception.\nScikit-learn provides the ColumnTransformer to deal with this:\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "href": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "title": "6  Applying transformers to columns",
    "section": "6.2 ApplyToCols and ApplyToFrame",
    "text": "6.2 ApplyToCols and ApplyToFrame\n\n6.2.1 Applying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n6.2.2 Applying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, it may be beneficial to apply the same transformer to a subset of columns in a dataframe.\nThis example dataframe contains some patient information, and some (random) metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "6  Applying transformers to columns",
    "section": "6.3 Selection operations in a scikit-learn pipeline",
    "text": "6.3 Selection operations in a scikit-learn pipeline\nIn some situations, it may be necessary to select or remove specific columns from a dataframe: unlike ApplyToCols and ApplyToFrame, this means removing some features from the original table. This can be done with SelectCols and DropCols, which work as their name suggests, and can take a cols parameter to choose which columns to select or drop respectively.",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html",
    "href": "content/chapters/04_selectors.html",
    "title": "7  Choose your column: selectors",
    "section": "",
    "text": "7.1 Skrub selectors\nVery often, column selection is more complex than simply passing a list of column names to a transformer: it may be necessary to select all columns that have a specific data type, or based on some other characteristic (presence of nulls, column cardinality etc.).\nThe skrub selectors implement a number of selection strategies that can be combined in various ways to build complex filtering conditions that can then be employed by ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors are available from the skrub.selectors namespace:\nimport skrub.selectors as s\nWe will use this example dataframe to test some of the selectors:\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\nAdditional conditions include:\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\nVarious selectors allow to choose columns based on their name:\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#skrub-selectors",
    "href": "content/chapters/04_selectors.html#skrub-selectors",
    "title": "7  Choose your column: selectors",
    "section": "",
    "text": ".float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\n\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\n\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#combining-selectors",
    "href": "content/chapters/04_selectors.html#combining-selectors",
    "title": "7  Choose your column: selectors",
    "section": "7.2 Combining selectors",
    "text": "7.2 Combining selectors\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -:\nFor example, to select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#extracting-selected-columns",
    "href": "content/chapters/04_selectors.html#extracting-selected-columns",
    "title": "7  Choose your column: selectors",
    "section": "7.3 Extracting selected columns",
    "text": "7.3 Extracting selected columns\nSelectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#designing-custom-filters",
    "href": "content/chapters/04_selectors.html#designing-custom-filters",
    "title": "7  Choose your column: selectors",
    "section": "7.4 Designing custom filters",
    "text": "7.4 Designing custom filters\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function is True. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0",
    "crumbs": [
      "Column transformations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html",
    "href": "content/chapters/05_feat_eng_numerical.html",
    "title": "8  Scaling numerical features safely",
    "section": "",
    "text": "8.1 Numerical features with outliers\nNow that we can apply selections to any column we want thanks to ApplyToCols and the selectors, it is time to scale numerical features safely.\nWhen dealing with numerical features that contain outliers (including infinite values), standard scaling methods can be problematic. Outliers can dramatically affect the centering and scaling of the entire dataset, causing the scaled inliers to be compressed into a narrow range.\nConsider this example:\nfrom helpers import (\n    generate_data_with_outliers,\n    plot_feature_with_outliers\n)\n\nvalues = generate_data_with_outliers()\n\nplot_feature_with_outliers(values)\nIn this case, most of the values are in the range [-2, 2], but there are some large outliers in the range [-40, 40] that can cause issues when the feature needs to be scaled.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "href": "content/chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "title": "8  Scaling numerical features safely",
    "section": "",
    "text": "8.1.1 Regular scalers and their limitations\nThe StandardScaler computes mean and standard deviation across all values. With outliers present, these statistics become unreliable, and the scaling factor can become too small, squashing inlier values.\nThe RobustScaler uses quantiles (typically the 25th and 75th percentiles) instead of mean/std, which makes it more resistant to outliers. However, it doesn’t bound the output values, so extreme outliers can still have very large scaled values.\n\n\n8.1.2 SquashingScaler: A robust solution\nThe SquashingScaler combines robust centering with smooth clipping to handle outliers effectively. It works in two stages:\n\n\n8.1.3 Stage 1: Robust Scaling\n\nCenters the median to zero\nScales using quantile-based statistics (by default, the interquartile range)\nFor columns where quantiles are equal, uses a custom MinMaxScaler\nFor columns with constant values, fills with zeros\n\n\n\n8.1.4 Stage 2: Soft Clipping\n\nApplies a smooth squashing function: \\(x_{\\text{out}} = \\frac{z}{\\sqrt{1 + (z/B)^2}}\\)\nConstrains all values to the range \\([-\\texttt{max\\_absolute\\_value}, \\texttt{max\\_absolute\\_value}]\\) (default: 3)\nMaps infinite values to the corresponding boundaries\nPreserves NaN values unchanged\n\n\n\n8.1.5 Key advantages of SquashingScaler\nThe SquashingScaler has various advantages over traditional scalers:\n\nIt is Outlier-resistant: Outliers don’t affect inlier scaling, unlike the StandardScaler.\nIt has Bounded output: All values stay in a predictable range, ideal for neural networks and linear models.\nIt Handles edge cases: The scaler works with infinite values and constant columns.\nIt Preserves missing data: NaN values are kept unchanged.\n\nA disadvantage of the SquashingScaler is that it isNon-invertible: The soft clipping function is smooth but cannot be exactly inverted.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html#comparison-with-other-scalers",
    "href": "content/chapters/05_feat_eng_numerical.html#comparison-with-other-scalers",
    "title": "8  Scaling numerical features safely",
    "section": "8.2 Comparison with other scalers",
    "text": "8.2 Comparison with other scalers\nWhen compared on data with outliers: - StandardScaler compresses inliers due to large scaling factors - RobustScaler preserves relative scales but allows extreme outlier values - SquashingScaler keeps inliers in a reasonable range while smoothly bounding all values\nIf we plot the impact of each scaler on the result, this is what we can see:\n\nfrom helpers import scale_feature_and_plot\nscale_feature_and_plot(values)",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html",
    "href": "content/chapters/06_feat_eng_datetimes.html",
    "title": "9  Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "9.1 Introduction to Datetime Features\nDatetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Parsing these formats correctly is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "href": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "title": "9  Encoding datetime features with DatetimeEncoder",
    "section": "9.2 Converting datetime strings to datetime objects",
    "text": "9.2 Converting datetime strings to datetime objects\nOften, the first operation that must be done to work with datetime objects is converting the datetimes from a string representation to a proper datetime object. This is beneficial because using datetimes gives access to datetime-specific features, and allows to access the different parts of the datetime.\nSkrub provides different objects to deal with the conversion problem.\nToDatetime is a single column transformer that tries to conver the given column to datetime either by relying on a user-provided format, or by guessing common formats. Since this transformer must be applied to single columns (rather than dataframes), it is typically better to use it in conjunction with ApplyToCols. Additionally, the allow_reject parameter of ApplyToCols should be set to True to avoid raising exceptions for non-datetime columns:\n\nfrom skrub import ApplyToCols, ToDatetime\n\nimport pandas as pd\n\ndata = {\n    \"dates\": [\n        \"2023-01-03\",\n        \"2023-02-15\",\n        \"2023-03-27\",\n        \"2023-04-10\",\n    ]\n}\ndf = pd.DataFrame(data)\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nto_datetime works similarly to pd.to_datetime, or the example shown above with ApplyToCols.\n\n\n\n\n\n\nWarning\n\n\n\nto_datetime is a stateless function, so it should not be used in a pipeline, because it does not guarantee consistency between fit_transform and successive transform. ApplyToCols(ToDatetime(), allow_reject=True) is a better solution for pipelines.\n\n\nFinally, the standard Cleaner can be used for parsing datetimes, as it uses ToDatetime under the hood, and can take the datetime_format. As the Cleaner is a transformer, it guarantees consistency between fit_transform and transform.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "title": "9  Encoding datetime features with DatetimeEncoder",
    "section": "9.3 Encoding datetime features",
    "text": "9.3 Encoding datetime features\nDatetimes cannot be used “as-is” for training ML models, and must instead be converted to numerical features. Typically, this is done by “splitting” the datetime parts (year, month, day etc.) into separate columns, so that each column contains only one number.\nAdditional features may also be of interest, such as the number of seconds since epoch (which increases monotonically and gives an indication of the order of entries), whether a date is a weekday or weekend, or the day of the year.\nTo achieve this with standard dataframe libraries, the code looks like this:\n\ndf_enc[\"year\"] = df_enc[\"dates\"].dt.year\ndf_enc[\"month\"] = df_enc[\"dates\"].dt.month\ndf_enc[\"day\"] = df_enc[\"dates\"].dt.day\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"total_seconds\"] = (df_enc[\"dates\"] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(seconds=1)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nyear\nmonth\nday\nweekday\nday_of_year\ntotal_seconds\n\n\n\n\n0\n2023-01-03\n2023\n1\n3\n1\n3\n1672704000\n\n\n1\n2023-02-15\n2023\n2\n15\n2\n46\n1676419200\n\n\n2\n2023-03-27\n2023\n3\n27\n0\n86\n1679875200\n\n\n3\n2023-04-10\n2023\n4\n10\n0\n100\n1681084800\n\n\n\n\n\n\n\nSkrub’s DatetimeEncoder allows to add the same features with a simpler interface. As the DatetimeEncoder is a single column transformer, we use again ApplyToCols.\n\nfrom skrub import DatetimeEncoder\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nde = DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True)\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_month\ndates_day\ndates_total_seconds\ndates_weekday\ndates_day_of_year\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "title": "9  Encoding datetime features with DatetimeEncoder",
    "section": "9.4 Periodic features",
    "text": "9.4 Periodic features\nPeriodic features are useful for training machine learning models because they capture the cyclical nature of certain data patterns. For example, time-related features such as hours in a day, days in a week, or months in a year often exhibit periodic behavior. By encoding these features periodically, models can better understand and predict patterns that repeat over time, such as daily traffic trends, weekly sales cycles, or seasonal variations. This ensures that the model treats the start and end of a cycle as close neighbors, improving its ability to generalize and make accurate predictions.\nThis can be done manually with dataframe libraries. For example, circular encoding (a.k.a., trigonometric or sin/cos encoding) can be implemented with Pandas like so:\n\nimport numpy as np \n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_enc[\"day_of_year\"] / 365)\ndf_enc[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_enc[\"day_of_year\"] / 365)\n\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"weekday_sin\"] = np.sin(2 * np.pi * df_enc[\"weekday\"] / 7)\ndf_enc[\"weekday_cos\"] = np.cos(2 * np.pi * df_enc[\"weekday\"] / 7)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023-01-03\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023-02-15\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023-03-27\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023-04-10\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000\n\n\n\n\n\n\n\nAlternatively, the DatetimeEncoder can add periodic features using either circular or spline encoding through the periodic_encoding parameter:\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_total_seconds\ndates_month_circular_0\ndates_month_circular_1\ndates_day_circular_0\ndates_day_circular_1\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023.0\n1.672704e+09\n0.500000\n8.660254e-01\n5.877853e-01\n0.809017\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023.0\n1.676419e+09\n0.866025\n5.000000e-01\n1.224647e-16\n-1.000000\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023.0\n1.679875e+09\n1.000000\n6.123234e-17\n-5.877853e-01\n0.809017\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023.0\n1.681085e+09\n0.866025\n-5.000000e-01\n8.660254e-01\n-0.500000\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "href": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "title": "9  Encoding datetime features with DatetimeEncoder",
    "section": "9.5 Conclusions",
    "text": "9.5 Conclusions\nIn this chapter, we explored the importance and challenges of working with datetime features. We covered how to convert string representations of dates to datetime objects using skrub’s ToDatetime transformer and the Cleaner, both of which can be integrated into pipelines for robust preprocessing.\nWe also discussed the need to encode datetime features into numerical representations suitable for machine learning models. The DatetimeEncoder provides a convenient way to extract useful components such as year, month, day, weekday, day of year, and total seconds since epoch. Additionally, we saw how periodic (circular) encoding can be used to capture cyclical patterns in time-based data.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html",
    "href": "content/chapters/07_feat_eng_categorical.html",
    "title": "10  Mixed data: dealing with categories",
    "section": "",
    "text": "10.1 Introduction: The Challenge of Categorical Features\nReal-world datasets rarely contain only numeric values. We frequently encounter categorical features—values that belong to discrete categories, such as names, occupations, geographic locations, or clothing sizes. Text data also falls into this category, since each unique string can be considered a categorical value.\nThe challenge is that machine learning models require numeric input. How do we convert these categorical values into numeric features that preserve their information and enable our models to make good predictions?\nThis chapter explores the various strategies and tools available in skrub to encode categorical features, helping us choose the right approach for our specific use case.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "href": "content/chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "title": "10  Mixed data: dealing with categories",
    "section": "10.2 Why Categorical Encoders Matter",
    "text": "10.2 Why Categorical Encoders Matter\nThe way we encode categorical features significantly impacts our machine learning pipeline:\n\nPerformance: The encoding choice directly affects how well our model learns from categorical information\nEfficiency: Some encodings create many features (potentially thousands), which increases computation time and memory usage\nInterpretability: Different encoders provide varying levels of transparency in what features represent\nScalability: Not all methods scale well to high-cardinality features (those with many unique values)\n\nUsing the appropriate encoder ensures we’re making the best use of categorical information while keeping our model efficient and interpretable.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "href": "content/chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "title": "10  Mixed data: dealing with categories",
    "section": "10.3 Categorical Encoders: Pros and Cons",
    "text": "10.3 Categorical Encoders: Pros and Cons\n\n10.3.1 One-Hot Encoding and Ordinal Encoding (scikit-learn)\nOneHotEncoder: Creates a binary indicator column for each unique category, where 1 denotes the presence of the category and 0 its absence.\nPros: - Straightforward and intuitive - Works well for low-cardinality features (few unique values) - Produces sparse matrices that can save memory\nCons: - Becomes impractical with high-cardinality features (creates hundreds or thousands of columns) - Results in mostly zero-valued sparse matrices when dense, which is the situation when working with dataframes - Increases overfitting risk and computational overhead\nThe OneHotEncoder is used by default by the skrub TableVectorizer for categorical features with fewer than 40 unique values.\nOrdinalEncoder: Assigns each category a numerical value (0, 1, 2, …).\nPros: - Very memory-efficient - Creates only one output column per input column - Fast to compute\nCons: - Introduces artificial ordering among categories that may not exist in reality - Can mislead models into thinking some categories are “greater than” others\n\n\n10.3.2 Categorical encoders in skrub\nAll the categorical encoders in skrub are designed to encode any number of unique values using a fixed number of components: this number is controlled by the parameter n_components in each transformer.\n\n\n10.3.3 StringEncoder\nApproach: Applies term frequency-inverse document frequency (tf-idf) vectorization to character n-grams, followed by truncated singular value decomposition (SVD) for dimensionality reduction. This method is also known as Latent Semantic Analysis.\nPros: - The best all-rounder: Performs well on both categorical and text data - Fast training time - Robust and generalizes well across different datasets - No artificial ordering introduced\nCons: - Less interpretable than one-hot encoding or ordinal encoding - May not capture semantic relationships as well as language model-based approaches - Performance depends on the nature of the categorical data\n\n\n10.3.4 TextEncoder\nApproach: Uses pretrained language models from HuggingFace Hub to generate dense vector representations of text.\nPros: - Exceptional performance on free-flowing text and natural language - Captures semantic meaning and context - Leverages knowledge from large-scale language model pretraining - Can excel on datasets where domain-specific information aligns with pretraining data\nCons: - Very computationally expensive: Significantly slower than other methods - Requires heavy dependencies (PyTorch, transformers) - Models are large and require downloading - Impractical for CPU-only environments - Performance on traditional categorical data (non-text, such as IDs) is not much better than simpler methods\n\n\n10.3.5 MinHashEncoder\nApproach: Decomposes strings into n-grams and applies the MinHash algorithm for quick dimension reduction.\nPros: - Very fast training time - Simple and lightweight - Minimal memory overhead - Good for quick prototyping or very large-scale datasets\nCons: - Performance generally lags behind StringEncoder and TextEncoder - Less nuanced feature representation - Less robust across different types of data\n\n\n10.3.6 GapEncoder\nApproach: Estimates latent categories by finding common n-gram patterns across values, then encodes these patterns as numeric features.\nPros: - Interpretable: Column names reflect the estimated categories - Can group similar strings intelligently - Good for exploratory data analysis - Reasonable performance across datasets\nCons: - Slower training time compared to StringEncoder and MinHashEncoder - Interpretability comes at the cost of training speed - May require more computational resources for large datasets",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html#conclusion",
    "href": "content/chapters/07_feat_eng_categorical.html#conclusion",
    "title": "10  Mixed data: dealing with categories",
    "section": "10.4 Conclusion",
    "text": "10.4 Conclusion\nEncoding categorical features is a critical step in preparing data for machine learning. The skrub library provides multiple encoders to handle different scenarios:\n\nStart with StringEncoder as a default for high-cardinality categorical features. It offers the best balance of speed, performance, and robustness across diverse datasets.\nUse OneHotEncoder for low-cardinality features (&lt; 40 unique values) to keep the feature space manageable.\nChoose TextEncoder if you’re working with true textual data (reviews, comments, descriptions) and have sufficient computational resources.\nConsider GapEncoder when interpretability is important and the additional training time can be dealt with.\nUse MinHashEncoder when you need maximum speed and are working with very large datasets.\n\nThe TableVectorizer integrates these encoders automatically, dispatching columns to the appropriate encoder based on their data type and cardinality. This automation makes it easy to process mixed-type datasets efficiently while still allowing fine-grained control when needed. By default, the TableVectorizer uses the OneHotEncoder for categorical features with cardinality &lt;= 40, and StringEncoder for categorical features with cardinality &gt; 40.\nFor a comprehensive empirical comparison of these methods, refer to the categorical encoders benchmark.",
    "crumbs": [
      "Feature engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html",
    "href": "content/chapters/08_feat_eng_table_vect.html",
    "title": "11  All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "11.1 What is the TableVectorizer?\nMachine learning models typically require numeric input features. When working with real-world datasets, we often have a mix of data types: numbers, text, dates, and categorical values. The TableVectorizer automates the entire process of converting a heterogeneous dataframe into a matrix of numeric features ready for machine learning.\nInstead of manually specifying how to handle each column, the TableVectorizer automatically detects the data type of each column and applies the appropriate transformation to encode the column using numerical features.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#how-does-the-tablevectorizer-work",
    "href": "content/chapters/08_feat_eng_table_vect.html#how-does-the-tablevectorizer-work",
    "title": "11  All the pre-processing in one place: TableVectorizer",
    "section": "11.2 How does the TableVectorizer work?",
    "text": "11.2 How does the TableVectorizer work?\nThe TableVectorizer operates in two phases:\n\n11.2.1 Phase 1: Data Cleaning and Type Detection\nFirst, it runs a Cleaner on the input data to: - Detect and parse datetime columns (possibly, with custom datetime formats) - Handle missing values represented as strings (e.g., “N/A”) - Clean up categorical columns to have consistent typing - Remove uninformative columns (those with only nulls, constant values, or all unique values) - Finally, convert all numerical features to float32 to reduce the computational cost.\nThis ensures that each column has the correct data type before encoding.\n\n\n11.2.2 Phase 2: Column Dispatch and Encoding\nAfter cleaning, the TableVectorizer categorizes columns and dispatches them to the appropriate transformer based on their data type and cardinality.\nThe TableVectorizer uses the following default transformers for each column type:\n\nNumeric columns: Left untouched (passthrough) - they’re already in the right format\nDatetime columns: Transformed by DatetimeEncoder to extract meaningful temporal features\nLow-cardinality categorical/string columns: Transformed with OneHotEncoder to create binary indicator variables\nHigh-cardinality categorical/string columns: Transformed with StringEncoder to create dense numeric representations",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#key-parameters",
    "href": "content/chapters/08_feat_eng_table_vect.html#key-parameters",
    "title": "11  All the pre-processing in one place: TableVectorizer",
    "section": "11.3 Key Parameters",
    "text": "11.3 Key Parameters\n\n11.3.1 Cardinality Threshold\nBy default, columns with 40 or fewer unique values are considered “low-cardinality” and one-hot encoded, while those with more unique values are “high-cardinality” and encoded with StringEncoder. We can change this threshold:\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(cardinality_threshold=30)  # Adjust the threshold\n\n\n\n11.3.2 Data Cleaning Parameters\nThe TableVectorizer forwards several parameters to the internal Cleaner:\n\ndrop_null_fraction: Fraction of nulls above which a column is dropped (default: 1.0)\ndrop_if_constant: Drop columns with only one unique value (default: False)\ndrop_if_unique: Drop string/categorical columns where all values are unique (default: False)\ndatetime_format: Format string for parsing dates\n\nNote that for drop_if_constant null values count as one additional distinct value. drop_if_unique should be used with care when working with free-flowing text, as in this case it is quite likely that all strings will be different, but the column is not uninformative.\n\ntv = TableVectorizer(\n    drop_null_fraction=0.9,  # Drop columns that are 90% null\n    drop_if_constant=True,\n    datetime_format=\"%Y-%m-%d\"\n)\n\n\n\n11.3.3 Specifying Custom Transformers\nThe TableVectorizer applies whatever transformer is provided to each of the numeric, datetime, high_cardinality, and low_cardinality paramters. To tweak the default parameters of the transformers a new transformer should be provided:\n\nfrom skrub import TableVectorizer, DatetimeEncoder, StringEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Create custom transformers\ndatetime_enc = DatetimeEncoder(periodic_encoding=\"circular\")\nstring_enc = StringEncoder(n_components=10)\n\n# Pass them to TableVectorizer\ntv = TableVectorizer(\n    datetime=datetime_enc,\n    high_cardinality=string_enc,\n    low_cardinality=OneHotEncoder(sparse_output=False)\n)\n\nThis allows to, for example, change the neumber of parameters in the StringEncoder, or provide a custom datetime format for the DatetimeEncoder.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#using-specific_transformers-for-column-specific-control",
    "href": "content/chapters/08_feat_eng_table_vect.html#using-specific_transformers-for-column-specific-control",
    "title": "11  All the pre-processing in one place: TableVectorizer",
    "section": "11.4 Using specific_transformers for Column-Specific Control",
    "text": "11.4 Using specific_transformers for Column-Specific Control\nFor fine-grained control, we can specify transformers for specific columns using the specific_transformers parameter. This is useful when we want to override the default behavior for particular columns:\n\nfrom sklearn.preprocessing import OrdinalEncoder\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"occupation\": [\"engineer\", \"teacher\", \"doctor\"],\n    \"salary\": [100000, 50000, 150000]\n})\n\n# Create a custom transformer for the 'occupation' column\nspecific_transformers = [(OrdinalEncoder(), [\"occupation\"])]\n\ntv = TableVectorizer(specific_transformers=specific_transformers)\nresult = tv.fit_transform(df)\n\nImportant notes about specific_transformers:\n\nColumns specified here bypass the default categorization logic\nThe transformer receives the column as-is, without any preprocessing\nThe transformer must be able to handle the column’s current data type and values\nFor more complex transformations, consider using ApplyToCols and the selectors API (explained in the previous chapters), or the skrub Data Ops.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html",
    "href": "content/chapters/09_tabular_pipeline.html",
    "title": "12  Building a tabular pipeline",
    "section": "",
    "text": "12.1 Introduction: From Raw Data to Predictions\nUp until now we have covered how to clean data with the Cleaner, extract features from different column types, and handle categorical features with specialized encoders. In this section we will show how we can combine all these preprocessing techniques into a complete machine learning pipeline.\nA pipeline ensures that: - Data transformations are applied consistently across training and test sets - Data leakage is avoided by fitting transformers only on training data - The workflow is reproducible and deployable - Preprocessing steps are properly chained together\nIn this chapter, we explore two approaches: building custom pipelines with TableVectorizer, and using the tabular_pipeline function for quick, well-tuned baselines.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "href": "content/chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "title": "12  Building a tabular pipeline",
    "section": "12.2 Manual Pipeline Construction with TableVectorizer",
    "text": "12.2 Manual Pipeline Construction with TableVectorizer\nThe TableVectorizer can be the foundation of a custom scikit-learn pipeline. We combine it with other preprocessing steps and a final estimator:\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skrub import TableVectorizer\n\nmodel = make_pipeline(\n    TableVectorizer(),           # Feature engineering\n    SimpleImputer(),             # Handle missing values\n    StandardScaler(),            # Normalize features\n    LogisticRegression()         # Final estimator\n)\nThis approach gives complete control over which preprocessing steps to use and in what order. We can customize the TableVectorizer parameters (cardinality threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\nIn the case of the example we used LogisticRegression as our estimator, but if we used a different estimator, such as the HistogramGradientBoostingClassifier, the scaling and imputation steps could have been avoided.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#the-tabular_pipeline-function",
    "href": "content/chapters/09_tabular_pipeline.html#the-tabular_pipeline-function",
    "title": "12  Building a tabular pipeline",
    "section": "12.3 The tabular_pipeline function",
    "text": "12.3 The tabular_pipeline function\nFor most common use cases, we can skip the manual pipeline construction and use the tabular_pipeline function. This function automatically creates an appropriate pipeline based on the estimator we provide:\nfrom skrub import tabular_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a complete pipeline for a specific estimator\nmodel = tabular_pipeline(LogisticRegression())\nOr, we can use a string to get a pre-configured pipeline with a default estimator:\n# Classification with HistGradientBoostingClassifier\nmodel = tabular_pipeline('classification')\n\n# Regression with HistGradientBoostingRegressor\nmodel = tabular_pipeline('regression')",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "href": "content/chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "title": "12  Building a tabular pipeline",
    "section": "12.4 How tabular_pipeline Adapts to Different Estimators",
    "text": "12.4 How tabular_pipeline Adapts to Different Estimators\nThe tabular_pipeline function intelligently configures the preprocessing pipeline based on the estimator type:\n\n12.4.1 For Linear Models (e.g., LogisticRegression, Ridge)\n\nTableVectorizer: Uses the default configuration, except for the addition of spline-encoded datetime features by the DatetimeEncoder\nSimpleImputer: Added because linear models cannot handle missing values\nSquashingScaler: Normalizes numeric features to improve convergence and performance\nEstimator: The provided linear model\n\nThis configuration ensures numeric features are properly scaled and missing values are handled appropriately.\n\n\n12.4.2 For Tree-Based Ensemble Models (RandomForest, HistGradientBoosting)\n\nTableVectorizer: Configured specifically for tree models\n\nLow-cardinality categorical features: Either kept as categorical (HistGradientBoosting) or ordinal encoded (RandomForest)\nHigh-cardinality features: StringEncoder for robust feature extraction\nDatetime features: No spline encoding (unnecessary for trees)\n\nSimpleImputer: Only added for older scikit-learn versions (&lt; 1.4) without native missing value support\nScaler: Not added (unnecessary for tree-based models)\nEstimator: The provided tree-based estimator\n\nThis configuration leverages the native capabilities of tree models while still providing effective feature engineering through the StringEncoder.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html#key-advantages-of-tabular_pipeline",
    "href": "content/chapters/09_tabular_pipeline.html#key-advantages-of-tabular_pipeline",
    "title": "12  Building a tabular pipeline",
    "section": "12.5 Key Advantages of tabular_pipeline",
    "text": "12.5 Key Advantages of tabular_pipeline\n\nSmart Configuration: Automatically selects preprocessing parameters appropriate for the estimator\nSimplicity: One-line creation of a complete, well-tuned baseline\nRobustness: Handles edge cases like missing values and mixed data types automatically\n\n\nUse tabular_pipeline when you want a quick, well-tuned baseline to benchmark against or as a starting point\nBuild manual pipelines when you need fine-grained control over preprocessing steps or want to experiment with custom transformers\n\nBoth approaches produce scikit-learn compatible pipelines that can be used with cross-validation, hyperparameter tuning, and other standard workflows.",
    "crumbs": [
      "Putting it all together",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  }
]