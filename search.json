[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub tutorials",
    "section": "",
    "text": "0.1 Prepration and setup\nThis online book contains the material used for the Inria Academy course on the skrub package.\nEach chapter of the book includes a theoretical section that describes a specific aspect of the library, and one or more exercises that are intended to put into practice what has been explained.\nQuiz sections split the major sections of the code.\nThe easiest way to work with the notebooks provided here is by installing and using pixi.\nThen, it is possible to run\nto create the environment, followed by\nto create a shell that allows to run the exercises.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "index.html#prepration-and-setup",
    "href": "index.html#prepration-and-setup",
    "title": "Skrub tutorials",
    "section": "",
    "text": "pixi install\n\npixi shell",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inria Academy - skrub like a pro</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html",
    "href": "content/chapters/00_intro.html",
    "title": "2  A world without skrub",
    "section": "",
    "text": "2.1 Strategizing\nLet’s begin the lesson by imagining a world without skrub, where we can use only Pandas and scikit-learn to clean data and prepare a machine learning model.\nLet’s take a look at the target::\nThis is a numerical column, and our task is predicting the value of current_annual_salary.\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000\nWe need to:\nOnce we have processed the data, we can train a machine learning model. For the sake of the example, we will use a linear model (Ridge), which means that we need to scale numerical features, besides imputing missing values.\nFinally, we want to evaluate the performance of the method across multiple cross-validation splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#strategizing",
    "href": "content/chapters/00_intro.html#strategizing",
    "title": "2  A world without skrub",
    "section": "",
    "text": "Impute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "href": "content/chapters/00_intro.html#building-a-traditional-pipeline",
    "title": "2  A world without skrub",
    "section": "2.2 Building a traditional pipeline",
    "text": "2.2 Building a traditional pipeline\nLet’s build a traditional predictive pipeline following the steps we just discussed.\n\n2.2.1 Step 1: Convert date features to numerical\nExtract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['years_since_hired'] = (pd.Timestamp.now() - X_processed['date_first_hired']).dt.days / 365.25\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 10)\n\n\n\n\n2.2.2 Step 2: Encode categorical features\nEncode only the non-numerical categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1219)\n\n\n\n\n2.2.3 Step 3: Impute missing values\nWe’ll impute missing values in the gender column using the most frequent strategy.\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)\n\n\n\n2.2.4 Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)\n\n\n\n2.2.5 Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10366.9520 (+/- 1403.5225)\n\n\n\n\n2.2.6 “Just ask an agent to write the code”\nIt’s what I did. Here are some of the issues I noticed:\n\nOperations in the wrong order.\nTrying to impute categorical features without encoding them as numerical values.\nThe datetime feature was encoded as a categorical (i.e, with dummmies).\nToo many print statements.\nCells could not be executed in order without proper debugging and re-prompting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "href": "content/chapters/00_intro.html#waking-up-from-a-nightmare",
    "title": "2  A world without skrub",
    "section": "2.3 Waking up from a nightmare",
    "text": "2.3 Waking up from a nightmare\nThankfully, we live in a world where we can import skrub. Let’s see what we can get if we use skrub.tabular_pipeline.\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.9096 (+/- 0.0147)\nMean test RMSE: 8742.8548 (+/- 981.1682)\n\n\nAll the code from before, the tokens and the debugging are replaced by a single import that gives better results.\nThroughout the tutorial, we will see how each step can be simplified, replaced, or improved using skrub features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A world without skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html",
    "href": "content/chapters/01_exploring_data.html",
    "title": "3  Exploring dataframes with skrub",
    "section": "",
    "text": "3.1 Exploring data with Pandas tools\nIn this notebook, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table.\nFirst, let’s import the necessary libraries and load the dataset.\nNow that we have a dataframe we can work with, we need to do some exploratory analysis to get an idea of the characteristics of the data. Among others, we would like to find out:\nFor the sake of the example, let’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   age             48842 non-null  int64   \n 1   workclass       46043 non-null  category\n 2   fnlwgt          48842 non-null  int64   \n 3   education       48842 non-null  category\n 4   education-num   48842 non-null  int64   \n 5   marital-status  48842 non-null  category\n 6   occupation      46033 non-null  category\n 7   relationship    48842 non-null  category\n 8   race            48842 non-null  category\n 9   sex             48842 non-null  category\n 10  capital-gain    48842 non-null  int64   \n 11  capital-loss    48842 non-null  int64   \n 12  hours-per-week  48842 non-null  int64   \n 13  native-country  47985 non-null  category\ndtypes: category(8), int64(6)\nmemory usage: 2.6 MB\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.2 Exploring data with the TableReport",
    "text": "3.2 Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nTableReport(data)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n3.2.1 Default view of the TableReport\nThe TableReport gives us a comprehensive overview of the dataset. The default view shows all the columns in the dataset, and allows to select and copy the content of the cells shown in the preview.\nThe TableReport is intended to show a preview of the data, so it does not contain all the rows in the dataset, rather it shows only the first and last few rows by default.\n\n\n3.2.2 The “Stats” tab\n\nTableReport(data, open_tab=\"stats\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Stats” tab provides a variety of descriptive statistics for each column in the dataset. This includes:\n\nThe column name\nThe detected data type of the column\nWhether the column is sorted or not\nThe number of null values in the column, as well as the percentage\nThe number of unique values in the column\n\nFor numerical columns, additional statistics are provided:\n\nMean\nStandard deviation\nMinimum and maximum values\nMedian\n\n\n\n3.2.3 The “Distributions” tab\n\nTableReport(data, open_tab=\"distributions\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Distributions” tab provides visualizations of the distributions of values in each column. This includes histograms for numerical columns and bar plots for categorical columns.\nThe “Distributions” tab helps with detecting potential issues in the data, such as:\n\nSkewed distributions\nOutliers\nUnexpected value frequencies\n\nFor example, in this dataset we can see that some columns are heavily skewed, such as “workclass”, “race”, and “native-country”: this is important information to keep track of, because these columns may require special handling during data preprocessing or modeling.\nAdditionally, the “Distributions” tab allows so select columns manually, so that they can be added to a script and selected for further analysis or modeling.\n\n\n3.2.4 The “Associations” tab\n\nTableReport(data, open_tab=\"associations\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Associations” tab provides insights into the relationships between different columns in the dataset. It shows Pearson’s correlation coefficients for numerical columns, as well as Cramér’s V for all columns.\nWhile this is a somewhat rough measure of association, it can help identify potential relationships worth exploring further during the analysis, and highlights highly correlated columns: depending on the modeling technique used, these may need to be handled specially to avoid issues with multicollinearity.\nIn this example, we can see that “education-num” and “education” have perfect correlation, which means that one of the two columns can be dropped without losing information.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "href": "content/chapters/01_exploring_data.html#exploring-the-target-variable",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.3 Exploring the target variable",
    "text": "3.3 Exploring the target variable\nLet’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "href": "content/chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.4 Configuring and saving the TableReport",
    "text": "3.4 Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console.\nIt is possible to configure various parameters using the skrub global config. For example, it is possible to replace the default Pandas or Polars dataframe display with the TableReport as follows:\n\nfrom skrub import set_config\nset_config(use_table_report=True)\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "href": "content/chapters/01_exploring_data.html#working-with-big-tables",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.5 Working with big tables",
    "text": "3.5 Working with big tables\nPlotting and measuring the column correlations are expensive operations, so when the dataframe under study is large it may be more convenient to skip them, as generating the Distributions and Associations tab may take a long time.\nThe max_plot_columns and max_association_columns parameters allow to set a threshold on the number of columns: the TableReport will skip the respective task if the number of colums in the dataframe is larger than the threshold:\n\nTableReport(data, max_association_columns=3, max_plot_columns=3)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhen the number of columns is too large, an error message is shown in the respective tab instead of the plots or correlations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/01_exploring_data.html#exercise-exploring-a-new-table",
    "href": "content/chapters/01_exploring_data.html#exercise-exploring-a-new-table",
    "title": "3  Exploring dataframes with skrub",
    "section": "3.6 Exercise: exploring a new table",
    "text": "3.6 Exercise: exploring a new table\nFor this exercise, we will use the employee_salaries dataframe to answer some questions.\nRun the following code to import the dataframe:\n\nfrom skrub.datasets import fetch_employee_salaries\n\nemployee_salaries = fetch_employee_salaries()\ndata = employee_salaries.X\n\nNow use the skrub TableReport and answer the following questions:\n\nQuestionsSolution\n\n\n\nWhat’s the size of the dataframe? (columns and rows)\nHow many columns have object/numerical/datetime\nAre there columns with a large number of missing values?\nAre there columns that have a high cardinality (&gt;40 unique values)?\nWere datetime columns parsed correctly?\nWhich columns have outliers?\nWhich columns have an imbalanced distribution?\nWhich columns are strongly correlated with each other?\n\n\n\n\nWhat’s the size of the dataframe? (columns and rows)\n\n9228 rows × 8 columns\n\nHow many columns have object/numerical/datetime\n\nNo datetime columns, one integer column (year_first_hired), all other columns are objects.\n\nAre there columns with a large number of missing values?\n\nNo, only the gender column contains a small fraction (0.2%) of missing values.\n\nAre there columns that have a high cardinality?\n\nYes, division, employee_position_title, date_first_hired have a cardinality larger than 40.\n\nWere datetime columns parsed correctly?\n\nNo, the date_first_hired column has dtype Object.\n\nWhich columns have outliers?\n\nNo columns seem to include outliers.\n\nWhich columns have an imbalanced distribution?\n\nassignment_category has an unbalanced distribution.\n\nWhich columns are strongly correlated with each other?\n\ndepartment and department_name have a Cramer’s V of 1, so they are very strongly correlated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring dataframes with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html",
    "href": "content/chapters/02_cleaning_data.html",
    "title": "4  Preprocessing data with skrub",
    "section": "",
    "text": "4.1 Cleaning data with Pandas\nIn this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner, and compare it to traditional methods using pandas.\nfrom sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=42074)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "href": "content/chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "title": "4  Preprocessing data with skrub",
    "section": "4.2 Using the skrub Cleaner",
    "text": "4.2 Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column:\n\nIt replaces common strings used to represent missing values (e.g., NULL, ?) with NA markers.\nIt uses the DropUninformative transformer to decide whether a column is “uninformative”, that is, it is not likely to bring information useful to train a ML model. For example, empty columns are uninformative.\nIt tries to parse datetime columns using common formats, or a user-provided datetime_format.\nIt processes categorical columns to ensure consistent typing depending on the dataframe library in use.\nIt converts columns to string, unless they have a data type that carries more information, such as numerical, datetime, and categorial columns.\nFinally, it can convert numerical columns to np.float32 dtype. This ensures a consistent representation of numbers and missing values, and helps reducing the memory footprint. This is useful if the Cleaner is used as the first step in a machine learning pipeline.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "href": "content/chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "title": "4  Preprocessing data with skrub",
    "section": "4.3 Under the hood: DropUninformative",
    "text": "4.3 Under the hood: DropUninformative\nWhen the cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that could be dropped as they do not bring useful information for training a ML model.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction. By default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nIt contains only one value, and no missing values. This is controlled by the drop_if_constant flag, which is False by default..\nAll values in the column are distinct. This may be the case if the column contains UIDs, but it can also happen when the column contains text. This check is off by default and can be turned on by setting drop_if_unique to True.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/02_cleaning_data.html#exercise",
    "href": "content/chapters/02_cleaning_data.html#exercise",
    "title": "4  Preprocessing data with skrub",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\nGiven the following dataframe, use skrub’s Cleaner to clean the data so that:\n\nConstant columns are removed\nAll columns with more than 50% missing values are removed\n\n\nimport pandas as pd\ndf = pd.read_csv(\"../data/synthetic_data.csv\")\n\nLet’s first examine the dataset before cleaning:\n\nfrom skrub import TableReport\nTableReport(df)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nNow, let’s use the Cleaner to clean the data:\n\nfrom skrub import Cleaner\n\n# Configure the Cleaner to:\n# - Remove constant columns (drop_if_constant=True)\n# - Remove columns with more than 50% missing values (drop_null_fraction=0.5)\ncleaner = Cleaner(drop_if_constant=True, drop_null_fraction=0.5)\n\n# Apply the cleaning\ndf_cleaned = cleaner.fit_transform(df)\n\n# Display the cleaned dataframe\nTableReport(df_cleaned)\n\nProcessing column   1 / 10Processing column   2 / 10Processing column   3 / 10Processing column   4 / 10Processing column   5 / 10Processing column   6 / 10Processing column   7 / 10Processing column   8 / 10Processing column   9 / 10Processing column  10 / 10\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWe can inspect which columns were dropped and what transformations were applied:\n\nprint(f\"Original shape: {df.shape}\")\nprint(f\"Cleaned shape: {df_cleaned.shape}\")\nprint(\n    f\"\\nColumns dropped: {[col for col in df.columns if col not in cleaner.all_outputs_]}\"\n)\n\nOriginal shape: (10000, 14)\nCleaned shape: (10000, 10)\n\nColumns dropped: ['with_nulls_1', 'with_nulls_2', 'constant_1', 'constant_2']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing data with skrub</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html",
    "href": "content/chapters/03_feat_eng_apply.html",
    "title": "5  Applying transformers to columns",
    "section": "",
    "text": "5.1 Introduction\nOften, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception.\nScikit-learn provides the ColumnTransformer to deal with this:\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "href": "content/chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "title": "5  Applying transformers to columns",
    "section": "5.2 ApplyToCols and ApplyToFrame",
    "text": "5.2 ApplyToCols and ApplyToFrame\n\n5.2.1 Applying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n5.2.2 Applying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, it may be beneficial to apply the same transformer to a subset of columns in a dataframe.\nThis example dataframe contains some patient information, and some (random) metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "content/chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "5  Applying transformers to columns",
    "section": "5.3 Selection operations in a scikit-learn pipeline",
    "text": "5.3 Selection operations in a scikit-learn pipeline\nIn some situations, it may be necessary to select or remove specific columns from a dataframe: unlike ApplyToCols and ApplyToFrame, this means removing some features from the original table. This can be done with SelectCols and DropCols, which work as their name suggests, and can take a cols parameter to choose which columns to select or drop respectively.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/03_feat_eng_apply.html#exercise-putting-everything-together-in-a-scikit-learn-pipeline",
    "href": "content/chapters/03_feat_eng_apply.html#exercise-putting-everything-together-in-a-scikit-learn-pipeline",
    "title": "5  Applying transformers to columns",
    "section": "5.4 Exercise: putting everything together in a scikit-learn pipeline",
    "text": "5.4 Exercise: putting everything together in a scikit-learn pipeline",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applying transformers to columns</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html",
    "href": "content/chapters/04_selectors.html",
    "title": "6  Choose your column: selectors",
    "section": "",
    "text": "6.1 Skrub selectors\nVery often, column selection is more complex than simply passing a list of column names to a transformer: it may be necessary to select all columns that have a specific data type, or based on some other characteristic (presence of nulls, column cardinality etc.).\nThe skrub selectors implement a number of selection strategies that can be combined in various ways to build complex filtering conditions that can then be employed by ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors are available from the skrub.selectors namespace:\nimport skrub.selectors as s\nWe will use this example dataframe to test some of the selectors:\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\nAdditional conditions include:\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\nVarious selectors allow to choose columns based on their name:\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#skrub-selectors",
    "href": "content/chapters/04_selectors.html#skrub-selectors",
    "title": "6  Choose your column: selectors",
    "section": "",
    "text": ".float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\n\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\n\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#combining-selectors",
    "href": "content/chapters/04_selectors.html#combining-selectors",
    "title": "6  Choose your column: selectors",
    "section": "6.2 Combining selectors",
    "text": "6.2 Combining selectors\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -:\nFor example, to select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#extracting-selected-columns",
    "href": "content/chapters/04_selectors.html#extracting-selected-columns",
    "title": "6  Choose your column: selectors",
    "section": "6.3 Extracting selected columns",
    "text": "6.3 Extracting selected columns\nSelectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#designing-custom-filters",
    "href": "content/chapters/04_selectors.html#designing-custom-filters",
    "title": "6  Choose your column: selectors",
    "section": "6.4 Designing custom filters",
    "text": "6.4 Designing custom filters\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function is True. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/04_selectors.html#exercise-using-selectors-together-with-applytocols",
    "href": "content/chapters/04_selectors.html#exercise-using-selectors-together-with-applytocols",
    "title": "6  Choose your column: selectors",
    "section": "6.5 Exercise: using selectors together with ApplyToCols",
    "text": "6.5 Exercise: using selectors together with ApplyToCols\nConsider this example dataframe:\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"metric_1\": [10.5, 20.3, 30.1, 40.2],\n        \"metric_2\": [5.1, 15.6, None, 35.8],\n        \"metric_3\": [1.1, 3.3, 2.6, .8],\n        \"num_id\": [101, 102, 103, 104],\n        \"str_id\": [\"A101\", \"A102\", \"A103\", \"A104\"],\n        \"description\": [\"apple\", None, \"cherry\", \"date\"],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\ndescription\nname\n\n\n\n\n0\n10.5\n5.1\n1.1\n101\nA101\napple\nAlice\n\n\n1\n20.3\n15.6\n3.3\n102\nA102\nNone\nBob\n\n\n2\n30.1\nNaN\n2.6\n103\nA103\ncherry\nCharlie\n\n\n3\n40.2\n35.8\n0.8\n104\nA104\ndate\nDavid\n\n\n\n\n\n\n\nUsing the skrub selectors and ApplyToCols:\n\nApply the StandardScaler to numeric columns, except \"num_id\".\nApply a OneHotEncoder with sparse_output=False on all string columns except \"str_id\".\n\n\nimport skrub.selectors as s\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom skrub import ApplyToCols\nfrom sklearn.pipeline import make_pipeline\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n\n\nimport skrub.selectors as s\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom skrub import ApplyToCols\nfrom sklearn.pipeline import make_pipeline\n\nscaler = ApplyToCols(StandardScaler(), cols=s.numeric() - \"num_id\")\none_hot = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string() - \"str_id\")\n\ntransformer = make_pipeline(scaler, one_hot)\n\ntransformer.fit_transform(df)\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\ndescription_apple\ndescription_cherry\ndescription_date\ndescription_None\nname_Alice\nname_Bob\nname_Charlie\nname_David\n\n\n\n\n0\n-1.336178\n-1.077965\n-0.820768\n101\nA101\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n-0.449914\n-0.253793\n1.303572\n102\nA102\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n0.436349\nNaN\n0.627646\n103\nA103\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n1.349743\n1.331758\n-1.110450\n104\nA104\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nGiven the same dataframe and using selectors, drop only string columns that contain nulls.\n\nfrom skrub import DropCols\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n\n\nfrom skrub import DropCols\n\nDropCols(cols=s.has_nulls() & s.string()).fit_transform(df)\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\nname\n\n\n\n\n0\n10.5\n5.1\n1.1\n101\nA101\nAlice\n\n\n1\n20.3\n15.6\n3.3\n102\nA102\nBob\n\n\n2\n30.1\nNaN\n2.6\n103\nA103\nCharlie\n\n\n3\n40.2\n35.8\n0.8\n104\nA104\nDavid\n\n\n\n\n\n\n\nNow write a custom function that selects columns where all values are lower than 10.0.\n\nfrom skrub import SelectCols\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n\n\nfrom skrub import SelectCols\n\ndef lower_than(col):\n    return all(col &lt; 10.0)\n\nSelectCols(cols=s.numeric() & s.filter(lower_than)).fit_transform(df)\n\n\n\n\n\n\n\n\nmetric_3\n\n\n\n\n0\n1.1\n\n\n1\n3.3\n\n\n2\n2.6\n\n\n3\n0.8",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Choose your column: selectors</span>"
    ]
  },
  {
    "objectID": "content/chapters/05_feat_eng_numerical.html",
    "href": "content/chapters/05_feat_eng_numerical.html",
    "title": "7  Scaling numerical features safely",
    "section": "",
    "text": "7.1 Exercise: comparing scalers\nNow that we can apply selections to any column we want thanks to ApplyToCols and the selectors, it is time to s\nScale the following data with the scikit-learn StandardScaler and RobustScaler, and then with the SquashingScaler.\nAnswer the following questions: - Mean/std of the transformed data - Max/min of the transformed data - Mean/std of the inliers (use provided iqr function)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Scaling numerical features safely</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html",
    "href": "content/chapters/06_feat_eng_datetimes.html",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "8.1 Introduction to Datetime Features\nDatetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Parsing these formats correctly is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "href": "content/chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "8.2 Converting datetime strings to datetime objects",
    "text": "8.2 Converting datetime strings to datetime objects\nOften, the first operation that must be done to work with datetime objects is converting the datetimes from a string representation to a proper datetime object. This is beneficial because using datetimes gives access to datetime-specific features, and allows to access the different parts of the datetime.\nSkrub provides different objects to deal with the conversion problem.\nToDatetime is a single column transformer that tries to conver the given column to datetime either by relying on a user-provided format, or by guessing common formats. Since this transformer must be applied to single columns (rather than dataframes), it is typically better to use it in conjunction with ApplyToCols. Additionally, the allow_reject parameter of ApplyToCols should be set to True to avoid raising exceptions for non-datetime columns:\n\nfrom skrub import ApplyToCols, ToDatetime\n\nimport pandas as pd\n\ndata = {\n    \"dates\": [\n        \"2023-01-03\",\n        \"2023-02-15\",\n        \"2023-03-27\",\n        \"2023-04-10\",\n    ]\n}\ndf = pd.DataFrame(data)\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nto_datetime works similarly to pd.to_datetime, or the example shown above with ApplyToCols.\n\n\n\n\n\n\nWarning\n\n\n\nto_datetime is a stateless function, so it should not be used in a pipeline, because it does not guarantee consistency between fit_transform and successive transform. ApplyToCols(ToDatetime(), allow_reject=True) is a better solution for pipelines.\n\n\nFinally, the standard Cleaner can be used for parsing datetimes, as it uses ToDatetime under the hood, and can take the datetime_format. As the Cleaner is a transformer, it guarantees consistency between fit_transform and transform.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "8.3 Encoding datetime features",
    "text": "8.3 Encoding datetime features\nDatetimes cannot be used “as-is” for training ML models, and must instead be converted to numerical features. Typically, this is done by “splitting” the datetime parts (year, month, day etc.) into separate columns, so that each column contains only one number.\nAdditional features may also be of interest, such as the number of seconds since epoch (which increases monotonically and gives an indication of the order of entries), whether a date is a weekday or weekend, or the day of the year.\nTo achieve this with standard dataframe libraries, the code looks like this:\n\ndf_enc[\"year\"] = df_enc[\"dates\"].dt.year\ndf_enc[\"month\"] = df_enc[\"dates\"].dt.month\ndf_enc[\"day\"] = df_enc[\"dates\"].dt.day\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"total_seconds\"] = (df_enc[\"dates\"] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(seconds=1)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nyear\nmonth\nday\nweekday\nday_of_year\ntotal_seconds\n\n\n\n\n0\n2023-01-03\n2023\n1\n3\n1\n3\n1672704000\n\n\n1\n2023-02-15\n2023\n2\n15\n2\n46\n1676419200\n\n\n2\n2023-03-27\n2023\n3\n27\n0\n86\n1679875200\n\n\n3\n2023-04-10\n2023\n4\n10\n0\n100\n1681084800\n\n\n\n\n\n\n\nSkrub’s DatetimeEncoder allows to add the same features with a simpler interface. As the DatetimeEncoder is a single column transformer, we use again ApplyToCols.\n\nfrom skrub import DatetimeEncoder\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nde = DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True)\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_month\ndates_day\ndates_total_seconds\ndates_weekday\ndates_day_of_year\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "href": "content/chapters/06_feat_eng_datetimes.html#periodic-features",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "8.4 Periodic features",
    "text": "8.4 Periodic features\nPeriodic features are useful for training machine learning models because they capture the cyclical nature of certain data patterns. For example, time-related features such as hours in a day, days in a week, or months in a year often exhibit periodic behavior. By encoding these features periodically, models can better understand and predict patterns that repeat over time, such as daily traffic trends, weekly sales cycles, or seasonal variations. This ensures that the model treats the start and end of a cycle as close neighbors, improving its ability to generalize and make accurate predictions.\nThis can be done manually with dataframe libraries. For example, circular encoding (a.k.a., trigonometric or sin/cos encoding) can be implemented with Pandas like so:\n\nimport numpy as np \n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_enc[\"day_of_year\"] / 365)\ndf_enc[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_enc[\"day_of_year\"] / 365)\n\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"weekday_sin\"] = np.sin(2 * np.pi * df_enc[\"weekday\"] / 7)\ndf_enc[\"weekday_cos\"] = np.cos(2 * np.pi * df_enc[\"weekday\"] / 7)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023-01-03\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023-02-15\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023-03-27\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023-04-10\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000\n\n\n\n\n\n\n\nAlternatively, the DatetimeEncoder can add periodic features using either circular or spline encoding through the periodic_encoding parameter:\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_total_seconds\ndates_month_circular_0\ndates_month_circular_1\ndates_day_circular_0\ndates_day_circular_1\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023.0\n1.672704e+09\n0.500000\n8.660254e-01\n5.877853e-01\n0.809017\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023.0\n1.676419e+09\n0.866025\n5.000000e-01\n1.224647e-16\n-1.000000\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023.0\n1.679875e+09\n1.000000\n6.123234e-17\n-5.877853e-01\n0.809017\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023.0\n1.681085e+09\n0.866025\n-5.000000e-01\n8.660254e-01\n-0.500000\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "href": "content/chapters/06_feat_eng_datetimes.html#conclusions",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "8.5 Conclusions",
    "text": "8.5 Conclusions\nIn this chapter, we explored the importance and challenges of working with datetime features. We covered how to convert string representations of dates to datetime objects using skrub’s ToDatetime transformer and the Cleaner, both of which can be integrated into pipelines for robust preprocessing.\nWe also discussed the need to encode datetime features into numerical representations suitable for machine learning models. The DatetimeEncoder provides a convenient way to extract useful components such as year, month, day, weekday, day of year, and total seconds since epoch. Additionally, we saw how periodic (circular) encoding can be used to capture cyclical patterns in time-based data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/06_feat_eng_datetimes.html#exercise",
    "href": "content/chapters/06_feat_eng_datetimes.html#exercise",
    "title": "8  Encoding datetime features with DatetimeEncoder",
    "section": "8.6 Exercise",
    "text": "8.6 Exercise\nUse one of the methods explained so far (Cleaner/ApplyToCols) to convert the provided dataframe to datetime dtype, then extract the following features: - All parts of the datetime - The number of seconds from epoch - The day in the week - The day of the year\nHint: use the format \"%d %B %Y\" for the datetime.\n\nimport pandas as pd\n\ndata = {\n    \"admission_dates\": [\n        \"03 January 2023\",\n        \"15 February 2023\",\n        \"27 March 2023\",\n        \"10 April 2023\",\n    ],\n    \"patient_ids\": [101, 102, 103, 104],\n    \"age\": [25, 34, 45, 52],\n    \"outcome\": [\"Recovered\", \"Under Treatment\", \"Recovered\", \"Deceased\"],\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n    admission_dates  patient_ids  age          outcome\n0   03 January 2023          101   25        Recovered\n1  15 February 2023          102   34  Under Treatment\n2     27 March 2023          103   45        Recovered\n3     10 April 2023          104   52         Deceased\n\n\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n\n\n# Solution with ApplyToCols and ToDatetime\nfrom skrub import ApplyToCols, ToDatetime, DatetimeEncoder\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\nto_datetime_encoder = ApplyToCols(ToDatetime(format=\"%d %B %Y\"), cols=\"admission_dates\")\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(to_datetime_encoder, datetime_encoder)\nencoder.fit_transform(df)\n\n\n\n\n\n\n\n\nadmission_dates_year\nadmission_dates_month\nadmission_dates_day\nadmission_dates_total_seconds\nadmission_dates_weekday\nadmission_dates_day_of_year\npatient_ids\nage\noutcome\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n101\n25\nRecovered\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n102\n34\nUnder Treatment\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n103\n45\nRecovered\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0\n104\n52\nDeceased\n\n\n\n\n\n\n\n\n# Solution with Cleaner\nfrom skrub import Cleaner\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(Cleaner(datetime_format=\"%d %B %Y\"), datetime_encoder)\nencoder.fit_transform(df)\n\n\n\n\n\n\n\n\nadmission_dates_year\nadmission_dates_month\nadmission_dates_day\nadmission_dates_total_seconds\nadmission_dates_weekday\nadmission_dates_day_of_year\npatient_ids\nage\noutcome\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n101\n25\nRecovered\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n102\n34\nUnder Treatment\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n103\n45\nRecovered\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0\n104\n52\nDeceased\n\n\n\n\n\n\n\nModify the script so that the DatetimeEncoder adds periodic encoding with sine and cosine (aka circular encoding):\n\n# Write your solution here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n\nNow modify the script above to add spline features (periodic_encoding=\"spline\").\n\n# Solution\nfrom skrub import Cleaner\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\n\ndatetime_encoder = ApplyToCols(\n    DatetimeEncoder(\n        periodic_encoding=\"spline\",\n        add_total_seconds=True,\n        add_weekday=True,\n        add_day_of_year=True,\n    ),\n    cols=s.any_date(),\n)\n\nencoder = make_pipeline(Cleaner(datetime_format=\"%d %B %Y\"), datetime_encoder)\nencoder.fit_transform(df)\n\n\n\n\n\n\n\n\nadmission_dates_year\nadmission_dates_total_seconds\nadmission_dates_day_of_year\nadmission_dates_month_spline_00\nadmission_dates_month_spline_01\nadmission_dates_month_spline_02\nadmission_dates_month_spline_03\nadmission_dates_month_spline_04\nadmission_dates_month_spline_05\nadmission_dates_month_spline_06\n...\nadmission_dates_weekday_spline_0\nadmission_dates_weekday_spline_1\nadmission_dates_weekday_spline_2\nadmission_dates_weekday_spline_3\nadmission_dates_weekday_spline_4\nadmission_dates_weekday_spline_5\nadmission_dates_weekday_spline_6\npatient_ids\nage\noutcome\n\n\n\n\n0\n2023.0\n1.672704e+09\n3.0\n0.0\n0.166667\n0.666667\n0.166667\n0.000000\n0.000000\n0.000000\n...\n0.0\n0.000000\n0.166667\n0.666667\n0.166667\n0.000000\n0.0\n101\n25\nRecovered\n\n\n1\n2023.0\n1.676419e+09\n46.0\n0.0\n0.000000\n0.166667\n0.666667\n0.166667\n0.000000\n0.000000\n...\n0.0\n0.000000\n0.000000\n0.166667\n0.666667\n0.166667\n0.0\n102\n34\nUnder Treatment\n\n\n2\n2023.0\n1.679875e+09\n86.0\n0.0\n0.000000\n0.000000\n0.166667\n0.666667\n0.166667\n0.000000\n...\n0.0\n0.166667\n0.666667\n0.166667\n0.000000\n0.000000\n0.0\n103\n45\nRecovered\n\n\n3\n2023.0\n1.681085e+09\n100.0\n0.0\n0.000000\n0.000000\n0.000000\n0.166667\n0.666667\n0.166667\n...\n0.0\n0.166667\n0.666667\n0.166667\n0.000000\n0.000000\n0.0\n104\n52\nDeceased\n\n\n\n\n4 rows × 29 columns",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Encoding datetime features with `DatetimeEncoder`</span>"
    ]
  },
  {
    "objectID": "content/chapters/07_feat_eng_categorical.html",
    "href": "content/chapters/07_feat_eng_categorical.html",
    "title": "9  Mixed data: dealing with categories",
    "section": "",
    "text": "Low cardinality: OneHotEncoder and OrdinalEncoder\nHigh cardinality default: StringEncoder\nTextEncoder\nOther encoders: GapEncoder, MinHashEncoder",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mixed data: dealing with categories</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html",
    "href": "content/chapters/08_feat_eng_table_vect.html",
    "title": "10  All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "10.1 Exercise\nReplicate the behavior of a TableVectorizer using ApplyToCols, the skrub selectors, and the given transformers.\nfrom skrub import Cleaner, ApplyToCols, StringEncoder, DatetimeEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nimport skrub.selectors as s\nNotes on the implementation:\nUse the following dataframe to test the result.\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", \"private\", \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"datetime-col\": [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n    ]\n    + [None],\n}\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\n2020-02-03T12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\n2021-03-15T00:37:15\n\n\n2\n63\n6.20\nprivate\nlawyer\nTrue\n2022-02-13T17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\n2023-05-22T08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nNone\nUse the following PassThrough transformer where needed.\nfrom skrub._apply_to_cols import SingleColumnTransformer\nclass PassThrough(SingleColumnTransformer):\n    def fit_transform(self, column, y=None):\n        return column\n\n    def transform(self, column):\n        return column\n# Write your code here\n#\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n#\ncleaner = ApplyToCols(Cleaner())\nhigh_cardinality = ApplyToCols(\n    StringEncoder(n_components=2), cols=~s.cardinality_below(4) & (s.string())\n)\nlow_cardinality = ApplyToCols(\n    OneHotEncoder(sparse_output=False, drop=\"if_binary\"),\n    cols=s.cardinality_below(4) & s.string(),\n)\nnumeric = ApplyToCols(PassThrough(), cols=s.numeric())\ndatetime = ApplyToCols(DatetimeEncoder(), cols=s.any_date())\n\nmy_table_vectorizer = make_pipeline(\n    cleaner, numeric, high_cardinality, low_cardinality, datetime\n)\n\nmy_table_vectorizer.fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1_public\nstr2_0\nstr2_1\nbool_True\ndatetime-col_year\ndatetime-col_month\ndatetime-col_day\ndatetime-col_hour\ndatetime-col_total_seconds\n\n\n\n\n0\n15\n5.20\n1.0\n0.820965\n-0.926903\n1.0\n2020.0\n2.0\n3.0\n12.0\n1.580733e+09\n\n\n1\n56\n2.40\n0.0\n0.820959\n-0.926888\n0.0\n2021.0\n3.0\n15.0\n0.0\n1.615769e+09\n\n\n2\n63\n6.20\n0.0\n0.862886\n-0.936535\n1.0\n2022.0\n2.0\n13.0\n17.0\n1.644772e+09\n\n\n3\n12\n10.45\n0.0\n1.029694\n1.352996\n0.0\n2023.0\n5.0\n22.0\n8.0\n1.684745e+09\n\n\n4\n44\n9.00\n1.0\n1.419121\n0.660158\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(\n    high_cardinality=StringEncoder(n_components=2), cardinality_threshold=4\n)\ntv.fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1_public\nstr2_0\nstr2_1\nbool\ndatetime-col_year\ndatetime-col_month\ndatetime-col_day\ndatetime-col_hour\ndatetime-col_total_seconds\n\n\n\n\n0\n15.0\n5.20\n1.0\n0.820970\n-0.926893\n1.0\n2020.0\n2.0\n3.0\n12.0\n1.580733e+09\n\n\n1\n56.0\n2.40\n0.0\n0.820966\n-0.926890\n0.0\n2021.0\n3.0\n15.0\n0.0\n1.615769e+09\n\n\n2\n63.0\n6.20\n0.0\n0.862891\n-0.936528\n1.0\n2022.0\n2.0\n13.0\n17.0\n1.644772e+09\n\n\n3\n12.0\n10.45\n0.0\n1.029687\n1.353006\n0.0\n2023.0\n5.0\n22.0\n8.0\n1.684745e+09\n\n\n4\n44.0\n9.00\n1.0\n1.419119\n0.660159\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/08_feat_eng_table_vect.html#exercise",
    "href": "content/chapters/08_feat_eng_table_vect.html#exercise",
    "title": "10  All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "In the first step, the TableVectorizer cleans the data to parse datetimes and other dtypes.\nNumeric features are left untouched, i.e., they use a Passthrough transformer.\nString and categorical feature are split into high and low cardinality features.\nFor this exercise, set the the cardinality threshold to 4.\nHigh cardinality features are transformed with a StringEncoder. In this exercise, set n_components to 2.\nLow cardinality features are transformed with a OneHotEncoder, and the first category in binary features is dropped (hint: check the docs of the OneHotEncoder for the drop parameter). Set sparse_output=True.\nRemember cardinality_below is one of the skrub selectors.\nDatetimes are transformed by a default DatetimeEncoder.\nEverything should be wrapped in a scikit-learn Pipeline.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All the pre-processing in one place: `TableVectorizer`</span>"
    ]
  },
  {
    "objectID": "content/chapters/09_tabular_pipeline.html",
    "href": "content/chapters/09_tabular_pipeline.html",
    "title": "11  Building a tabular pipeline",
    "section": "",
    "text": "11.1 Exercise:\nWe can now put data cleaning and feature engineering together to build a full machine learning pipeline.\nfrom skrub.datasets import fetch_employee_salaries\nfrom sklearn.datasets import fetch_openml\n\nadult = fetch_openml(\"adult\", version=2)  \nX = adult.data\ny = adult.target\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=\"category\")(X)\nnumerical_columns = selector(dtype_include=\"number\")(X)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel_base = make_pipeline(ct, SimpleImputer(), LogisticRegression())\nmodel_base\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fnlwgt',\n                                                   'education-num',\n                                                   'capital-gain',\n                                                   'capital-loss',\n                                                   'hours-per-week']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['workclass', 'education',\n                                                   'marital-status',\n                                                   'occupation', 'relationship',\n                                                   'race', 'sex',\n                                                   'native-country'])])),\n                ('simpleimputer', SimpleImputer()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('columntransformer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('standardscaler', ...), ('onehotencoder', ...)]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    standardscaler['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    onehotencoder['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'mean'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer()\n\nmodel_tv = make_pipeline(tv, SimpleImputer(), StandardScaler(), LogisticRegression())\nmodel_tv\n\nPipeline(steps=[('tablevectorizer', TableVectorizer()),\n                ('simpleimputer', SimpleImputer()),\n                ('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_threshold \n40\n\n\n\nlow_cardinality \nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinality \nStringEncoder()\n\n\n\nnumeric \nPassThrough()\n\n\n\ndatetime \nDatetimeEncoder()\n\n\n\nspecific_transformers \n()\n\n\n\ndrop_null_fraction \n1.0\n\n\n\ndrop_if_constant \nFalse\n\n\n\ndrop_if_unique \nFalse\n\n\n\ndatetime_format \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolution \n'hour'\n\n\n\nadd_weekday \nFalse\n\n\n\nadd_total_seconds \nTrue\n\n\n\nadd_day_of_year \nFalse\n\n\n\nperiodic_encoding \nNone\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \n'if_binary'\n\n\n\nsparse_output \nFalse\n\n\n\ndtype \n'float32'\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \n30\n\n\n\nvectorizer \n'tfidf'\n\n\n\nngram_range \n(3, ...)\n\n\n\nanalyzer \n'char_wb'\n\n\n\nstop_words \nNone\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'mean'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a tabular pipeline</span>"
    ]
  }
]