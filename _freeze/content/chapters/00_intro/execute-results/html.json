{
  "hash": "6488a1de99df146045643c4b9490325e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A world without skrub\"\nformat:\n    revealjs:\n        slide-number: true\n        toc: true\n        code-fold: false\n        code-tools: true\n\n---\n\nLet's begin the lesson by imagining a world without skrub, where we can use \nonly Pandas and scikit-learn to clean data and prepare a machine learning model. \n\n::: {#7af3d99d .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom skrub.datasets import fetch_employee_salaries\n\ndata = fetch_employee_salaries()\nX, y = data.X, data.y\n```\n:::\n\n\nLet's take a look at the target::\n\n::: {#5b7454d1 .cell execution_count=2}\n``` {.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0        69222.18\n1        97392.47\n2       104717.28\n3        52734.57\n4        93396.00\n          ...    \n9223     72094.53\n9224    169543.85\n9225    102736.52\n9226    153747.50\n9227     75484.08\nName: current_annual_salary, Length: 9228, dtype: float64\n```\n:::\n:::\n\n\nThis is a numerical column, and our task is predicting the value of `current_annual_salary`.\n\n## Strategizing\nWe can begin by exploring the dataframe with `.describe`, and then think of a \nplan for pre-processing our data. \n\n::: {#e84e35fa .cell execution_count=3}\n``` {.python .cell-code}\nX.describe(include=\"all\")\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>department</th>\n      <th>department_name</th>\n      <th>division</th>\n      <th>assignment_category</th>\n      <th>employee_position_title</th>\n      <th>date_first_hired</th>\n      <th>year_first_hired</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9211</td>\n      <td>9228</td>\n      <td>9228</td>\n      <td>9228</td>\n      <td>9228</td>\n      <td>9228</td>\n      <td>9228</td>\n      <td>9228.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n      <td>37</td>\n      <td>37</td>\n      <td>694</td>\n      <td>2</td>\n      <td>443</td>\n      <td>2264</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>M</td>\n      <td>POL</td>\n      <td>Department of Police</td>\n      <td>School Health Services</td>\n      <td>Fulltime-Regular</td>\n      <td>Bus Operator</td>\n      <td>12/12/2016</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5481</td>\n      <td>1844</td>\n      <td>1844</td>\n      <td>300</td>\n      <td>8394</td>\n      <td>638</td>\n      <td>87</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2003.597529</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.327078</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1965.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1998.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2005.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2012.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2016.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe need to:\n\n- Impute some missing values in the `gender` column.\n- Encode convert categorical features into numerical features. \n- Convert the column `date_first_hired` into numerical features.\n\nOnce we have processed the data, we can train a machine learning model. For the sake\nof the example, we will use a linear model (`Ridge`), which means that we need to scale numerical features, besides imputing missing values. \n\nFinally, we want to evaluate the performance of the method across multiple \ncross-validation splits.\n\n## Building a traditional pipeline\nLet's build a traditional predictive pipeline following the steps we just discussed. \n\n### Step 1: Convert date features to numerical\n\nExtract numerical features from the `date_first_hired` column.\n\n::: {#3047d965 .cell execution_count=4}\n``` {.python .cell-code}\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['years_since_hired'] = (pd.Timestamp.now() - X_processed['date_first_hired']).dt.days / 365.25\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeatures after date transformation:\n\nShape: (9228, 10)\n```\n:::\n:::\n\n\n### Step 2: Encode categorical features\n\nEncode only the non-numerical categorical features using one-hot encoding.\n\n::: {#8377a7ee .cell execution_count=5}\n``` {.python .cell-code}\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1219)\n```\n:::\n:::\n\n\n### Step 3: Impute missing values\n\nWe'll impute missing values in the `gender` column using the most frequent strategy.\n\n::: {#292fe093 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)\n```\n:::\n\n\n### Step 4: Scale numerical features\n\nScale numerical features for the Ridge regression model.\n\n::: {#838da329 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)\n```\n:::\n\n\n### Step 5: Train Ridge model with cross-validation\n\nTrain a Ridge regression model and evaluate with cross-validation.\n\n::: {#3290d074 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10366.9520 (+/- 1403.5225)\n```\n:::\n:::\n\n\n### \"Just ask an agent to write the code\"\nIt's what I did. Here are some of the issues I noticed: \n\n- Operations in the wrong order.\n- Trying to impute categorical features without encoding them as numerical values.\n- The datetime feature was encoded as a categorical (i.e, with dummmies).\n- Too many print statements.\n- Cells could not be executed in order without proper debugging and re-prompting.\n\n\n## Waking up from a nightmare\nThankfully, we live in a world where we can `import skrub`. Let's see what we can\nget if we use `skrub.tabular_pipeline`. \n\n::: {#0c5952cc .cell execution_count=9}\n``` {.python .cell-code}\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-Validation Results:\nMean test R²: 0.9096 (+/- 0.0147)\nMean test RMSE: 8742.8548 (+/- 981.1682)\n```\n:::\n:::\n\n\nAll the code from before, the tokens and the debugging are replaced by a single \nimport that gives better results.\n\nThroughout the tutorial, we will see how each step can be simplified, replaced, or\nimproved using skrub features.\n\n",
    "supporting": [
      "00_intro_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}