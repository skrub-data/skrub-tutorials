[
  {
    "objectID": "chapters/quiz_04.html",
    "href": "chapters/quiz_04.html",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "What is the proper way of setting the number of components used by the high cardinality encoder in the TableVectorizer?\n\nA) By passing n_components as kwargs:\n\nTableVectorizer(n_components=10)\n\nB) By creating a new encoder with the specified parameter:\n\nnew_encoder = StringEncoder(n_components=10)\nTableVectorizer(high_cardinality=new_encoder)\n\nC) Both A) and B) are correct\nD) None of the above\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\n\n\n\n\n\n\n\n\n\n\n\n\nThe TableVectorizer categorizes columns based on their characteristics. What are the types of columns that it handles?\n\nA) Numeric, categorical, datetimes\nB) Numeric, datetimes, high cardinality categorical, low cardinality categorical\nC) Integer, float, string, categorical, datetimes\nD) Numeric, datetimes, high cardinality categorical, low cardinality categorical, and complex objects such as structs or lists\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nBy default, the TableVectorizer converts complex objects to their string representation, before checking their cardinality. No special transformation is provided for columns that contain structs or lists."
  },
  {
    "objectID": "chapters/quiz_04.html#question-1",
    "href": "chapters/quiz_04.html#question-1",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "What is the proper way of setting the number of components used by the high cardinality encoder in the TableVectorizer?\n\nA) By passing n_components as kwargs:\n\nTableVectorizer(n_components=10)\n\nB) By creating a new encoder with the specified parameter:\n\nnew_encoder = StringEncoder(n_components=10)\nTableVectorizer(high_cardinality=new_encoder)\n\nC) Both A) and B) are correct\nD) None of the above\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)"
  },
  {
    "objectID": "chapters/quiz_04.html#question-2",
    "href": "chapters/quiz_04.html#question-2",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "The TableVectorizer categorizes columns based on their characteristics. What are the types of columns that it handles?\n\nA) Numeric, categorical, datetimes\nB) Numeric, datetimes, high cardinality categorical, low cardinality categorical\nC) Integer, float, string, categorical, datetimes\nD) Numeric, datetimes, high cardinality categorical, low cardinality categorical, and complex objects such as structs or lists\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nBy default, the TableVectorizer converts complex objects to their string representation, before checking their cardinality. No special transformation is provided for columns that contain structs or lists."
  },
  {
    "objectID": "chapters/quiz_04.html#question-3",
    "href": "chapters/quiz_04.html#question-3",
    "title": "Quiz: Column-level transformations",
    "section": "Question 3",
    "text": "Question 3\n\n\n\n\n\n\nWhat are the default estimators used by the tabular_pipeline when it is called with \"regression\" and \"classification\" respectively?\ntabular_pipeline(\"regression\")\ntabular_pipeline(\"classification\")\n\nA) RidgeCV and ExtraTreeClassifier\nB) HistGradientBoostingRegressor and LogisticRegression\nC) RandomForestRegressor and RandomForestClassifier\nD) HistGradientBoostingRegressor and HistGradientBoosting\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: D)"
  },
  {
    "objectID": "chapters/quiz_04.html#question-4",
    "href": "chapters/quiz_04.html#question-4",
    "title": "Quiz: Column-level transformations",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nHow are linear models (Ridge, LogisticRegression) handled by the tabular_pipeline?\n\nA) The TableVectorizer uses default parameters, no other steps are added to the pipeline.\nB) The TableVectorizer uses default parameters, SimpleImputer and StandardScaler are added before the encoder.\nC) The TableVectorizer uses a datetime encoder that periodic encodings with splines, SimpleImputer and StandardScaler are added before the encoder.\nD) The TableVectorizer uses a datetime encoder that periodic encodings with splines, no other steps are added to the pipeline.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: C)"
  },
  {
    "objectID": "chapters/quiz_02.html",
    "href": "chapters/quiz_02.html",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider this diagram. Which column transformer can replicate this behavior if it wraps a OneHotEncoder?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: A) ApplyToCols takes a transformer, then clones it and applies it separately to each column under selection (in this case, Name and Desc). Columns that were not selected are left unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nConsider this diagram. Which column transformer can replicate this behavior if it wraps a PCA?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B) ApplyToFrame takes a transformer and a list of columns (usually, a subset of the columns in the dataframe), then applies the transformer to all the selected columns at once, replacing them with the output of the transfromer. Columns that were not selected are left unchanged.\n\n\n\n\n\n\n\n\n\n\n\n\nConsider these transformers:\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\nIs the output of these two snippets the same? In other words, does the order of the transformers matter?\ncase_1 = make_pipeline(encode, scale)\ncase_1.fit_transform(df)\ncase_2 = make_pipeline(scale, encode)\ncase_2.fit_transform(df)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: No, the output is different.\nThe order of the operations matters, and a different order leads to different results.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols, ApplyToCols, ApplyToFrame\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport skrub.selectors as s \n\nn_patients = 5\ndf = pd.DataFrame(\n    {\n        \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n        \"age\": np.random.randint(18, 80, size=n_patients),\n        \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n    }\n)\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\n\ncase_1 = make_pipeline(encode, scale)\ndf_1 = case_1.fit_transform(df)\ndf_1.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n2.0\n-0.5\n-0.5\n-0.5\n-0.5\n-0.755087\n0.816497\n-0.816497\n\n\n1\n-0.5\n2.0\n-0.5\n-0.5\n-0.5\n-0.401140\n-1.224745\n1.224745\n\n\n2\n-0.5\n-0.5\n2.0\n-0.5\n-0.5\n1.958506\n-1.224745\n1.224745\n\n\n3\n-0.5\n-0.5\n-0.5\n2.0\n-0.5\n-0.637104\n0.816497\n-0.816497\n\n\n4\n-0.5\n-0.5\n-0.5\n-0.5\n2.0\n-0.165175\n0.816497\n-0.816497\n\n\n\n\n\n\n\n\ncase_2 = make_pipeline(scale, encode)\ndf_2 = case_2.fit_transform(df)\ndf_2.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n-0.755087\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.401140\n0.0\n1.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n1.958506\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n-0.637104\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n-0.165175\n1.0\n0.0"
  },
  {
    "objectID": "chapters/quiz_02.html#question-1",
    "href": "chapters/quiz_02.html#question-1",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider this diagram. Which column transformer can replicate this behavior if it wraps a OneHotEncoder?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: A) ApplyToCols takes a transformer, then clones it and applies it separately to each column under selection (in this case, Name and Desc). Columns that were not selected are left unchanged."
  },
  {
    "objectID": "chapters/quiz_02.html#question-2",
    "href": "chapters/quiz_02.html#question-2",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider this diagram. Which column transformer can replicate this behavior if it wraps a PCA?\n\n\nA) ApplyToCols\nB) ApplyToFrame\nC) DropCols\nD) SelectCols\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B) ApplyToFrame takes a transformer and a list of columns (usually, a subset of the columns in the dataframe), then applies the transformer to all the selected columns at once, replacing them with the output of the transfromer. Columns that were not selected are left unchanged."
  },
  {
    "objectID": "chapters/quiz_02.html#question-3",
    "href": "chapters/quiz_02.html#question-3",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider these transformers:\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\nIs the output of these two snippets the same? In other words, does the order of the transformers matter?\ncase_1 = make_pipeline(encode, scale)\ncase_1.fit_transform(df)\ncase_2 = make_pipeline(scale, encode)\ncase_2.fit_transform(df)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: No, the output is different.\nThe order of the operations matters, and a different order leads to different results.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols, ApplyToCols, ApplyToFrame\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport skrub.selectors as s \n\nn_patients = 5\ndf = pd.DataFrame(\n    {\n        \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n        \"age\": np.random.randint(18, 80, size=n_patients),\n        \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n    }\n)\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\n\ncase_1 = make_pipeline(encode, scale)\ndf_1 = case_1.fit_transform(df)\ndf_1.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n2.0\n-0.5\n-0.5\n-0.5\n-0.5\n-0.755087\n0.816497\n-0.816497\n\n\n1\n-0.5\n2.0\n-0.5\n-0.5\n-0.5\n-0.401140\n-1.224745\n1.224745\n\n\n2\n-0.5\n-0.5\n2.0\n-0.5\n-0.5\n1.958506\n-1.224745\n1.224745\n\n\n3\n-0.5\n-0.5\n-0.5\n2.0\n-0.5\n-0.637104\n0.816497\n-0.816497\n\n\n4\n-0.5\n-0.5\n-0.5\n-0.5\n2.0\n-0.165175\n0.816497\n-0.816497\n\n\n\n\n\n\n\n\ncase_2 = make_pipeline(scale, encode)\ndf_2 = case_2.fit_transform(df)\ndf_2.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n-0.755087\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.401140\n0.0\n1.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n1.958506\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n-0.637104\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n-0.165175\n1.0\n0.0"
  },
  {
    "objectID": "chapters/quiz_02.html#question-4",
    "href": "chapters/quiz_02.html#question-4",
    "title": "Quiz: Column-level transformations",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nWhat does this selector do?\n\nfrom skrub import SelectCols\nimport skrub.selectors as s\n\ndef fun(col):\n    mean = col.mean()\n    return mean &gt; 40000\n\nsel = s.numeric() & s.filter(fun) \nt = SelectCols(cols=sel)\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\n\n\n\n\n0\n45000.0\n\n\n1\n52000.0\n\n\n2\n61000.0\n\n\n3\nNaN\n\n\n4\n48000.0\n\n\n\n\n\n\n\n\nA) It drops only numerical columns with mean &lt; 40000\nB) It selects only numerical column with mean &gt; 40000\nC) It selects non-numeric columns or numeric columns that have mean &gt; 40000\nD) It drops only numeric columns unless their mean is &lt; 40000\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\n\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\n\n\n\n\n0\n45000.0\n\n\n1\n52000.0\n\n\n2\n61000.0\n\n\n3\nNaN\n\n\n4\n48000.0"
  },
  {
    "objectID": "chapters/quiz_02.html#question-5",
    "href": "chapters/quiz_02.html#question-5",
    "title": "Quiz: Column-level transformations",
    "section": "Question 5",
    "text": "Question 5\n\n\n\n\n\n\nWhat does this selector do?\n\nsel = s.cols(\"salary\") | s.filter_names(lambda name: name.endswith(\"_title\"))\n\nt = SelectCols(cols=sel)\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\njob_title\ndepartment_title\n\n\n\n\n0\n45000.0\nengineer\nIT\n\n\n1\n52000.0\nanalyst\nFinance\n\n\n2\n61000.0\nconsultant\nConsulting\n\n\n3\nNaN\ndesigner\nDesign\n\n\n4\n48000.0\ndeveloper\nDevelopment\n\n\n\n\n\n\n\n\nA) It selects the column “salary”, numerical columns, and columns that end in “_title”\nB) It drops all columns (including “salary”), except the columns that end in “_title”\nC) It selects the column “salary”, and columns whose name end in “_title”\nD) It drops the column “salary”, and columns whose name end in “_title”\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: C)\n\nt.fit_transform(df)\n\n\n\n\n\n\n\n\nsalary\njob_title\ndepartment_title\n\n\n\n\n0\n45000.0\nengineer\nIT\n\n\n1\n52000.0\nanalyst\nFinance\n\n\n2\n61000.0\nconsultant\nConsulting\n\n\n3\nNaN\ndesigner\nDesign\n\n\n4\n48000.0\ndeveloper\nDevelopment"
  },
  {
    "objectID": "chapters/08_table_vectorizer.html",
    "href": "chapters/08_table_vectorizer.html",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "Machine learning models typically require numeric input features. When working with real-world datasets, we often have a mix of data types: numbers, text, dates, and categorical values. The TableVectorizer automates the entire process of converting a heterogeneous dataframe into a matrix of numeric features ready for machine learning.\nInstead of manually specifying how to handle each column, the TableVectorizer automatically detects the data type of each column and applies the appropriate transformation to encode the column using numerical features."
  },
  {
    "objectID": "chapters/08_table_vectorizer.html#introduction",
    "href": "chapters/08_table_vectorizer.html#introduction",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "",
    "text": "Machine learning models typically require numeric input features. When working with real-world datasets, we often have a mix of data types: numbers, text, dates, and categorical values. The TableVectorizer automates the entire process of converting a heterogeneous dataframe into a matrix of numeric features ready for machine learning.\nInstead of manually specifying how to handle each column, the TableVectorizer automatically detects the data type of each column and applies the appropriate transformation to encode the column using numerical features."
  },
  {
    "objectID": "chapters/08_table_vectorizer.html#how-does-the-tablevectorizer-work",
    "href": "chapters/08_table_vectorizer.html#how-does-the-tablevectorizer-work",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "How does the TableVectorizer work?",
    "text": "How does the TableVectorizer work?\nThe TableVectorizer operates in two phases:\n\nPhase 1: Data cleaning and type detection\nFirst, it runs a Cleaner on the input data to:\n\nDetect and parse datetime columns (possibly, with custom datetime formats)\nDetect and parse numerical columns written as strings\nHandle missing values represented as strings (e.g., “N/A”)\nClean up categorical columns to have consistent typing\nRemove uninformative columns (those with only nulls, constant values, or all unique values)\nFinally, convert all numerical features to float32 to reduce the computational cost.\n\nThis ensures that each column has the correct data type before encoding.\n\n\nPhase 2: Column dispatch and encoding\nAfter cleaning, the TableVectorizer categorizes columns and dispatches them to the appropriate transformer based on their data type and cardinality. Categorical columns with a cardinality (i.e., number of unique values) larger than 40 (by default)) are considered “high cardinality”, while all other categorical columns are “low cardinality”.\nThe TableVectorizer uses the following default transformers for each column type:\n\nNumeric columns: Left untouched (passthrough) - they’re already in the right format\nDatetime columns: Transformed by DatetimeEncoder to extract meaningful temporal features\nLow-cardinality categorical/string columns: Transformed with OneHotEncoder to create binary indicator variables\nHigh-cardinality categorical/string columns: Transformed with StringEncoder to create dense numeric representations"
  },
  {
    "objectID": "chapters/08_table_vectorizer.html#key-parameters",
    "href": "chapters/08_table_vectorizer.html#key-parameters",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "Key Parameters",
    "text": "Key Parameters\n\nCardinality threshold\nThe cardinality threshold that splits columns in “high” and “low” cardinality can be changed by setting the relative parameter:\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(cardinality_threshold=10)  # Adjust the threshold\n\n\n\nData cleaning parameters\nThe TableVectorizer forwards several parameters to the internal Cleaner, which behave in the same way:\n\ndrop_null_fraction: Fraction of nulls above which a column is dropped (default: 1.0)\ndrop_if_constant: Drop columns with only one unique value (default: False)\ndrop_if_unique: Drop string/categorical columns where all values are unique (default: False)\ndatetime_format: Format string for parsing dates\n\n\ntv = TableVectorizer(\n    drop_null_fraction=0.9,  # Drop columns that are 90% null\n    drop_if_constant=True,\n    datetime_format=\"%Y-%m-%d\"\n)\n\n\n\nCustomizing the transformers used by TableVectorizer\nThe TableVectorizer applies whatever transformer is provided to each of the numeric, datetime, high_cardinality, and low_cardinality paramters. To tweak the default parameters of the transformers a new transformer should be provided:\n\nfrom skrub import TableVectorizer, DatetimeEncoder, StringEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Create custom transformers\ndatetime_enc = DatetimeEncoder(periodic_encoding=\"circular\")\nstring_enc = StringEncoder(n_components=10)\n\n# Pass them to TableVectorizer\ntv = TableVectorizer(\n    datetime=datetime_enc,\n    high_cardinality=string_enc,\n)\n\nThis allows to, for example, change the number of parameters in the StringEncoder, provide a custom datetime format for the DatetimeEncoder, or use a completely different encoder such as the TextEncoder.\n\n\nApplying the TableVectorizer only to a subset of columns\nBy default, the TableVectorizer is applied to all the columns in the given dataframe. In some cases, it may be important to keep specific columns “as is”, so that they are not modified by the transformer.\nThis can be done by wrapping the vectorizer into an ApplyToCols object.\nFor example, in this case we might want to avoid modifying the two *_id columns.\n\nimport pandas as pd\nfrom skrub import ApplyToCols\nimport skrub.selectors as s\n\ndf = pd.DataFrame(\n    {\n        \"metric_1\": [10.5, 20.3, 30.1, 40.2],\n        \"metric_2\": [5.1, 15.6, None, 35.8],\n        \"metric_3\": [1.1, 3.3, 2.6, .8],\n        \"num_id\": [101, 102, 103, 104],\n        \"str_id\": [\"A101\", \"A102\", \"A103\", \"A104\"],\n        \"description\": [\"apple\", None, \"cherry\", \"date\"],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\ndescription\nname\n\n\n\n\n0\n10.5\n5.1\n1.1\n101\nA101\napple\nAlice\n\n\n1\n20.3\n15.6\n3.3\n102\nA102\nNone\nBob\n\n\n2\n30.1\nNaN\n2.6\n103\nA103\ncherry\nCharlie\n\n\n3\n40.2\n35.8\n0.8\n104\nA104\ndate\nDavid\n\n\n\n\n\n\n\nWe can use ApplyToCols and the skrub selectors as follows:\n\ntv = ApplyToCols(TableVectorizer(), cols=s.all()-s.glob(\"*_id\"))\ndf_enc = tv.fit_transform(df)\n\nprint(\"Original\")\nprint(df[[\"num_id\", \"str_id\"]])\nprint(\"\\nEncoded\")\nprint(df_enc[[\"num_id\", \"str_id\"]])\n\nOriginal\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\nEncoded\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\n\nThe id strings are the same, while all other columns have been encoded as expected.\n\n\nUsing specific_transformers for more low-level control\nFor fine-grained control, we can specify transformers for specific columns using the specific_transformers parameter. This is useful when we want to override the default behavior for particular columns:\n\nfrom sklearn.preprocessing import OrdinalEncoder\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"occupation\": [\"engineer\", \"teacher\", \"doctor\"],\n    \"salary\": [100000, 50000, 150000]\n})\n\n# Create a custom transformer for the 'occupation' column\nspecific_transformers = [(OrdinalEncoder(), [\"occupation\"])]\n\ntv = TableVectorizer(specific_transformers=specific_transformers)\nresult = tv.fit_transform(df)\n\nImportant notes about specific_transformers:\n\nColumns specified here bypass the default categorization logic\nThe transformer receives the column as-is, without any preprocessing\nThe transformer must be able to handle the column’s current data type and values\nFor more complex transformations, consider using ApplyToCols and the selectors API (explained in the previous chapters), or the skrub Data Ops."
  },
  {
    "objectID": "chapters/08_table_vectorizer.html#conclusions",
    "href": "chapters/08_table_vectorizer.html#conclusions",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "Conclusions",
    "text": "Conclusions\nThe TableVectorizer is a self-contained feature engineering engine that\n\nCleans your data to have consistent representation of data types and null values, and\nEncodes all columns depending on their data type and characteristics using good defaults.\n\nThe idea behind the TableVectorizer is that you should be able to provide any dataframe, and get a good feature matrix based on that dataframe as a result.\nThe TableVectorizer makes use of most of the objects that have been explained so far, and is an important part of the tabular_pipeline explained in the next chapter."
  },
  {
    "objectID": "chapters/08_table_vectorizer.slides.html#introduction",
    "href": "chapters/08_table_vectorizer.slides.html#introduction",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "Introduction",
    "text": "Introduction\nMachine learning models typically require numeric input features. When working with real-world datasets, we often have a mix of data types: numbers, text, dates, and categorical values. The TableVectorizer automates the entire process of converting a heterogeneous dataframe into a matrix of numeric features ready for machine learning.\nInstead of manually specifying how to handle each column, the TableVectorizer automatically detects the data type of each column and applies the appropriate transformation to encode the column using numerical features."
  },
  {
    "objectID": "chapters/08_table_vectorizer.slides.html#how-does-the-tablevectorizer-work",
    "href": "chapters/08_table_vectorizer.slides.html#how-does-the-tablevectorizer-work",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "How does the TableVectorizer work?",
    "text": "How does the TableVectorizer work?\nThe TableVectorizer operates in two phases:\nPhase 1: Data cleaning and type detection\nFirst, it runs a Cleaner on the input data to:\n\nDetect and parse datetime columns (possibly, with custom datetime formats)\nDetect and parse numerical columns written as strings\nHandle missing values represented as strings (e.g., “N/A”)\nClean up categorical columns to have consistent typing\nRemove uninformative columns (those with only nulls, constant values, or all unique values)\nFinally, convert all numerical features to float32 to reduce the computational cost.\n\nThis ensures that each column has the correct data type before encoding.\nPhase 2: Column dispatch and encoding\nAfter cleaning, the TableVectorizer categorizes columns and dispatches them to the appropriate transformer based on their data type and cardinality. Categorical columns with a cardinality (i.e., number of unique values) larger than 40 (by default)) are considered “high cardinality”, while all other categorical columns are “low cardinality”.\nThe TableVectorizer uses the following default transformers for each column type:\n\nNumeric columns: Left untouched (passthrough) - they’re already in the right format\nDatetime columns: Transformed by DatetimeEncoder to extract meaningful temporal features\nLow-cardinality categorical/string columns: Transformed with OneHotEncoder to create binary indicator variables\nHigh-cardinality categorical/string columns: Transformed with StringEncoder to create dense numeric representations"
  },
  {
    "objectID": "chapters/08_table_vectorizer.slides.html#key-parameters",
    "href": "chapters/08_table_vectorizer.slides.html#key-parameters",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "Key Parameters",
    "text": "Key Parameters\nCardinality threshold\nThe cardinality threshold that splits columns in “high” and “low” cardinality can be changed by setting the relative parameter:\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer(cardinality_threshold=10)  # Adjust the threshold\n\nData cleaning parameters\nThe TableVectorizer forwards several parameters to the internal Cleaner, which behave in the same way:\n\ndrop_null_fraction: Fraction of nulls above which a column is dropped (default: 1.0)\ndrop_if_constant: Drop columns with only one unique value (default: False)\ndrop_if_unique: Drop string/categorical columns where all values are unique (default: False)\ndatetime_format: Format string for parsing dates\n\n\ntv = TableVectorizer(\n    drop_null_fraction=0.9,  # Drop columns that are 90% null\n    drop_if_constant=True,\n    datetime_format=\"%Y-%m-%d\"\n)\n\nCustomizing the transformers used by TableVectorizer\nThe TableVectorizer applies whatever transformer is provided to each of the numeric, datetime, high_cardinality, and low_cardinality paramters. To tweak the default parameters of the transformers a new transformer should be provided:\n\nfrom skrub import TableVectorizer, DatetimeEncoder, StringEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Create custom transformers\ndatetime_enc = DatetimeEncoder(periodic_encoding=\"circular\")\nstring_enc = StringEncoder(n_components=10)\n\n# Pass them to TableVectorizer\ntv = TableVectorizer(\n    datetime=datetime_enc,\n    high_cardinality=string_enc,\n)\n\nThis allows to, for example, change the number of parameters in the StringEncoder, provide a custom datetime format for the DatetimeEncoder, or use a completely different encoder such as the TextEncoder.\nApplying the TableVectorizer only to a subset of columns\nBy default, the TableVectorizer is applied to all the columns in the given dataframe. In some cases, it may be important to keep specific columns “as is”, so that they are not modified by the transformer.\nThis can be done by wrapping the vectorizer into an ApplyToCols object.\nFor example, in this case we might want to avoid modifying the two *_id columns.\n\nimport pandas as pd\nfrom skrub import ApplyToCols\nimport skrub.selectors as s\n\ndf = pd.DataFrame(\n    {\n        \"metric_1\": [10.5, 20.3, 30.1, 40.2],\n        \"metric_2\": [5.1, 15.6, None, 35.8],\n        \"metric_3\": [1.1, 3.3, 2.6, .8],\n        \"num_id\": [101, 102, 103, 104],\n        \"str_id\": [\"A101\", \"A102\", \"A103\", \"A104\"],\n        \"description\": [\"apple\", None, \"cherry\", \"date\"],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nmetric_1\nmetric_2\nmetric_3\nnum_id\nstr_id\ndescription\nname\n\n\n\n\n0\n10.5\n5.1\n1.1\n101\nA101\napple\nAlice\n\n\n1\n20.3\n15.6\n3.3\n102\nA102\nNone\nBob\n\n\n2\n30.1\nNaN\n2.6\n103\nA103\ncherry\nCharlie\n\n\n3\n40.2\n35.8\n0.8\n104\nA104\ndate\nDavid\n\n\n\n\n\n\n\nWe can use ApplyToCols and the skrub selectors as follows:\n\ntv = ApplyToCols(TableVectorizer(), cols=s.all()-s.glob(\"*_id\"))\ndf_enc = tv.fit_transform(df)\n\nprint(\"Original\")\nprint(df[[\"num_id\", \"str_id\"]])\nprint(\"\\nEncoded\")\nprint(df_enc[[\"num_id\", \"str_id\"]])\n\nOriginal\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\nEncoded\n   num_id str_id\n0     101   A101\n1     102   A102\n2     103   A103\n3     104   A104\n\n\nThe id strings are the same, while all other columns have been encoded as expected.\nUsing specific_transformers for more low-level control\nFor fine-grained control, we can specify transformers for specific columns using the specific_transformers parameter. This is useful when we want to override the default behavior for particular columns:\n\nfrom sklearn.preprocessing import OrdinalEncoder\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"occupation\": [\"engineer\", \"teacher\", \"doctor\"],\n    \"salary\": [100000, 50000, 150000]\n})\n\n# Create a custom transformer for the 'occupation' column\nspecific_transformers = [(OrdinalEncoder(), [\"occupation\"])]\n\ntv = TableVectorizer(specific_transformers=specific_transformers)\nresult = tv.fit_transform(df)\n\nImportant notes about specific_transformers:\n\nColumns specified here bypass the default categorization logic\nThe transformer receives the column as-is, without any preprocessing\nThe transformer must be able to handle the column’s current data type and values\nFor more complex transformations, consider using ApplyToCols and the selectors API (explained in the previous chapters), or the skrub Data Ops."
  },
  {
    "objectID": "chapters/08_table_vectorizer.slides.html#conclusions",
    "href": "chapters/08_table_vectorizer.slides.html#conclusions",
    "title": "All the pre-processing in one place: TableVectorizer",
    "section": "Conclusions",
    "text": "Conclusions\nThe TableVectorizer is a self-contained feature engineering engine that\n\nCleans your data to have consistent representation of data types and null values, and\nEncodes all columns depending on their data type and characteristics using good defaults.\n\nThe idea behind the TableVectorizer is that you should be able to provide any dataframe, and get a good feature matrix based on that dataframe as a result.\nThe TableVectorizer makes use of most of the objects that have been explained so far, and is an important part of the tabular_pipeline explained in the next chapter."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html",
    "href": "chapters/06_feat_eng_datetimes.html",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "Datetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Correct parsing of these and more exotic formats is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html#introduction",
    "href": "chapters/06_feat_eng_datetimes.html#introduction",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "",
    "text": "Datetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Correct parsing of these and more exotic formats is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "href": "chapters/06_feat_eng_datetimes.html#converting-datetime-strings-to-datetime-objects",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Converting datetime strings to datetime objects",
    "text": "Converting datetime strings to datetime objects\nOften, the first operation that must be done to work with datetime objects is converting the datetimes from a string representation to a proper datetime object. This is beneficial because using datetimes gives access to datetime-specific features, and allows to access the different parts of the datetime.\nSkrub provides different objects to deal with the conversion problem.\nToDatetime is a single column transformer that tries to conver the given column to datetime either by relying on a user-provided format, or by guessing common formats. Since this transformer must be applied to single columns (rather than dataframes), it is typically better to use it in conjunction with ApplyToCols. Additionally, the allow_reject parameter of ApplyToCols should be set to True to avoid raising exceptions for non-datetime columns:\n\nfrom skrub import ApplyToCols, ToDatetime\n\nimport pandas as pd\n\ndata = {\n    \"dates\": [\n        \"2023-01-03\",\n        \"2023-02-15\",\n        \"2023-03-27\",\n        \"2023-04-10\",\n    ]\n}\ndf = pd.DataFrame(data)\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\ndf_enc.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   dates   4 non-null      datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 164.0 bytes\n\n\nto_datetime works similarly to pd.to_datetime, or the example shown above with ApplyToCols.\n\n\n\n\n\n\nWarning\n\n\n\nto_datetime is a stateless function, so it should not be used in a pipeline, because it does not guarantee consistency between fit_transform and successive transform. ApplyToCols(ToDatetime(), allow_reject=True) is a better solution for pipelines.\n\n\nFinally, the standard Cleaner can be used for parsing datetimes, as it uses ToDatetime under the hood, and can take the datetime_format. As the Cleaner is a transformer, it guarantees consistency between fit_transform and transform."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "href": "chapters/06_feat_eng_datetimes.html#encoding-datetime-features",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\nDatetimes cannot be used “as-is” for training ML models, and must instead be converted to numerical features. Typically, this is done by “splitting” the datetime parts (year, month, day etc.) into separate columns, so that each column contains only one number.\nAdditional features may also be of interest, such as the number of seconds since epoch (which increases monotonically and gives an indication of the order of entries), whether a date is a weekday or weekend, or the day of the year.\nTo achieve this with standard dataframe libraries, the code looks like this:\n\ndf_enc[\"year\"] = df_enc[\"dates\"].dt.year\ndf_enc[\"month\"] = df_enc[\"dates\"].dt.month\ndf_enc[\"day\"] = df_enc[\"dates\"].dt.day\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"total_seconds\"] = (\n    df_enc[\"dates\"] - pd.Timestamp(\"1970-01-01\")\n) // pd.Timedelta(seconds=1)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nyear\nmonth\nday\nweekday\nday_of_year\ntotal_seconds\n\n\n\n\n0\n2023-01-03\n2023\n1\n3\n1\n3\n1672704000\n\n\n1\n2023-02-15\n2023\n2\n15\n2\n46\n1676419200\n\n\n2\n2023-03-27\n2023\n3\n27\n0\n86\n1679875200\n\n\n3\n2023-04-10\n2023\n4\n10\n0\n100\n1681084800\n\n\n\n\n\n\n\nSkrub’s DatetimeEncoder allows to add the same features with a simpler interface. As the DatetimeEncoder is a single column transformer, we use again ApplyToCols.\nThe DatetimeEncoder includes various parameters to add more features to the transformed dataframe: - add_total_seconds adds the number of seconds since Epoch (1970-01-01) - add_weekday adds the day in the week (to highlight weekends, for example) - add_day_of_year adds the day in year of the datetime\n\nfrom skrub import DatetimeEncoder\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nde = DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True)\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_month\ndates_day\ndates_total_seconds\ndates_weekday\ndates_day_of_year\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0"
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html#periodic-features",
    "href": "chapters/06_feat_eng_datetimes.html#periodic-features",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Periodic features",
    "text": "Periodic features\nPeriodic features are useful for training machine learning models because they capture the cyclical nature of certain data patterns. For example, features such as hours in a day or days in a week often exhibit periodic behavior. By encoding these features periodically, models can better understand and predict patterns that repeat over time, such as daily traffic trends, or seasonal variations. This ensures that the model treats the start and end of a cycle as close neighbors, improving its ability to generalize and make accurate predictions.\nThis can be done manually with dataframe libraries. For example, circular encoding (a.k.a., trigonometric or sin/cos encoding) can be implemented with Pandas like so:\n\nimport numpy as np \n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_enc[\"day_of_year\"] / 365)\ndf_enc[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_enc[\"day_of_year\"] / 365)\n\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"weekday_sin\"] = np.sin(2 * np.pi * df_enc[\"weekday\"] / 7)\ndf_enc[\"weekday_cos\"] = np.cos(2 * np.pi * df_enc[\"weekday\"] / 7)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023-01-03\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023-02-15\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023-03-27\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023-04-10\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000\n\n\n\n\n\n\n\nAlternatively, the DatetimeEncoder can add periodic features using either circular or spline encoding through the periodic_encoding parameter:\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_total_seconds\ndates_month_circular_0\ndates_month_circular_1\ndates_day_circular_0\ndates_day_circular_1\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023.0\n1.672704e+09\n0.500000\n8.660254e-01\n5.877853e-01\n0.809017\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023.0\n1.676419e+09\n0.866025\n5.000000e-01\n1.224647e-16\n-1.000000\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023.0\n1.679875e+09\n1.000000\n6.123234e-17\n-5.877853e-01\n0.809017\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023.0\n1.681085e+09\n0.866025\n-5.000000e-01\n8.660254e-01\n-0.500000\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000"
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.html#conclusions",
    "href": "chapters/06_feat_eng_datetimes.html#conclusions",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter, we explored the importance and challenges of working with datetime features. We covered how to convert string representations of dates to datetime objects using skrub’s ToDatetime transformer and the Cleaner, both of which can be integrated into pipelines for robust preprocessing.\nWe also discussed the need to encode datetime features into numerical representations suitable for machine learning models. The DatetimeEncoder provides a convenient way to extract useful components such as year, month, day, weekday, day of year, and total seconds since epoch. Additionally, we saw how periodic (circular) encoding can be used to capture cyclical patterns in time-based data.\nIn the next chapter, we will cover the final type of columns: categorical/string columns."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.slides.html#introduction",
    "href": "chapters/06_feat_eng_datetimes.slides.html#introduction",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Introduction",
    "text": "Introduction\nDatetime features are very important for many data analysis and machine learning tasks, as they often carry significant information about temporal patterns and trends. For instance, including as features the day of the week, time of day, or season can provide valuable insights for predictive modeling.\nHowever, working with datetime data can be difficult due to the variety of formats in which dates and times are represented. Typical formats include \"%Y-%m-%d\", \"%d/%m/%Y\", and \"%d %B %Y\", among others. Correct parsing of these and more exotic formats is essential to avoid errors and ensure accurate feature extraction.\nIn this section we are going to cover how skrub can help with dealing with datetimes using to_datetime, ToDatetime, and the DatetimeEncoder."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.slides.html#converting-datetime-strings-to-datetime-objects",
    "href": "chapters/06_feat_eng_datetimes.slides.html#converting-datetime-strings-to-datetime-objects",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Converting datetime strings to datetime objects",
    "text": "Converting datetime strings to datetime objects\nOften, the first operation that must be done to work with datetime objects is converting the datetimes from a string representation to a proper datetime object. This is beneficial because using datetimes gives access to datetime-specific features, and allows to access the different parts of the datetime.\nSkrub provides different objects to deal with the conversion problem.\nToDatetime is a single column transformer that tries to conver the given column to datetime either by relying on a user-provided format, or by guessing common formats. Since this transformer must be applied to single columns (rather than dataframes), it is typically better to use it in conjunction with ApplyToCols. Additionally, the allow_reject parameter of ApplyToCols should be set to True to avoid raising exceptions for non-datetime columns:\n\nfrom skrub import ApplyToCols, ToDatetime\n\nimport pandas as pd\n\ndata = {\n    \"dates\": [\n        \"2023-01-03\",\n        \"2023-02-15\",\n        \"2023-03-27\",\n        \"2023-04-10\",\n    ]\n}\ndf = pd.DataFrame(data)\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\ndf_enc.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   dates   4 non-null      datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 164.0 bytes\n\n\nto_datetime works similarly to pd.to_datetime, or the example shown above with ApplyToCols.\n\n\n\n\n\n\nWarning\n\n\nto_datetime is a stateless function, so it should not be used in a pipeline, because it does not guarantee consistency between fit_transform and successive transform. ApplyToCols(ToDatetime(), allow_reject=True) is a better solution for pipelines.\n\n\n\nFinally, the standard Cleaner can be used for parsing datetimes, as it uses ToDatetime under the hood, and can take the datetime_format. As the Cleaner is a transformer, it guarantees consistency between fit_transform and transform."
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.slides.html#encoding-datetime-features",
    "href": "chapters/06_feat_eng_datetimes.slides.html#encoding-datetime-features",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\nDatetimes cannot be used “as-is” for training ML models, and must instead be converted to numerical features. Typically, this is done by “splitting” the datetime parts (year, month, day etc.) into separate columns, so that each column contains only one number.\nAdditional features may also be of interest, such as the number of seconds since epoch (which increases monotonically and gives an indication of the order of entries), whether a date is a weekday or weekend, or the day of the year.\nTo achieve this with standard dataframe libraries, the code looks like this:\n\ndf_enc[\"year\"] = df_enc[\"dates\"].dt.year\ndf_enc[\"month\"] = df_enc[\"dates\"].dt.month\ndf_enc[\"day\"] = df_enc[\"dates\"].dt.day\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"total_seconds\"] = (\n    df_enc[\"dates\"] - pd.Timestamp(\"1970-01-01\")\n) // pd.Timedelta(seconds=1)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nyear\nmonth\nday\nweekday\nday_of_year\ntotal_seconds\n\n\n\n\n0\n2023-01-03\n2023\n1\n3\n1\n3\n1672704000\n\n\n1\n2023-02-15\n2023\n2\n15\n2\n46\n1676419200\n\n\n2\n2023-03-27\n2023\n3\n27\n0\n86\n1679875200\n\n\n3\n2023-04-10\n2023\n4\n10\n0\n100\n1681084800\n\n\n\n\n\n\n\nSkrub’s DatetimeEncoder allows to add the same features with a simpler interface. As the DatetimeEncoder is a single column transformer, we use again ApplyToCols.\nThe DatetimeEncoder includes various parameters to add more features to the transformed dataframe: - add_total_seconds adds the number of seconds since Epoch (1970-01-01) - add_weekday adds the day in the week (to highlight weekends, for example) - add_day_of_year adds the day in year of the datetime\n\nfrom skrub import DatetimeEncoder\n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\nde = DatetimeEncoder(add_total_seconds=True, add_weekday=True, add_day_of_year=True)\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_month\ndates_day\ndates_total_seconds\ndates_weekday\ndates_day_of_year\n\n\n\n\n0\n2023.0\n1.0\n3.0\n1.672704e+09\n2.0\n3.0\n\n\n1\n2023.0\n2.0\n15.0\n1.676419e+09\n3.0\n46.0\n\n\n2\n2023.0\n3.0\n27.0\n1.679875e+09\n1.0\n86.0\n\n\n3\n2023.0\n4.0\n10.0\n1.681085e+09\n1.0\n100.0"
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.slides.html#periodic-features",
    "href": "chapters/06_feat_eng_datetimes.slides.html#periodic-features",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Periodic features",
    "text": "Periodic features\nPeriodic features are useful for training machine learning models because they capture the cyclical nature of certain data patterns. For example, features such as hours in a day or days in a week often exhibit periodic behavior. By encoding these features periodically, models can better understand and predict patterns that repeat over time, such as daily traffic trends, or seasonal variations. This ensures that the model treats the start and end of a cycle as close neighbors, improving its ability to generalize and make accurate predictions.\nThis can be done manually with dataframe libraries. For example, circular encoding (a.k.a., trigonometric or sin/cos encoding) can be implemented with Pandas like so:\n\nimport numpy as np \n\ndf_enc = ApplyToCols(ToDatetime(), allow_reject=True).fit_transform(df)\n\ndf_enc[\"day_of_year\"] = df_enc[\"dates\"].dt.day_of_year\ndf_enc[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_enc[\"day_of_year\"] / 365)\ndf_enc[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_enc[\"day_of_year\"] / 365)\n\ndf_enc[\"weekday\"] = df_enc[\"dates\"].dt.weekday\ndf_enc[\"weekday_sin\"] = np.sin(2 * np.pi * df_enc[\"weekday\"] / 7)\ndf_enc[\"weekday_cos\"] = np.cos(2 * np.pi * df_enc[\"weekday\"] / 7)\n\ndf_enc\n\n\n\n\n\n\n\n\ndates\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023-01-03\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023-02-15\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023-03-27\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023-04-10\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000\n\n\n\n\n\n\n\nAlternatively, the DatetimeEncoder can add periodic features using either circular or spline encoding through the periodic_encoding parameter:\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\n\ndf_enc = ApplyToCols(de, cols=\"dates\").fit_transform(df_enc)\ndf_enc\n\n\n\n\n\n\n\n\ndates_year\ndates_total_seconds\ndates_month_circular_0\ndates_month_circular_1\ndates_day_circular_0\ndates_day_circular_1\nday_of_year\nday_of_year_sin\nday_of_year_cos\nweekday\nweekday_sin\nweekday_cos\n\n\n\n\n0\n2023.0\n1.672704e+09\n0.500000\n8.660254e-01\n5.877853e-01\n0.809017\n3\n0.051620\n0.998667\n1\n0.781831\n0.623490\n\n\n1\n2023.0\n1.676419e+09\n0.866025\n5.000000e-01\n1.224647e-16\n-1.000000\n46\n0.711657\n0.702527\n2\n0.974928\n-0.222521\n\n\n2\n2023.0\n1.679875e+09\n1.000000\n6.123234e-17\n-5.877853e-01\n0.809017\n86\n0.995919\n0.090252\n0\n0.000000\n1.000000\n\n\n3\n2023.0\n1.681085e+09\n0.866025\n-5.000000e-01\n8.660254e-01\n-0.500000\n100\n0.988678\n-0.150055\n0\n0.000000\n1.000000"
  },
  {
    "objectID": "chapters/06_feat_eng_datetimes.slides.html#conclusions",
    "href": "chapters/06_feat_eng_datetimes.slides.html#conclusions",
    "title": "Encoding datetime features with DatetimeEncoder",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter, we explored the importance and challenges of working with datetime features. We covered how to convert string representations of dates to datetime objects using skrub’s ToDatetime transformer and the Cleaner, both of which can be integrated into pipelines for robust preprocessing.\nWe also discussed the need to encode datetime features into numerical representations suitable for machine learning models. The DatetimeEncoder provides a convenient way to extract useful components such as year, month, day, weekday, day of year, and total seconds since epoch. Additionally, we saw how periodic (circular) encoding can be used to capture cyclical patterns in time-based data.\nIn the next chapter, we will cover the final type of columns: categorical/string columns."
  },
  {
    "objectID": "chapters/04_selectors.html",
    "href": "chapters/04_selectors.html",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "Very often, column selection is more complex than simply passing a list of column names to a transformer: it may be necessary to select all columns that have a specific data type, or based on some other characteristic (presence of nulls, column cardinality, etc.).\nThe skrub selectors implement a number of selection strategies that can be combined in various ways to build complex filtering conditions that can then be employed by ApplyToCols, ApplyToFrame, SelectCols and DropCols.\n\n\nSelectors are available from the skrub.selectors namespace:\n\nimport skrub.selectors as s\n\nWe will use this example dataframe to test some of the selectors:\n\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\n\n\n\n\n\n\n\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\n\n.float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\n\n\n\n\n\n\n\nAdditional conditions include:\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\n\n\n\n\n\n\n\nVarious selectors allow to choose columns based on their name:\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions\n\n\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+\n\n\n\n\n\n\n\n\n\n\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -; for example to select all columns except for “datetime-col”, one would write:\n\nSelectCols(cols=s.all() - \"datetime-col\").fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\n\n\n\n\n\n\n\nTo select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT\n\n\n\n\n\n\n\n\n\n\nSelectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+\n\n\n\n\n\n\n\n\n\n\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function on the given column is True. It is also possible to pass arguments to the function to further tweak the conditions. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0\n\n\n\n\n\n\n\n\n\n\nIn this chapter we covered the skrub selectors, how they allow to select specific columns either through simple conditions, or by combining different selectors. More use cases of ApplyToCols and the selectors will be shown through the rest of the course, starting from the next chapter on feature engineering."
  },
  {
    "objectID": "chapters/04_selectors.html#skrub-selectors",
    "href": "chapters/04_selectors.html#skrub-selectors",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "Selectors are available from the skrub.selectors namespace:\n\nimport skrub.selectors as s\n\nWe will use this example dataframe to test some of the selectors:\n\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\n\n\n\n\n\n\n\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\n\n.float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\n\n\n\n\n\n\n\nAdditional conditions include:\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\n\n\n\n\n\n\n\nVarious selectors allow to choose columns based on their name:\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions\n\n\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+"
  },
  {
    "objectID": "chapters/04_selectors.html#combining-selectors",
    "href": "chapters/04_selectors.html#combining-selectors",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "Selectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -; for example to select all columns except for “datetime-col”, one would write:\n\nSelectCols(cols=s.all() - \"datetime-col\").fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\n\n\n\n\n\n\n\nTo select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT"
  },
  {
    "objectID": "chapters/04_selectors.html#extracting-selected-columns",
    "href": "chapters/04_selectors.html#extracting-selected-columns",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "Selectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+"
  },
  {
    "objectID": "chapters/04_selectors.html#designing-custom-filters",
    "href": "chapters/04_selectors.html#designing-custom-filters",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "Finally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function on the given column is True. It is also possible to pass arguments to the function to further tweak the conditions. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0"
  },
  {
    "objectID": "chapters/04_selectors.html#conclusion",
    "href": "chapters/04_selectors.html#conclusion",
    "title": "Choose your column: selectors",
    "section": "",
    "text": "In this chapter we covered the skrub selectors, how they allow to select specific columns either through simple conditions, or by combining different selectors. More use cases of ApplyToCols and the selectors will be shown through the rest of the course, starting from the next chapter on feature engineering."
  },
  {
    "objectID": "chapters/04_selectors.slides.html#skrub-selectors",
    "href": "chapters/04_selectors.slides.html#skrub-selectors",
    "title": "Choose your column: selectors",
    "section": "Skrub selectors",
    "text": "Skrub selectors\nSelectors are available from the skrub.selectors namespace:\n\nimport skrub.selectors as s\n\nWe will use this example dataframe to test some of the selectors:\n\nimport pandas as pd\nimport datetime\n\ndata = {\n    \"int\": [15, 56, 63, 12, 44],\n    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n    \"str1\": [\"public\", \"private\", None, \"private\", \"public\"],\n    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n    \"bool\": [True, False, True, False, True],\n    \"cat1\": pd.Categorical([\"yes\", \"yes\", None, \"yes\", \"no\"]),\n    \"cat2\": pd.Categorical([\"20K+\", \"40K+\", \"60K+\", \"30K+\", \"50K+\"]),\n    \"datetime-col\": [\n        datetime.datetime.fromisoformat(dt)\n        for dt in [\n            \"2020-02-03T12:30:05\",\n            \"2021-03-15T00:37:15\",\n            \"2022-02-13T17:03:25\",\n            \"2023-05-22T08:45:55\",\n        ]\n    ]\n    + [None],    }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\ndatetime-col\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n2020-02-03 12:30:05\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n2021-03-15 00:37:15\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n2022-02-13 17:03:25\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n2023-05-22 08:45:55\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\nNaT\n\n\n\n\n\n\n\nSelectors should be used in conjunction with the transformers described in the previous chapter: ApplyToCols, ApplyToFrame, SelectCols and DropCols.\nSelectors allow to filter columns by data type:\n\n.float: floating-point columns\n.integer: integer columns\n.any_date: date or datetime columns\n.boolean: boolean columns\n.string: columns with a String data type\n.categorical: columns with a Categorical data type\n.numeric: numeric (either integer or float) columns\n\n\nfrom skrub import SelectCols\nstring_selector = s.string()\n\nSelectCols(cols=string_selector).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\nstr2\n\n\n\n\n0\npublic\nofficer\n\n\n1\nprivate\nmanager\n\n\n2\nNone\nlawyer\n\n\n3\nprivate\nchef\n\n\n4\npublic\nteacher\n\n\n\n\n\n\n\nAdditional conditions include:\n\n.all: select all columns\n.cardinality_below: select all columns with a number of unique values lower than the given threshold\n.has_nulls: select all columns that include at least one null value\n\n\nSelectCols(cols=s.has_nulls()).fit_transform(df)\n\n\n\n\n\n\n\n\nstr1\ncat1\ndatetime-col\n\n\n\n\n0\npublic\nyes\n2020-02-03 12:30:05\n\n\n1\nprivate\nyes\n2021-03-15 00:37:15\n\n\n2\nNone\nNaN\n2022-02-13 17:03:25\n\n\n3\nprivate\nyes\n2023-05-22 08:45:55\n\n\n4\npublic\nno\nNaT\n\n\n\n\n\n\n\nVarious selectors allow to choose columns based on their name:\n\n.cols: choose the provided column name (or list of names)\n\nnote that transformers that can accept selectors can also take column names or lists of columns by default\n\n.glob: use Unix shell style glob to select column names\n.regex: select columns using regular expressions\n\n\nSelectCols(cols=s.glob(\"cat*\")).fit_transform(df)\n\n\n\n\n\n\n\n\ncat1\ncat2\n\n\n\n\n0\nyes\n20K+\n\n\n1\nyes\n40K+\n\n\n2\nNaN\n60K+\n\n\n3\nyes\n30K+\n\n\n4\nno\n50K+"
  },
  {
    "objectID": "chapters/04_selectors.slides.html#combining-selectors",
    "href": "chapters/04_selectors.slides.html#combining-selectors",
    "title": "Choose your column: selectors",
    "section": "Combining selectors",
    "text": "Combining selectors\nSelectors can be inverted using .inv or the logical operator ~ to select all other columns, and they can be combined using the & and | logical operators. It is also possible to remove from a selection with -; for example to select all columns except for “datetime-col”, one would write:\n\nSelectCols(cols=s.all() - \"datetime-col\").fit_transform(df)\n\n\n\n\n\n\n\n\nint\nfloat\nstr1\nstr2\nbool\ncat1\ncat2\n\n\n\n\n0\n15\n5.20\npublic\nofficer\nTrue\nyes\n20K+\n\n\n1\n56\n2.40\nprivate\nmanager\nFalse\nyes\n40K+\n\n\n2\n63\n6.20\nNone\nlawyer\nTrue\nNaN\n60K+\n\n\n3\n12\n10.45\nprivate\nchef\nFalse\nyes\n30K+\n\n\n4\n44\n9.00\npublic\nteacher\nTrue\nno\n50K+\n\n\n\n\n\n\n\nTo select all datetime columns OR all string columns that do not contain nulls, we can do:\n\nSelectCols(cols=(s.any_date() | (s.string()) & (~s.has_nulls()))).fit_transform(df)\n\n\n\n\n\n\n\n\nstr2\ndatetime-col\n\n\n\n\n0\nofficer\n2020-02-03 12:30:05\n\n\n1\nmanager\n2021-03-15 00:37:15\n\n\n2\nlawyer\n2022-02-13 17:03:25\n\n\n3\nchef\n2023-05-22 08:45:55\n\n\n4\nteacher\nNaT"
  },
  {
    "objectID": "chapters/04_selectors.slides.html#extracting-selected-columns",
    "href": "chapters/04_selectors.slides.html#extracting-selected-columns",
    "title": "Choose your column: selectors",
    "section": "Extracting selected columns",
    "text": "Extracting selected columns\nSelectors can use the expand and expand_index methods to extract the columns that have been selected:\n\nhas_nulls = s.has_nulls()\nhas_nulls.expand(df)\n\n['str1', 'cat1', 'datetime-col']\n\n\nThis can be used, for example, to pass a list of columns to a dataframe library.\n\ndf.drop(columns=has_nulls.expand(df))\n\n\n\n\n\n\n\n\nint\nfloat\nstr2\nbool\ncat2\n\n\n\n\n0\n15\n5.20\nofficer\nTrue\n20K+\n\n\n1\n56\n2.40\nmanager\nFalse\n40K+\n\n\n2\n63\n6.20\nlawyer\nTrue\n60K+\n\n\n3\n12\n10.45\nchef\nFalse\n30K+\n\n\n4\n44\n9.00\nteacher\nTrue\n50K+"
  },
  {
    "objectID": "chapters/04_selectors.slides.html#designing-custom-filters",
    "href": "chapters/04_selectors.slides.html#designing-custom-filters",
    "title": "Choose your column: selectors",
    "section": "Designing custom filters",
    "text": "Designing custom filters\nFinally, it is possible to define function-based selectors using .filter and .filter_names.\n.filter selects columns for which the predicate evaluated by a user-defined function on the given column is True. It is also possible to pass arguments to the function to further tweak the conditions. For example, it is possible to select columns that include a certain amount of nulls by defining a function like the following:\n\nimport pandas as pd\nimport skrub.selectors as s\nfrom skrub import DropCols\n\ndf = pd.DataFrame({\"a\": [None, None, None, 1], \"b\": [1,2,3,4]})\n\ndef more_nulls_than(col, threshold=.5):\n    return col.isnull().sum()/len(col) &gt; threshold\n\nDropCols(cols=s.filter(more_nulls_than, threshold=0.5)).fit_transform(df)\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n.filter_names is similar to .filter in the sense that it takes a function that returns a predicate, but in this case the function is evaluated over the column names.\nIf we define this example dataframe:\n\nfrom skrub import selectors as s\nimport pandas as pd\ndf = pd.DataFrame(\n    {\n        \"height_mm\": [297.0, 420.0],\n        \"width_mm\": [210.0, 297.0],\n        \"kind\": [\"A4\", \"A3\"],\n        \"ID\": [4, 3],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\nkind\nID\n\n\n\n\n0\n297.0\n210.0\nA4\n4\n\n\n1\n420.0\n297.0\nA3\n3\n\n\n\n\n\n\n\nWe can select all the columns that end with \"_mm\" as follows:\n\nselector = s.filter_names(lambda name: name.endswith('_mm'))\ns.select(df, selector)\n\n\n\n\n\n\n\n\nheight_mm\nwidth_mm\n\n\n\n\n0\n297.0\n210.0\n\n\n1\n420.0\n297.0"
  },
  {
    "objectID": "chapters/04_selectors.slides.html#conclusion",
    "href": "chapters/04_selectors.slides.html#conclusion",
    "title": "Choose your column: selectors",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we covered the skrub selectors, how they allow to select specific columns either through simple conditions, or by combining different selectors. More use cases of ApplyToCols and the selectors will be shown through the rest of the course, starting from the next chapter on feature engineering."
  },
  {
    "objectID": "chapters/02_cleaning_data.html",
    "href": "chapters/02_cleaning_data.html",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "",
    "text": "In this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner."
  },
  {
    "objectID": "chapters/02_cleaning_data.html#introduction",
    "href": "chapters/02_cleaning_data.html#introduction",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "",
    "text": "In this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner."
  },
  {
    "objectID": "chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "href": "chapters/02_cleaning_data.html#using-the-skrub-cleaner",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Using the skrub Cleaner",
    "text": "Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column:\nConsider this example dataframe:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"numerical_1\": [1, 2, 3, 4, 5],\n        \"numerical_2\": [10.5, 20.3, None, 40.1, 50.2],\n        \"string_column\": [\"apple\", \"?\", \"banana\", \"cherry\", \"?\"],\n        \"datetime_column\": [\n            \"03 Jan 2020\",\n            \"04 Jan 2020\",\n            \"05 Jan 2020\",\n            \"06 Jan 2020\",\n            \"07 Jan 2020\",\n        ],\n        \"all_none\": [None, None, None, None, None],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\nall_none\n\n\n\n\n0\n1\n10.5\napple\n03 Jan 2020\nNone\n\n\n1\n2\n20.3\n?\n04 Jan 2020\nNone\n\n\n2\n3\nNaN\nbanana\n05 Jan 2020\nNone\n\n\n3\n4\n40.1\ncherry\n06 Jan 2020\nNone\n\n\n4\n5\n50.2\n?\n07 Jan 2020\nNone\n\n\n\n\n\n\n\nThis dataframe has mixed type columns, with some of the missing values denoted as None and some \"?\". The datetime column has a non-standard format and has been parsed as a string column. Finally, one of the columns is completely empty.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   numerical_1      5 non-null      int64  \n 1   numerical_2      4 non-null      float64\n 2   string_column    5 non-null      object \n 3   datetime_column  5 non-null      object \n 4   all_none         0 non-null      object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 332.0+ bytes\n\n\n\nUsing plain pandas\nCleaning this dataset using plain pandas may require writing code like this:\n\n# Parse the datetime strings with a specific format\ndf['datetime_column'] = pd.to_datetime(df['datetime_column'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_clean = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Replace \"?\" with np.nan in all string columns\ndf_clean = df_clean.replace(\"?\", np.nan)\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_clean = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_clean.columns[df_clean.eq('').all()]\n    df_clean = df_clean.drop(columns=empty_string_cols)\n    return df_clean\n\n# Apply the function to the DataFrame\ndf_clean = drop_empty_columns(df_clean)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNaN\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNaN\n2020-01-07\n\n\n\n\n\n\n\n\n\nThe alternative: skrub.Cleaner\nBy default, the Cleaner applies various transformations that can sanitize many common use cases:\n\nfrom skrub import Cleaner\ndf_clean = Cleaner().fit_transform(df)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNone\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNone\n2020-01-07\n\n\n\n\n\n\n\nWe can see that the cleaned version of the dataframe is now marking missing values correctly, and that the datetime column has been parsed accordingly:\n\ndf_clean.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   numerical_1      5 non-null      int64         \n 1   numerical_2      4 non-null      float64       \n 2   string_column    3 non-null      object        \n 3   datetime_column  5 non-null      datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes\n\n\n\n\nCleaning steps performed by the Cleaner\nIn more detail, the Cleaner executes the following steps in order:\n\nIt replaces common strings used to represent missing values (e.g., NULL, ?) with NA markers.\nIt uses the DropUninformative transformer to decide whether a column is “uninformative”, that is, it is not likely to bring information useful to train a ML model. For example, empty columns are uninformative.\nIt tries to parse datetime columns using common formats, or a user-provided datetime_format.\nIt processes categorical columns to ensure consistent typing depending on the dataframe library in use.\nIt converts columns to string, unless they have a data type that carries more information, such as numerical, datetime, and categorial columns.\nIt automatically parses numbers written as strings (“1.324”) to convert them to actual numerical columns.\nFinally, it can convert numerical columns to np.float32 dtype if called with the parameter numeric_dtype=\"float32\". This ensures a consistent representation of numbers and missing values, and helps reducing the memory footprint."
  },
  {
    "objectID": "chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "href": "chapters/02_cleaning_data.html#under-the-hood-dropuninformative",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Under the hood: DropUninformative",
    "text": "Under the hood: DropUninformative\nWhen the Cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that do not bring useful information for training a ML model, and should therefore be dropped.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction.\n\nBy default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nSetting the threshold to None will disable this check and therefore retain empty columns.\n\nIt contains only one value, and no missing values.\n\nThis is controlled by the drop_if_constant flag, which is False by default.\n\nAll values in the column are distinct.\n\nThis may be the case if the column contains UIDs, but it can also happen when the column contains text.\nThis check is off by default and can be turned on by setting drop_if_unique to True.\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf2 = pd.DataFrame({\n    \"id\": [101, 102, 103, 104, 105],\n    \"age\": [34, 28, 45, 52, 39],\n    \"salary\": [70000, None, 85000, None, None], \n    \"department\": [\"HR\", \"Finance\", \"IT\", \"HR\", \"Finance\"],\n    \"constant_col\": [\"active\"] * 5,  # single constant value\n})\ndf2\n\n\n\n\n\n\n\n\nid\nage\nsalary\ndepartment\nconstant_col\n\n\n\n\n0\n101\n34\n70000.0\nHR\nactive\n\n\n1\n102\n28\nNaN\nFinance\nactive\n\n\n2\n103\n45\n85000.0\nIT\nactive\n\n\n3\n104\n52\nNaN\nHR\nactive\n\n\n4\n105\n39\nNaN\nFinance\nactive\n\n\n\n\n\n\n\n\nfrom skrub import ApplyToCols, DropUninformative\n\ndrop = ApplyToCols(DropUninformative(drop_if_constant=True, drop_null_fraction=0.5))\ndrop.fit_transform(df2)\n\n\n\n\n\n\n\n\nid\nage\ndepartment\n\n\n\n\n0\n101\n34\nHR\n\n\n1\n102\n28\nFinance\n\n\n2\n103\n45\nIT\n\n\n3\n104\n52\nHR\n\n\n4\n105\n39\nFinance"
  },
  {
    "objectID": "chapters/02_cleaning_data.html#conclusion",
    "href": "chapters/02_cleaning_data.html#conclusion",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we have covered how the skrub Cleaner helps with sanitizing data by implementing a number of common transformations that need to be executed in order to ensure that the data used by the pipeline are consistent and can be used as indended by ML models.\n\nThe Cleaner object automates common data cleaning steps, making it easy to prepare tabular data for analysis or modeling with minimal configuration.\nCleaner leverages transformers like DropUninformative to automatically remove columns that are empty, constant, or contain mostly unique values (such as IDs), based on user-defined thresholds.\nThe DropUninformative transformer can also be used directly for fine-grained control over which columns to drop, according to missing value fraction, constant values, or uniqueness.\nskrub objects are designed to work seamlessly with pandas DataFrames, streamlining the preprocessing workflow and reducing the need for manual data cleaning code.\n\nIn the next chapter we will see how skrub helps with applying this and other transformations to specific columns in the data."
  },
  {
    "objectID": "chapters/02_cleaning_data.slides.html#introduction",
    "href": "chapters/02_cleaning_data.slides.html#introduction",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we will show how we can quickly pre-process and sanitize data using skrub’s Cleaner."
  },
  {
    "objectID": "chapters/02_cleaning_data.slides.html#using-the-skrub-cleaner",
    "href": "chapters/02_cleaning_data.slides.html#using-the-skrub-cleaner",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Using the skrub Cleaner",
    "text": "Using the skrub Cleaner\nThe Cleaner is intended to be a first step in preparing tabular data for analysis or modeling, and can handle a variety of common data cleaning tasks automatically. It is designed to work out-of-the-box with minimal configuration, although it is also possible to customize its behavior if needed.\nGiven a dataframe, the Cleaner applies a sequence of transformers to each column:\nConsider this example dataframe:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"numerical_1\": [1, 2, 3, 4, 5],\n        \"numerical_2\": [10.5, 20.3, None, 40.1, 50.2],\n        \"string_column\": [\"apple\", \"?\", \"banana\", \"cherry\", \"?\"],\n        \"datetime_column\": [\n            \"03 Jan 2020\",\n            \"04 Jan 2020\",\n            \"05 Jan 2020\",\n            \"06 Jan 2020\",\n            \"07 Jan 2020\",\n        ],\n        \"all_none\": [None, None, None, None, None],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\nall_none\n\n\n\n\n0\n1\n10.5\napple\n03 Jan 2020\nNone\n\n\n1\n2\n20.3\n?\n04 Jan 2020\nNone\n\n\n2\n3\nNaN\nbanana\n05 Jan 2020\nNone\n\n\n3\n4\n40.1\ncherry\n06 Jan 2020\nNone\n\n\n4\n5\n50.2\n?\n07 Jan 2020\nNone\n\n\n\n\n\n\n\nThis dataframe has mixed type columns, with some of the missing values denoted as None and some \"?\". The datetime column has a non-standard format and has been parsed as a string column. Finally, one of the columns is completely empty.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   numerical_1      5 non-null      int64  \n 1   numerical_2      4 non-null      float64\n 2   string_column    5 non-null      object \n 3   datetime_column  5 non-null      object \n 4   all_none         0 non-null      object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 332.0+ bytes\n\n\nUsing plain pandas\nCleaning this dataset using plain pandas may require writing code like this:\n\n# Parse the datetime strings with a specific format\ndf['datetime_column'] = pd.to_datetime(df['datetime_column'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_clean = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Replace \"?\" with np.nan in all string columns\ndf_clean = df_clean.replace(\"?\", np.nan)\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_clean = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_clean.columns[df_clean.eq('').all()]\n    df_clean = df_clean.drop(columns=empty_string_cols)\n    return df_clean\n\n# Apply the function to the DataFrame\ndf_clean = drop_empty_columns(df_clean)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNaN\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNaN\n2020-01-07\n\n\n\n\n\n\n\nThe alternative: skrub.Cleaner\nBy default, the Cleaner applies various transformations that can sanitize many common use cases:\n\nfrom skrub import Cleaner\ndf_clean = Cleaner().fit_transform(df)\ndf_clean\n\n\n\n\n\n\n\n\nnumerical_1\nnumerical_2\nstring_column\ndatetime_column\n\n\n\n\n0\n1\n10.5\napple\n2020-01-03\n\n\n1\n2\n20.3\nNone\n2020-01-04\n\n\n2\n3\nNaN\nbanana\n2020-01-05\n\n\n3\n4\n40.1\ncherry\n2020-01-06\n\n\n4\n5\n50.2\nNone\n2020-01-07\n\n\n\n\n\n\n\nWe can see that the cleaned version of the dataframe is now marking missing values correctly, and that the datetime column has been parsed accordingly:\n\ndf_clean.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   numerical_1      5 non-null      int64         \n 1   numerical_2      4 non-null      float64       \n 2   string_column    3 non-null      object        \n 3   datetime_column  5 non-null      datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1), object(1)\nmemory usage: 292.0+ bytes"
  },
  {
    "objectID": "chapters/02_cleaning_data.slides.html#under-the-hood-dropuninformative",
    "href": "chapters/02_cleaning_data.slides.html#under-the-hood-dropuninformative",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Under the hood: DropUninformative",
    "text": "Under the hood: DropUninformative\nWhen the Cleaner is fitted on a dataframe, it checks whether the dataframe includes uninformative columns, that is columns that do not bring useful information for training a ML model, and should therefore be dropped.\nThis is done by the DropUninformative transformer, which is a standalone transformer that the Cleaner leverages to sanitize data. DropUninformative marks a columns as “uninformative” if it satisfies one of these conditions:\n\nThe fraction of missing values is larger than the threshold provided by the user with drop_null_fraction.\n\nBy default, this threshold is 1.0, i.e., only columns that contain only missing values are dropped.\nSetting the threshold to None will disable this check and therefore retain empty columns.\n\nIt contains only one value, and no missing values.\n\nThis is controlled by the drop_if_constant flag, which is False by default.\n\nAll values in the column are distinct.\n\nThis may be the case if the column contains UIDs, but it can also happen when the column contains text.\nThis check is off by default and can be turned on by setting drop_if_unique to True.\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf2 = pd.DataFrame({\n    \"id\": [101, 102, 103, 104, 105],\n    \"age\": [34, 28, 45, 52, 39],\n    \"salary\": [70000, None, 85000, None, None], \n    \"department\": [\"HR\", \"Finance\", \"IT\", \"HR\", \"Finance\"],\n    \"constant_col\": [\"active\"] * 5,  # single constant value\n})\ndf2\n\n\n\n\n\n\n\n\nid\nage\nsalary\ndepartment\nconstant_col\n\n\n\n\n0\n101\n34\n70000.0\nHR\nactive\n\n\n1\n102\n28\nNaN\nFinance\nactive\n\n\n2\n103\n45\n85000.0\nIT\nactive\n\n\n3\n104\n52\nNaN\nHR\nactive\n\n\n4\n105\n39\nNaN\nFinance\nactive\n\n\n\n\n\n\n\n\nfrom skrub import ApplyToCols, DropUninformative\n\ndrop = ApplyToCols(DropUninformative(drop_if_constant=True, drop_null_fraction=0.5))\ndrop.fit_transform(df2)\n\n\n\n\n\n\n\n\nid\nage\ndepartment\n\n\n\n\n0\n101\n34\nHR\n\n\n1\n102\n28\nFinance\n\n\n2\n103\n45\nIT\n\n\n3\n104\n52\nHR\n\n\n4\n105\n39\nFinance"
  },
  {
    "objectID": "chapters/02_cleaning_data.slides.html#conclusion",
    "href": "chapters/02_cleaning_data.slides.html#conclusion",
    "title": "Preprocessing data with the skrub Cleaner",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we have covered how the skrub Cleaner helps with sanitizing data by implementing a number of common transformations that need to be executed in order to ensure that the data used by the pipeline are consistent and can be used as indended by ML models.\n\nThe Cleaner object automates common data cleaning steps, making it easy to prepare tabular data for analysis or modeling with minimal configuration.\nCleaner leverages transformers like DropUninformative to automatically remove columns that are empty, constant, or contain mostly unique values (such as IDs), based on user-defined thresholds.\nThe DropUninformative transformer can also be used directly for fine-grained control over which columns to drop, according to missing value fraction, constant values, or uniqueness.\nskrub objects are designed to work seamlessly with pandas DataFrames, streamlining the preprocessing workflow and reducing the need for manual data cleaning code.\n\nIn the next chapter we will see how skrub helps with applying this and other transformations to specific columns in the data."
  },
  {
    "objectID": "chapters/00_intro.html",
    "href": "chapters/00_intro.html",
    "title": "Chapter 1: Introduction",
    "section": "",
    "text": "Let’s consider a world where skrub does not exist, and all we can do is use pandas and scikit-learn to prepare data for a machine learning model."
  },
  {
    "objectID": "chapters/00_intro.html#a-world-without-skrub",
    "href": "chapters/00_intro.html#a-world-without-skrub",
    "title": "Chapter 1: Introduction",
    "section": "",
    "text": "Let’s consider a world where skrub does not exist, and all we can do is use pandas and scikit-learn to prepare data for a machine learning model."
  },
  {
    "objectID": "chapters/00_intro.html#load-and-explore-the-data",
    "href": "chapters/00_intro.html#load-and-explore-the-data",
    "title": "Chapter 1: Introduction",
    "section": "Load and explore the data",
    "text": "Load and explore the data\n\nimport pandas as pd\nimport numpy as np\n\nX = pd.read_csv(\"../data/employee_salaries/data.csv\")\ny = pd.read_csv(\"../data/employee_salaries/target.csv\")[\"current_annual_salary\"]\nX.head(5)\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\n0\nF\nPOL\nDepartment of Police\nMSB Information Mgmt and Tech Division Records...\nFulltime-Regular\nOffice Services Coordinator\n09/22/1986\n1986\n\n\n1\nM\nPOL\nDepartment of Police\nISB Major Crimes Division Fugitive Section\nFulltime-Regular\nMaster Police Officer\n09/12/1988\n1988\n\n\n2\nF\nHHS\nDepartment of Health and Human Services\nAdult Protective and Case Management Services\nFulltime-Regular\nSocial Worker IV\n11/19/1989\n1989\n\n\n3\nM\nCOR\nCorrection and Rehabilitation\nPRRS Facility and Security\nFulltime-Regular\nResident Supervisor II\n05/05/2014\n2014\n\n\n4\nM\nHCA\nDepartment of Housing and Community Affairs\nAffordable Housing Programs\nFulltime-Regular\nPlanning Specialist III\n03/05/2007\n2007"
  },
  {
    "objectID": "chapters/00_intro.html#explore-the-target",
    "href": "chapters/00_intro.html#explore-the-target",
    "title": "Chapter 1: Introduction",
    "section": "Explore the target",
    "text": "Explore the target\nLet’s take a look at the target:\n\ny.head(5)\n\n0     69222.18\n1     97392.47\n2    104717.28\n3     52734.57\n4     93396.00\nName: current_annual_salary, dtype: float64\n\n\nThis is a regression task: we want to predict the value of current_annual_salary."
  },
  {
    "objectID": "chapters/00_intro.html#strategizing",
    "href": "chapters/00_intro.html#strategizing",
    "title": "Chapter 1: Introduction",
    "section": "Strategizing",
    "text": "Strategizing\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\n\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000"
  },
  {
    "objectID": "chapters/00_intro.html#our-plan",
    "href": "chapters/00_intro.html#our-plan",
    "title": "Chapter 1: Introduction",
    "section": "Our plan",
    "text": "Our plan\nWe need to:\n\nImpute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.\nScale numerical features.\nEvaluate the performance of the model."
  },
  {
    "objectID": "chapters/00_intro.html#step-1-convert-date-features-to-numerical",
    "href": "chapters/00_intro.html#step-1-convert-date-features-to-numerical",
    "title": "Chapter 1: Introduction",
    "section": "Step 1: Convert date features to numerical",
    "text": "Step 1: Convert date features to numerical\nWe extract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 9)"
  },
  {
    "objectID": "chapters/00_intro.html#step-2-encode-categorical-features",
    "href": "chapters/00_intro.html#step-2-encode-categorical-features",
    "title": "Chapter 1: Introduction",
    "section": "Step 2: Encode categorical features",
    "text": "Step 2: Encode categorical features\nWe encode the categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1218)"
  },
  {
    "objectID": "chapters/00_intro.html#step-3-impute-missing-values",
    "href": "chapters/00_intro.html#step-3-impute-missing-values",
    "title": "Chapter 1: Introduction",
    "section": "Step 3: Impute missing values",
    "text": "Step 3: Impute missing values\nWe impute the missing values in the gender column\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)"
  },
  {
    "objectID": "chapters/00_intro.html#step-4-scale-numerical-features",
    "href": "chapters/00_intro.html#step-4-scale-numerical-features",
    "title": "Chapter 1: Introduction",
    "section": "Step 4: Scale numerical features",
    "text": "Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)"
  },
  {
    "objectID": "chapters/00_intro.html#step-5-train-ridge-model-with-cross-validation",
    "href": "chapters/00_intro.html#step-5-train-ridge-model-with-cross-validation",
    "title": "Chapter 1: Introduction",
    "section": "Step 5: Train Ridge model with cross-validation",
    "text": "Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, scoring=[\"r2\", \"neg_mean_squared_error\"])\n\n# Convert MSE to RMSE\ntest_rmse = np.sqrt(-cv_results[\"test_neg_mean_squared_error\"])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(\n    f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\"\n)\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10367.1206 (+/- 1403.4322)"
  },
  {
    "objectID": "chapters/00_intro.html#just-ask-an-agent-to-write-the-code",
    "href": "chapters/00_intro.html#just-ask-an-agent-to-write-the-code",
    "title": "Chapter 1: Introduction",
    "section": "“Just ask an agent to write the code”",
    "text": "“Just ask an agent to write the code”\n\nOperations in the wrong order.\nTrying to impute categorical features without converting them to numeric values.\nThe datetime feature was treated like a categorical feature.\nCells could not be executed in order without proper debugging and re-prompting.\npd.get_dummies was executed on the full dataframe, rather than only on the training split, leading to data leakage."
  },
  {
    "objectID": "chapters/00_intro.html#waking-up-from-a-nightmare",
    "href": "chapters/00_intro.html#waking-up-from-a-nightmare",
    "title": "Chapter 1: Introduction",
    "section": "Waking up from a nightmare",
    "text": "Waking up from a nightmare\nThankfully, we can import skrub:\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.9089 (+/- 0.0161)\nMean test RMSE: 8773.7865 (+/- 1052.5788)"
  },
  {
    "objectID": "chapters/00_intro.html#roadmap-for-the-course",
    "href": "chapters/00_intro.html#roadmap-for-the-course",
    "title": "Chapter 1: Introduction",
    "section": "Roadmap for the course",
    "text": "Roadmap for the course\n\nData exploration with skrub’s TableReport\nData cleaning and sanitization with the Cleaner\nIntermission: simplifying column operations with skrub\nFeature engineering with the skrub encoders\nPutting everything together: TableVectorizer and tabular_pipeline"
  },
  {
    "objectID": "chapters/00_intro.html#what-we-saw-in-this-chapter",
    "href": "chapters/00_intro.html#what-we-saw-in-this-chapter",
    "title": "Chapter 1: Introduction",
    "section": "What we saw in this chapter",
    "text": "What we saw in this chapter\n\nWe built a predictive pipeline using traditional tools\nWe saw some possible shortcomings\nWe tested skrub’s tabular_pipeline"
  },
  {
    "objectID": "chapters/00_intro.slides.html#a-world-without-skrub",
    "href": "chapters/00_intro.slides.html#a-world-without-skrub",
    "title": "Chapter 1: Introduction",
    "section": "A world without skrub",
    "text": "A world without skrub\nLet’s consider a world where skrub does not exist, and all we can do is use pandas and scikit-learn to prepare data for a machine learning model."
  },
  {
    "objectID": "chapters/00_intro.slides.html#load-and-explore-the-data",
    "href": "chapters/00_intro.slides.html#load-and-explore-the-data",
    "title": "Chapter 1: Introduction",
    "section": "Load and explore the data",
    "text": "Load and explore the data\n\nimport pandas as pd\nimport numpy as np\n\nX = pd.read_csv(\"../data/employee_salaries/data.csv\")\ny = pd.read_csv(\"../data/employee_salaries/target.csv\")[\"current_annual_salary\"]\nX.head(5)\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\n0\nF\nPOL\nDepartment of Police\nMSB Information Mgmt and Tech Division Records...\nFulltime-Regular\nOffice Services Coordinator\n09/22/1986\n1986\n\n\n1\nM\nPOL\nDepartment of Police\nISB Major Crimes Division Fugitive Section\nFulltime-Regular\nMaster Police Officer\n09/12/1988\n1988\n\n\n2\nF\nHHS\nDepartment of Health and Human Services\nAdult Protective and Case Management Services\nFulltime-Regular\nSocial Worker IV\n11/19/1989\n1989\n\n\n3\nM\nCOR\nCorrection and Rehabilitation\nPRRS Facility and Security\nFulltime-Regular\nResident Supervisor II\n05/05/2014\n2014\n\n\n4\nM\nHCA\nDepartment of Housing and Community Affairs\nAffordable Housing Programs\nFulltime-Regular\nPlanning Specialist III\n03/05/2007\n2007"
  },
  {
    "objectID": "chapters/00_intro.slides.html#explore-the-target",
    "href": "chapters/00_intro.slides.html#explore-the-target",
    "title": "Chapter 1: Introduction",
    "section": "Explore the target",
    "text": "Explore the target\nLet’s take a look at the target:\n\ny.head(5)\n\n0     69222.18\n1     97392.47\n2    104717.28\n3     52734.57\n4     93396.00\nName: current_annual_salary, dtype: float64\n\n\nThis is a regression task: we want to predict the value of current_annual_salary."
  },
  {
    "objectID": "chapters/00_intro.slides.html#strategizing",
    "href": "chapters/00_intro.slides.html#strategizing",
    "title": "Chapter 1: Introduction",
    "section": "Strategizing",
    "text": "Strategizing\nWe can begin by exploring the dataframe with .describe, and then think of a plan for pre-processing our data.\n\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000"
  },
  {
    "objectID": "chapters/00_intro.slides.html#our-plan",
    "href": "chapters/00_intro.slides.html#our-plan",
    "title": "Chapter 1: Introduction",
    "section": "Our plan",
    "text": "Our plan\nWe need to:\n\nImpute some missing values in the gender column.\nEncode convert categorical features into numerical features.\nConvert the column date_first_hired into numerical features.\nScale numerical features.\nEvaluate the performance of the model."
  },
  {
    "objectID": "chapters/00_intro.slides.html#step-1-convert-date-features-to-numerical",
    "href": "chapters/00_intro.slides.html#step-1-convert-date-features-to-numerical",
    "title": "Chapter 1: Introduction",
    "section": "Step 1: Convert date features to numerical",
    "text": "Step 1: Convert date features to numerical\nWe extract numerical features from the date_first_hired column.\n\n# Create a copy to work with\nX_processed = X.copy()\n\n# Parse the date column\nX_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n\n# Extract numerical features from date\nX_processed['hired_month'] = X_processed['date_first_hired'].dt.month\nX_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n\n# Drop original date column\nX_processed = X_processed.drop('date_first_hired', axis=1)\n\nprint(\"Features after date transformation:\")\nprint(\"\\nShape:\", X_processed.shape)\n\nFeatures after date transformation:\n\nShape: (9228, 9)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#step-2-encode-categorical-features",
    "href": "chapters/00_intro.slides.html#step-2-encode-categorical-features",
    "title": "Chapter 1: Introduction",
    "section": "Step 2: Encode categorical features",
    "text": "Step 2: Encode categorical features\nWe encode the categorical features using one-hot encoding.\n\n# Identify only the non-numerical (truly categorical) columns\ncategorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns to encode:\", categorical_cols)\n\n# Apply one-hot encoding only to categorical columns\nX_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\nprint(\"\\nShape after encoding:\", X_encoded.shape)\n\nCategorical columns to encode: ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title']\n\nShape after encoding: (9228, 1218)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#step-3-impute-missing-values",
    "href": "chapters/00_intro.slides.html#step-3-impute-missing-values",
    "title": "Chapter 1: Introduction",
    "section": "Step 3: Impute missing values",
    "text": "Step 3: Impute missing values\nWe impute the missing values in the gender column\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nX_encoded_imputed = pd.DataFrame(\n    imputer.fit_transform(X_encoded),\n    columns=X_encoded.columns\n)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#step-4-scale-numerical-features",
    "href": "chapters/00_intro.slides.html#step-4-scale-numerical-features",
    "title": "Chapter 1: Introduction",
    "section": "Step 4: Scale numerical features",
    "text": "Step 4: Scale numerical features\nScale numerical features for the Ridge regression model.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X_encoded_imputed)\nX_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#step-5-train-ridge-model-with-cross-validation",
    "href": "chapters/00_intro.slides.html#step-5-train-ridge-model-with-cross-validation",
    "title": "Chapter 1: Introduction",
    "section": "Step 5: Train Ridge model with cross-validation",
    "text": "Step 5: Train Ridge model with cross-validation\nTrain a Ridge regression model and evaluate with cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport numpy as np\n\n# Initialize Ridge model\nridge = Ridge(alpha=1.0)\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(ridge, X_scaled, y, cv=5, scoring=[\"r2\", \"neg_mean_squared_error\"])\n\n# Convert MSE to RMSE\ntest_rmse = np.sqrt(-cv_results[\"test_neg_mean_squared_error\"])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(\n    f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\"\n)\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.8722 (+/- 0.0274)\nMean test RMSE: 10367.1206 (+/- 1403.4322)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#just-ask-an-agent-to-write-the-code",
    "href": "chapters/00_intro.slides.html#just-ask-an-agent-to-write-the-code",
    "title": "Chapter 1: Introduction",
    "section": "“Just ask an agent to write the code”",
    "text": "“Just ask an agent to write the code”\n\nOperations in the wrong order.\nTrying to impute categorical features without converting them to numeric values.\nThe datetime feature was treated like a categorical feature.\nCells could not be executed in order without proper debugging and re-prompting.\npd.get_dummies was executed on the full dataframe, rather than only on the training split, leading to data leakage."
  },
  {
    "objectID": "chapters/00_intro.slides.html#waking-up-from-a-nightmare",
    "href": "chapters/00_intro.slides.html#waking-up-from-a-nightmare",
    "title": "Chapter 1: Introduction",
    "section": "Waking up from a nightmare",
    "text": "Waking up from a nightmare\nThankfully, we can import skrub:\n\nfrom skrub import tabular_pipeline\n\n# Perform cross-validation (5-fold)\ncv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n                            scoring=['r2', 'neg_mean_squared_error'],\n                            return_train_score=True)\n\n# Convert MSE to RMSE\ntrain_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\ntest_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\nprint(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")\n\nCross-Validation Results:\nMean test R²: 0.9089 (+/- 0.0161)\nMean test RMSE: 8773.7865 (+/- 1052.5788)"
  },
  {
    "objectID": "chapters/00_intro.slides.html#roadmap-for-the-course",
    "href": "chapters/00_intro.slides.html#roadmap-for-the-course",
    "title": "Chapter 1: Introduction",
    "section": "Roadmap for the course",
    "text": "Roadmap for the course\n\nData exploration with skrub’s TableReport\nData cleaning and sanitization with the Cleaner\nIntermission: simplifying column operations with skrub\nFeature engineering with the skrub encoders\nPutting everything together: TableVectorizer and tabular_pipeline"
  },
  {
    "objectID": "chapters/00_intro.slides.html#what-we-saw-in-this-chapter",
    "href": "chapters/00_intro.slides.html#what-we-saw-in-this-chapter",
    "title": "Chapter 1: Introduction",
    "section": "What we saw in this chapter",
    "text": "What we saw in this chapter\n\nWe built a predictive pipeline using traditional tools\nWe saw some possible shortcomings\nWe tested skrub’s tabular_pipeline"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub Tutorial Slides",
    "section": "",
    "text": "Chapter 0: Introduction"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Skrub Tutorial Slides",
    "section": "",
    "text": "Chapter 0: Introduction"
  },
  {
    "objectID": "index.html#exploring-and-sanitizing-data",
    "href": "index.html#exploring-and-sanitizing-data",
    "title": "Skrub Tutorial Slides",
    "section": "Exploring and sanitizing data",
    "text": "Exploring and sanitizing data\n\nChapter 1: Exploring Data\nChapter 2: Cleaning Data\nQuiz 01"
  },
  {
    "objectID": "index.html#transforming-columns",
    "href": "index.html#transforming-columns",
    "title": "Skrub Tutorial Slides",
    "section": "Transforming columns",
    "text": "Transforming columns\n\nChapter 3: Apply Transformations\nChapter 4: Selectors\nQuiz 02"
  },
  {
    "objectID": "index.html#feature-engineering-with-skrub",
    "href": "index.html#feature-engineering-with-skrub",
    "title": "Skrub Tutorial Slides",
    "section": "Feature Engineering with skrub",
    "text": "Feature Engineering with skrub\n\nChapter 5: Numerical Features\nChapter 6: Datetime Features\nChapter 7: Categorical Features\nQuiz 03"
  },
  {
    "objectID": "index.html#bringing-it-all-together",
    "href": "index.html#bringing-it-all-together",
    "title": "Skrub Tutorial Slides",
    "section": "Bringing it all together",
    "text": "Bringing it all together\n\nChapter 8: Table Vectorization\nChapter 9: Tabular Pipelines\nQuiz 04"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Skrub Tutorial Slides",
    "section": "Conclusion",
    "text": "Conclusion\n\nChapter 10: Conclusion"
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#introduction",
    "href": "chapters/01_exploring_data.slides.html#introduction",
    "title": "Exploring dataframes with skrub",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table, and perform some exploratory analysis to learn about the characteristics of the data.\nFirst, let’s import the necessary libraries and load the dataset.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\n# Load the Adult Census dataset\ndata =  pd.read_csv(\"../data/adult_census/data.csv\")\ntarget =  pd.read_csv(\"../data/adult_census/target.csv\")\n\nNow that we have a dataframe we can work with, here is a list of features of the data we would like to find out:\n\nThe size of the dataset.\nThe data types and names of the columns.\nThe distribution of values in the columns.\nWhether null values are present, in what measure and where.\nDiscrete/categorical features, and their cardinality.\nColumns strongly correlated with each other."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#exploring-data-with-pandas-tools",
    "href": "chapters/01_exploring_data.slides.html#exploring-data-with-pandas-tools",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring data with Pandas tools",
    "text": "Exploring data with Pandas tools\nLet’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\n\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n\n\n\n\n\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   age             48842 non-null  int64 \n 1   workclass       46043 non-null  object\n 2   fnlwgt          48842 non-null  int64 \n 3   education       48842 non-null  object\n 4   education-num   48842 non-null  int64 \n 5   marital-status  48842 non-null  object\n 6   occupation      46033 non-null  object\n 7   relationship    48842 non-null  object\n 8   race            48842 non-null  object\n 9   sex             48842 non-null  object\n 10  capital-gain    48842 non-null  int64 \n 11  capital-loss    48842 non-null  int64 \n 12  hours-per-week  48842 non-null  int64 \n 13  native-country  47985 non-null  object\ndtypes: int64(6), object(8)\nmemory usage: 5.2+ MB\n\n\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\n\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\n\n\n\n\n\n\n\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#exploring-data-with-the-tablereport",
    "href": "chapters/01_exploring_data.slides.html#exploring-data-with-the-tablereport",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring data with the TableReport",
    "text": "Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nfrom skrub import TableReport\nTableReport(data, verbose=0)\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Stats” tab\n\nTableReport(data, open_tab=\"stats\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n\n\nFilters\n\n\nPre-made column filters are also available, allowing to select columns by dtype or other characteristics. Filters are shared across tabs.\n\n\n\nThe “Distributions” tab\n\nTableReport(data, open_tab=\"distributions\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Associations” tab\n\nTableReport(data, open_tab=\"associations\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\")."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#exploring-the-target-variable",
    "href": "chapters/01_exploring_data.slides.html#exploring-the-target-variable",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring the target variable",
    "text": "Exploring the target variable\nBesides dataframes, the TableReport handles series and mono- and bi-dimensional numpy arrays.\nSo, let’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\")."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#configuring-and-saving-the-tablereport",
    "href": "chapters/01_exploring_data.slides.html#configuring-and-saving-the-tablereport",
    "title": "Exploring dataframes with skrub",
    "section": "Configuring and saving the TableReport",
    "text": "Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#working-with-big-tables",
    "href": "chapters/01_exploring_data.slides.html#working-with-big-tables",
    "title": "Exploring dataframes with skrub",
    "section": "Working with big tables",
    "text": "Working with big tables\n\nTableReport(\n    data, max_association_columns=3, max_plot_columns=3, open_tab=\"distributions\"\n)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\")."
  },
  {
    "objectID": "chapters/01_exploring_data.slides.html#conclusions",
    "href": "chapters/01_exploring_data.slides.html#conclusions",
    "title": "Exploring dataframes with skrub",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe TableReport is a powerful EDA tool\nIt shows a rich preview of the content of the dataframe\nIt provides precomputed statistics for all the columns\nIt prepares distribution plots for each column\nIt measures the association between columns"
  },
  {
    "objectID": "chapters/01_exploring_data.html",
    "href": "chapters/01_exploring_data.html",
    "title": "Exploring dataframes with skrub",
    "section": "",
    "text": "In this chapter, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table, and perform some exploratory analysis to learn about the characteristics of the data.\nFirst, let’s import the necessary libraries and load the dataset.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\n# Load the Adult Census dataset\ndata =  pd.read_csv(\"../data/adult_census/data.csv\")\ntarget =  pd.read_csv(\"../data/adult_census/target.csv\")\n\nNow that we have a dataframe we can work with, here is a list of features of the data we would like to find out:\n\nThe size of the dataset.\nThe data types and names of the columns.\nThe distribution of values in the columns.\nWhether null values are present, in what measure and where.\nDiscrete/categorical features, and their cardinality.\nColumns strongly correlated with each other."
  },
  {
    "objectID": "chapters/01_exploring_data.html#introduction",
    "href": "chapters/01_exploring_data.html#introduction",
    "title": "Exploring dataframes with skrub",
    "section": "",
    "text": "In this chapter, we will show how we use the skrub TableReport to explore tabular data. We will use the Adult Census dataset as our example table, and perform some exploratory analysis to learn about the characteristics of the data.\nFirst, let’s import the necessary libraries and load the dataset.\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\n# Load the Adult Census dataset\ndata =  pd.read_csv(\"../data/adult_census/data.csv\")\ntarget =  pd.read_csv(\"../data/adult_census/target.csv\")\n\nNow that we have a dataframe we can work with, here is a list of features of the data we would like to find out:\n\nThe size of the dataset.\nThe data types and names of the columns.\nThe distribution of values in the columns.\nWhether null values are present, in what measure and where.\nDiscrete/categorical features, and their cardinality.\nColumns strongly correlated with each other."
  },
  {
    "objectID": "chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "href": "chapters/01_exploring_data.html#exploring-data-with-pandas-tools",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring data with Pandas tools",
    "text": "Exploring data with Pandas tools\nLet’s first explore the data using Pandas only.\nWe can get an idea of the content of the table by printing the first few lines, which gives an idea of the datatypes and the columns we are dealing with.\n\ndata.head(5)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n\n\n\n\n\nIf we want to have a simpler view of the datatypes in the dataframe, we must use data.info():\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48842 entries, 0 to 48841\nData columns (total 14 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   age             48842 non-null  int64 \n 1   workclass       46043 non-null  object\n 2   fnlwgt          48842 non-null  int64 \n 3   education       48842 non-null  object\n 4   education-num   48842 non-null  int64 \n 5   marital-status  48842 non-null  object\n 6   occupation      46033 non-null  object\n 7   relationship    48842 non-null  object\n 8   race            48842 non-null  object\n 9   sex             48842 non-null  object\n 10  capital-gain    48842 non-null  int64 \n 11  capital-loss    48842 non-null  int64 \n 12  hours-per-week  48842 non-null  int64 \n 13  native-country  47985 non-null  object\ndtypes: int64(6), object(8)\nmemory usage: 5.2+ MB\n\n\nWith .info() we can find out the shape of the dataframe (the number of rows and columns), the datatype and the number of non-null values for each column.\nWe can also get a richer summary of the data with the .describe() method:\n\ndata.describe(include=\"all\")\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\ncount\n48842.000000\n46043\n4.884200e+04\n48842\n48842.000000\n48842\n46033\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n47985\n\n\nunique\nNaN\n8\nNaN\n16\nNaN\n7\n14\n6\n5\n2\nNaN\nNaN\nNaN\n41\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\n\n\n\n\n\n\n\nThis gives us useful information about all the features in the dataset. Among others, we can find the number of unique values in each column, various statistics for the numerical columns and the number of null values."
  },
  {
    "objectID": "chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "href": "chapters/01_exploring_data.html#exploring-data-with-the-tablereport",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring data with the TableReport",
    "text": "Exploring data with the TableReport\nNow, let’s create a TableReport to explore the dataset.\n\nfrom skrub import TableReport\nTableReport(data, verbose=0)\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\nDefault view of the TableReport\nThe TableReport gives us a comprehensive overview of the dataset. The default view shows all the columns in the dataset, and allows to select and copy the content of the cells shown in the preview.\nThe TableReport is intended to show a preview of the data, so it does not contain all the rows in the dataset, rather it shows only the first and last few rows by default. Similarly, it stores only the top 10 most frequent values for each column, if column distributions are plotted.\n\n\nThe “Stats” tab\n\nTableReport(data, open_tab=\"stats\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Stats” tab provides a variety of descriptive statistics for each column in the dataset. This includes:\n\nThe column name\nThe detected data type of the column\nWhether the column is sorted or not\nThe number of null values in the column, as well as the percentage\nThe number of unique values in the column\n\nFor numerical columns, additional statistics are provided:\n\nMean\nStandard deviation\nMinimum and maximum values\nMedian\n\nStat columns can also be sorted, for example to quickly identify which columns contain the most nulls, or have the largest cardinality (number of unique values).\n\n\n\n\n\n\nNoneFilters\n\n\n\nPre-made column filters are also available, allowing to select columns by dtype or other characteristics. Filters are shared across tabs.\n\n\n\n\nThe “Distributions” tab\n\nTableReport(data, open_tab=\"distributions\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Distributions” tab provides visualizations of the distributions of values in each column. This includes histograms for numerical columns and bar plots for categorical columns.\nThe “Distributions” tab helps with detecting potential issues in the data, such as:\n\nSkewed distributions\nOutliers\nUnexpected value frequencies\n\nFor example, in this dataset we can see that some columns are heavily skewed, such as “workclass”, “race”, and “native-country”: this is important information to keep track of, because these columns may require special handling during data preprocessing or modeling.\nAdditionally, the “Distributions” tab allows to select columns manually, so that they can be added to a script and selected for further analysis or modeling.\n\n\n\n\n\n\nCautionOutlier detection\n\n\n\nThe TableReport detects outliers using a simple interquartile test, marking as outliers all values that are beyond the IQR. This is a simple heuristic, and should not be treated as perfect. If your problem requires reliable outlier detection, you should not rely exclusively on what the TableReport shows.\n\n\n\n\nThe “Associations” tab\n\nTableReport(data, open_tab=\"associations\")\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe “Associations” tab provides insights into the relationships between different columns in the dataset. It shows Pearson’s correlation coefficient for numerical columns, as well as Cramér’s V for all columns.\nWhile this is a somewhat rough measure of association, it can help identify potential relationships worth exploring further during the analysis, and highlights highly correlated columns: depending on the modeling technique used, these may need to be handled specially to avoid issues with multicollinearity.\nIn this example, we can see that “education-num” and “education” have perfect correlation, which means that one of the two columns can be dropped without losing information."
  },
  {
    "objectID": "chapters/01_exploring_data.html#exploring-the-target-variable",
    "href": "chapters/01_exploring_data.html#exploring-the-target-variable",
    "title": "Exploring dataframes with skrub",
    "section": "Exploring the target variable",
    "text": "Exploring the target variable\nBesides dataframes, the TableReport handles series and mono- and bi-dimensional numpy arrays.\nSo, let’s take a closer look at the target variable, which indicates whether an individual’s income exceeds $50K per year. We can create a separate TableReport for the target variable to explore its distribution:\n\nTableReport(target)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\")."
  },
  {
    "objectID": "chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "href": "chapters/01_exploring_data.html#configuring-and-saving-the-tablereport",
    "title": "Exploring dataframes with skrub",
    "section": "Configuring and saving the TableReport",
    "text": "Configuring and saving the TableReport\nThe TableReport can be saved on disk as an HTML.\nTableReport(data).write_html(\"report.html\")\nThen, the report can be opened using any internet browser, with no need to run a Jupyter notebok or a python interactive console.\nIt is possible to configure various parameters using the skrub global config. For example, it is possible to replace the default Pandas or Polars dataframe display with the TableReport by using patch_display (and unpatch_display):\n\nfrom skrub import patch_display, unpatch_display\n\n# replace the default pandas repr \npatch_display()\ndata\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nTo disable, use unpatch_display:\n\nunpatch_display()\ndata\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n\n\n\n\n48842 rows × 14 columns\n\n\n\nMore detail on the skrub configuration is reported in the User Guide."
  },
  {
    "objectID": "chapters/01_exploring_data.html#working-with-big-tables",
    "href": "chapters/01_exploring_data.html#working-with-big-tables",
    "title": "Exploring dataframes with skrub",
    "section": "Working with big tables",
    "text": "Working with big tables\nPlotting and measuring the column correlations are expensive operations and may take a long time, so when the dataframe under study is large it may be more convenient to skip them for quicker development.\nThe max_plot_columns and max_association_columns parameters allow to set a threshold on the number of columns: the TableReport will skip the respective task if the number of colums in the dataframe is larger than the threshold:\nWhen the number of columns is too large, an information message is shown in the respective tab instead of the plots or correlations.\n\nTableReport(\n    data, max_association_columns=3, max_plot_columns=3, open_tab=\"distributions\"\n)\n\nProcessing column   1 / 14Processing column   2 / 14Processing column   3 / 14Processing column   4 / 14Processing column   5 / 14Processing column   6 / 14Processing column   7 / 14Processing column   8 / 14Processing column   9 / 14Processing column  10 / 14Processing column  11 / 14Processing column  12 / 14Processing column  13 / 14Processing column  14 / 14\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\")."
  },
  {
    "objectID": "chapters/01_exploring_data.html#conclusions",
    "href": "chapters/01_exploring_data.html#conclusions",
    "title": "Exploring dataframes with skrub",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter we have learned how the TableReport can be used to speed up data exploration, allowing us to find possible criticalities in the data.\nWe covered:\n\nCreating and configuring a TableReport for fast, interactive data exploration\nExploring column statistics, value distributions, and associations visually\nDetecting nulls, outliers, and highly correlated columns at a glance\nFiltering columns by type or characteristics using built-in filters\nSaving and sharing interactive reports as standalone HTML files\nAdjusting TableReport settings for large datasets to optimize performance\n\nIn the next chapter, we will find out how to address some of the possible problems using the skrub Cleaner."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.slides.html#introduction",
    "href": "chapters/03_feat_eng_apply.slides.html#introduction",
    "title": "Applying transformers to columns",
    "section": "Introduction",
    "text": "Introduction\nOften, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception. In other cases, specific columns may need particular treatment, and should therefore be ignored by the Cleaner.\nScikit-learn provides the ColumnTransformer to deal with this:\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\n\n\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.slides.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "chapters/03_feat_eng_apply.slides.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "Applying transformers to columns",
    "section": "Selection operations in a scikit-learn pipeline",
    "text": "Selection operations in a scikit-learn pipeline\nSelectCols and DropCols allow selecting or removing specific columns in a dataframe according to user-provided rules. For example, to remove columns that include null values, or to select only columns that have a specific dtype.\nSelectCols and DropCols take a cols parameter to choose which columns to select or drop respectively.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nWe can selectively choose or drop columns based on names, or more complex rules (see the next chapter).\n\nfrom skrub import SelectCols\nSelectCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\ndate\n\n\n\n\n0\n03 January 2023\n\n\n1\n04 February 2023\n\n\n2\n05 March 2023\n\n\n\n\n\n\n\n\nfrom skrub import DropCols\nDropCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\nvalues\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30"
  },
  {
    "objectID": "chapters/03_feat_eng_apply.slides.html#applytocols-and-applytoframe",
    "href": "chapters/03_feat_eng_apply.slides.html#applytocols-and-applytoframe",
    "title": "Applying transformers to columns",
    "section": "ApplyToCols and ApplyToFrame",
    "text": "ApplyToCols and ApplyToFrame\nBesides selecting and dropping columns, pre-processing pipelines are intended to transform specific columns in specific ways. To make this process easier, skrub provides the ApplyToCols and ApplyToFrame transformers.\nApplying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ndate_03 January 2023\ndate_04 February 2023\ndate_05 March 2023\nvalues\n\n\n\n\n0\n1.0\n0.0\n0.0\n-1.224745\n\n\n1\n0.0\n1.0\n0.0\n0.000000\n\n\n2\n0.0\n0.0\n1.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n\n\n\n\nImportant\n\n\nApplyToCols is intended to work on dataframes, which are dense. As a result, transformers that produce sparse outputs (like the OneHotEncoder) must be set so that their output is dense.\n\n\n\nApplying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, there may be a need to apply the same transformer only to a subset of columns in a dataframe.\nConsider this example dataframe, which some patient information, and some metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541\n\n\n\n\n\n\n\nThe allow_reject parameter\nWhen ApplyToCols or ApplyToFrame are using a skrub transformer, they can use the allow_reject parameter for more flexibility. By setting allow_reject to True, columns that cannot be treated by the current transformer will be ignored rather than raising an exception.\nConsider this example. By default, ToDatetime raises a RejectColumn exception when it finds a column it cannot convert to datetime.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nBy setting allow_reject=True, the datetime column is converted properly and the other column is passed through without issues.\n\nwith_reject = ApplyToCols(ToDatetime(), allow_reject=True)\nwith_reject.fit_transform(df)\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n2023-01-03\n10\n\n\n1\n2023-02-04\n20\n\n\n2\n2023-03-05\n30"
  },
  {
    "objectID": "chapters/03_feat_eng_apply.slides.html#concatenating-the-skrub-column-transformers",
    "href": "chapters/03_feat_eng_apply.slides.html#concatenating-the-skrub-column-transformers",
    "title": "Applying transformers to columns",
    "section": "Concatenating the skrub column transformers",
    "text": "Concatenating the skrub column transformers\nSkrub column transformers can be concatenated by using scikit-learn pipelines. In the following example, we first select only the column patient_id, then encode it using OneHotEncoder and finally use PCA to reduce the number of dimensions.\nThis is done by wrapping the latter two steps in ApplyToCols and ApplyToFrame respectively, and then putting all transformers in order in a scikit-learn pipeline using make_pipeline.\n\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols\n\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nselect = SelectCols(\"patient_id\")\nencode = ApplyToCols(OneHotEncoder(sparse_output=False))\nreduce = ApplyToFrame(PCA(n_components=2))\n\ntransform = make_pipeline(select, encode, reduce)\ndft= transform.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npca0\npca1\n\n\n\n\n0\n1.451188e-17\n9.393890e-18\n\n\n1\n-2.405452e-02\n9.397337e-01\n\n\n2\n-2.305851e-01\n9.374222e-03\n\n\n3\n-5.287468e-02\n9.374222e-03\n\n\n4\n-7.954573e-02\n-6.807817e-03\n\n\n\n\n\n\n\nThe order of column transformations is important\nSome care must be taken when concatenating columnn transformers, in particular when selection is done on datatypes. Consider this case:\n\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\nIn the first case, we encode and then scale, in the second case we instead scale first and then encode.\n\ntransform_1 = make_pipeline(encode, scale)\ndft = transform_1.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.301570\n0.904534\n-0.904534\n\n\n1\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.709947\n0.904534\n-0.904534\n\n\n2\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n0.059162\n-1.105542\n1.105542\n\n\n3\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.479057\n-1.105542\n1.105542\n\n\n4\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.473298\n0.904534\n-0.904534\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ntransform_2 = make_pipeline(scale, encode)\ndft = transform_2.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.301570\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.709947\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.059162\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.479057\n0.0\n1.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.473298\n1.0\n0.0\n\n\n\n\n5 rows × 23 columns\n\n\n\nThe result of transform_1 is that the features that have been generated by the OneHotEncoder are then scaled by the StandardScaler, because the new features are numeric and are therefore selected in the next step.\nIn many cases, this behavior is not desired: while some model types may not be affected by the different ordering (such as tree-based models), linear models and NN-based models may produce worse results."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.slides.html#conclusions",
    "href": "chapters/03_feat_eng_apply.slides.html#conclusions",
    "title": "Applying transformers to columns",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter we explored how skrub helps with selecting and transforming specific columns using various transformers. While these transformers can take simple lists of columns to work, they become far more flexible and powerful when they are combined with the skrub selectors, which is the subject of the next chapter."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html",
    "href": "chapters/03_feat_eng_apply.html",
    "title": "Applying transformers to columns",
    "section": "",
    "text": "Often, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception. In other cases, specific columns may need particular treatment, and should therefore be ignored by the Cleaner.\nScikit-learn provides the ColumnTransformer to deal with this:\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\n\n\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#introduction",
    "href": "chapters/03_feat_eng_apply.html#introduction",
    "title": "Applying transformers to columns",
    "section": "",
    "text": "Often, transformers need to be applied only to a subset of columns, rather than the entire dataframe.\nAs an example, it does not make sense to apply a StandardScaler to a column that contains strings, and indeed doing so would raise an exception. In other cases, specific columns may need particular treatment, and should therefore be ignored by the Cleaner.\nScikit-learn provides the ColumnTransformer to deal with this:\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])\n\n\nmake_column_selector allows to choose columns based on their datatype, or by using regex to filter column names. In some cases, this degree of control is not sufficient.\nTo address such situations, skrub implements different transformers that allow to modify columns from within scikit-learn pipelines. Additionally, the selectors API allows to implement powerful, custom-made column selection filters.\nSelectCols and DropCols are transformers that can be used as part of a pipeline to filter columns according to the selectors API, while ApplyToCols and ApplyToFrame replicate the ColumnTransformer behavior with a different syntax and access to the selectors."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "href": "chapters/03_feat_eng_apply.html#selection-operations-in-a-scikit-learn-pipeline",
    "title": "Applying transformers to columns",
    "section": "Selection operations in a scikit-learn pipeline",
    "text": "Selection operations in a scikit-learn pipeline\nSelectCols and DropCols allow selecting or removing specific columns in a dataframe according to user-provided rules. For example, to remove columns that include null values, or to select only columns that have a specific dtype.\nSelectCols and DropCols take a cols parameter to choose which columns to select or drop respectively.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nWe can selectively choose or drop columns based on names, or more complex rules (see the next chapter).\n\nfrom skrub import SelectCols\nSelectCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\ndate\n\n\n\n\n0\n03 January 2023\n\n\n1\n04 February 2023\n\n\n2\n05 March 2023\n\n\n\n\n\n\n\n\nfrom skrub import DropCols\nDropCols(\"date\").fit_transform(df)\n\n\n\n\n\n\n\n\nvalues\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30"
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "href": "chapters/03_feat_eng_apply.html#applytocols-and-applytoframe",
    "title": "Applying transformers to columns",
    "section": "ApplyToCols and ApplyToFrame",
    "text": "ApplyToCols and ApplyToFrame\nBesides selecting and dropping columns, pre-processing pipelines are intended to transform specific columns in specific ways. To make this process easier, skrub provides the ApplyToCols and ApplyToFrame transformers.\n\nApplying a transformer to separate columns: ApplyToCols\nIn many cases, ApplyToCols can be a direct replacememnt for the ColumnTransformer, like in the following example:\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ndate_03 January 2023\ndate_04 February 2023\ndate_05 March 2023\nvalues\n\n\n\n\n0\n1.0\n0.0\n0.0\n-1.224745\n\n\n1\n0.0\n1.0\n0.0\n0.000000\n\n\n2\n0.0\n0.0\n1.0\n1.224745\n\n\n\n\n\n\n\nIn this case, we are applying the StandardScaler only to numeric features using s.numeric(), and OneHotEncoder with s.string().\nUnder the hood, ApplyToCol selects all columns that satisfy the condition specified in cols (in this case, that the dtype is numeric), then clones and applies the specified transformer (StandardScaler) to each column separately.\n\n\n\n\n\n\nImportant\n\n\n\nColumns that are not selected are passed through without any change, thus string columns are not touched by the numeric transformer.\n\n\nBy passing through unselected columns without changes it is possible to chain several ApplyToCols together by putting them in a scikit-learn pipeline.\n\n\n\n\n\n\nImportant\n\n\n\nApplyToCols is intended to work on dataframes, which are dense. As a result, transformers that produce sparse outputs (like the OneHotEncoder) must be set so that their output is dense.\n\n\n\n\nApplying the same transformer to multiple columns at once: ApplyToFrame\nIn some cases, there may be a need to apply the same transformer only to a subset of columns in a dataframe.\nConsider this example dataframe, which some patient information, and some metrics.\n\nimport pandas as pd\nimport numpy as np\n\nn_patients = 20\nnp.random.seed(42)\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nfor i in range(5):\n    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n\ndf[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\ndf.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\nmetric_0\nmetric_1\nmetric_2\nmetric_3\nmetric_4\ndiagnosis\n\n\n\n\n0\nP000\n56\nF\n39.871689\n52.088636\n41.607825\n50.870471\n52.961203\nB\n\n\n1\nP001\n69\nM\n53.142473\n30.403299\n46.907876\n47.009926\n52.610553\nA\n\n\n2\nP002\n46\nF\n40.919759\n36.718140\n53.312634\n50.917608\n50.051135\nB\n\n\n3\nP003\n32\nF\n35.876963\n51.968612\n59.755451\n30.124311\n47.654129\nB\n\n\n4\nP004\n60\nF\n64.656488\n57.384666\n45.208258\n47.803281\n35.846293\nC\n\n\n\n\n\n\n\nWith ApplyToFrame, it is easy to apply a decomposition algorithm such as PCA to condense the metric_* columns into a smaller number of features:\n\nfrom skrub import ApplyToFrame\nfrom sklearn.decomposition import PCA\n\nreduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n\ndf_reduced = reduce.fit_transform(df)\ndf_reduced.head()\n\n\n\n\n\n\n\n\npatient_id\nage\nsex\ndiagnosis\npca0\npca1\n\n\n\n\n0\nP000\n56\nF\nB\n-2.647377\n7.025046\n\n\n1\nP001\n69\nM\nA\n-2.480564\n-11.246997\n\n\n2\nP002\n46\nF\nB\n4.274840\n-5.039065\n\n\n3\nP003\n32\nF\nB\n14.116747\n15.620615\n\n\n4\nP004\n60\nF\nC\n-19.073862\n1.186541\n\n\n\n\n\n\n\n\n\nThe allow_reject parameter\nWhen ApplyToCols or ApplyToFrame are using a skrub transformer, they can use the allow_reject parameter for more flexibility. By setting allow_reject to True, columns that cannot be treated by the current transformer will be ignored rather than raising an exception.\nConsider this example. By default, ToDatetime raises a RejectColumn exception when it finds a column it cannot convert to datetime.\n\nfrom skrub import ToDatetime\ndf = pd.DataFrame({\n    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n    \"values\": [10, 20, 30]\n})\ndf\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n03 January 2023\n10\n\n\n1\n04 February 2023\n20\n\n\n2\n05 March 2023\n30\n\n\n\n\n\n\n\nBy setting allow_reject=True, the datetime column is converted properly and the other column is passed through without issues.\n\nwith_reject = ApplyToCols(ToDatetime(), allow_reject=True)\nwith_reject.fit_transform(df)\n\n\n\n\n\n\n\n\ndate\nvalues\n\n\n\n\n0\n2023-01-03\n10\n\n\n1\n2023-02-04\n20\n\n\n2\n2023-03-05\n30"
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#concatenating-the-skrub-column-transformers",
    "href": "chapters/03_feat_eng_apply.html#concatenating-the-skrub-column-transformers",
    "title": "Applying transformers to columns",
    "section": "Concatenating the skrub column transformers",
    "text": "Concatenating the skrub column transformers\nSkrub column transformers can be concatenated by using scikit-learn pipelines. In the following example, we first select only the column patient_id, then encode it using OneHotEncoder and finally use PCA to reduce the number of dimensions.\nThis is done by wrapping the latter two steps in ApplyToCols and ApplyToFrame respectively, and then putting all transformers in order in a scikit-learn pipeline using make_pipeline.\n\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import SelectCols\n\ndf = pd.DataFrame({\n    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n    \"age\": np.random.randint(18, 80, size=n_patients),\n    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n})\n\nselect = SelectCols(\"patient_id\")\nencode = ApplyToCols(OneHotEncoder(sparse_output=False))\nreduce = ApplyToFrame(PCA(n_components=2))\n\ntransform = make_pipeline(select, encode, reduce)\ndft= transform.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npca0\npca1\n\n\n\n\n0\n1.451188e-17\n9.393890e-18\n\n\n1\n-2.405452e-02\n9.397337e-01\n\n\n2\n-2.305851e-01\n9.374222e-03\n\n\n3\n-5.287468e-02\n9.374222e-03\n\n\n4\n-7.954573e-02\n-6.807817e-03\n\n\n\n\n\n\n\n\nThe order of column transformations is important\nSome care must be taken when concatenating columnn transformers, in particular when selection is done on datatypes. Consider this case:\n\nencode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\nscale = ApplyToCols(StandardScaler(), cols=s.numeric())\n\nIn the first case, we encode and then scale, in the second case we instead scale first and then encode.\n\ntransform_1 = make_pipeline(encode, scale)\ndft = transform_1.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.301570\n0.904534\n-0.904534\n\n\n1\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.709947\n0.904534\n-0.904534\n\n\n2\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n0.059162\n-1.105542\n1.105542\n\n\n3\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-1.479057\n-1.105542\n1.105542\n\n\n4\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n4.358899\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n...\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.229416\n-0.473298\n0.904534\n-0.904534\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ntransform_2 = make_pipeline(scale, encode)\ndft = transform_2.fit_transform(df)\ndft.head(5)\n\n\n\n\n\n\n\n\npatient_id_P000\npatient_id_P001\npatient_id_P002\npatient_id_P003\npatient_id_P004\npatient_id_P005\npatient_id_P006\npatient_id_P007\npatient_id_P008\npatient_id_P009\n...\npatient_id_P013\npatient_id_P014\npatient_id_P015\npatient_id_P016\npatient_id_P017\npatient_id_P018\npatient_id_P019\nage\nsex_F\nsex_M\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.301570\n1.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.709947\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.059162\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.479057\n0.0\n1.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.473298\n1.0\n0.0\n\n\n\n\n5 rows × 23 columns\n\n\n\nThe result of transform_1 is that the features that have been generated by the OneHotEncoder are then scaled by the StandardScaler, because the new features are numeric and are therefore selected in the next step.\nIn many cases, this behavior is not desired: while some model types may not be affected by the different ordering (such as tree-based models), linear models and NN-based models may produce worse results."
  },
  {
    "objectID": "chapters/03_feat_eng_apply.html#conclusions",
    "href": "chapters/03_feat_eng_apply.html#conclusions",
    "title": "Applying transformers to columns",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter we explored how skrub helps with selecting and transforming specific columns using various transformers. While these transformers can take simple lists of columns to work, they become far more flexible and powerful when they are combined with the skrub selectors, which is the subject of the next chapter."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.slides.html#introduction",
    "href": "chapters/05_feat_eng_numerical.slides.html#introduction",
    "title": "Scaling numerical features safely",
    "section": "Introduction",
    "text": "Introduction\nNow that we can transform any column we want thanks to ApplyToCols, ApplyToFrame and the selectors, we can start covering the feature engineering part of our pipeline, beginning from numerical features.\nSpecifically, we will find out how to safely scale numerical features with the skrub SquashingScaler."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.slides.html#numerical-features-with-outliers",
    "href": "chapters/05_feat_eng_numerical.slides.html#numerical-features-with-outliers",
    "title": "Scaling numerical features safely",
    "section": "Numerical features with outliers",
    "text": "Numerical features with outliers\nWhen dealing with numerical features that contain outliers (including infinite values), standard scaling methods can be problematic. Outliers can dramatically affect the centering and scaling of the entire dataset, causing the scaled inliers to be compressed into a narrow range.\nConsider this example:\n\nfrom helpers import (\n    generate_data_with_outliers,\n    plot_feature_with_outliers\n)\n\nvalues = generate_data_with_outliers()\n\nplot_feature_with_outliers(values)\n\n\n\n\n\n\n\n\nIn this case, most of the values are in the range [-2, 2], but there are some large outliers in the range [-40, 40] that can cause issues when the feature needs to be scaled.\nRegular scalers and their limitations\nThe StandardScaler computes mean and standard deviation across all values. With outliers present, these statistics become unreliable, and the scaling factor can become too small, squashing inlier values.\nThe RobustScaler uses quantiles (typically the 25th and 75th percentiles) instead of mean/std, which makes it more resistant to outliers. However, it doesn’t bound the output values, so extreme outliers can still have very large scaled values."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.slides.html#squashingscaler-a-robust-solution",
    "href": "chapters/05_feat_eng_numerical.slides.html#squashingscaler-a-robust-solution",
    "title": "Scaling numerical features safely",
    "section": "SquashingScaler: A robust solution",
    "text": "SquashingScaler: A robust solution\nThe SquashingScaler combines robust centering with smooth clipping to handle outliers effectively.\nIt works as following:\n\nIt centers the median to 0, then it scales values using quantile-based statistics.\nIt fills constant columns with 0s.\nIt applies a smooth squashing function: \\(x_{\\text{out}} = \\frac{z}{\\sqrt{1 + (z/B)^2}}\\)\nIt constrains all values to the range \\([-\\texttt{max\\_absolute\\_value}, \\texttt{max\\_absolute\\_value}]\\) (default: 3)\nInfinite values are mapped to the corresponding boundaries.\nNaN values are kept unchanged.\n\nAdvantages and disadvantages of SquashingScaler\nThe SquashingScaler has various advantages over traditional scalers:\n\nIt is outlier-resistant: Outliers don’t affect inlier scaling, unlike the StandardScaler.\nIt has bounded output: All values stay in a predictable range, ideal for neural networks and linear models.\nIt handles edge cases: The scaler works with infinite values and constant columns.\nIt preserves missing data: NaN values are kept unchanged.\n\nA disadvantage of the SquashingScaler is that it is non-invertible: The soft clipping function is smooth but cannot be exactly inverted."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.slides.html#conclusion",
    "href": "chapters/05_feat_eng_numerical.slides.html#conclusion",
    "title": "Scaling numerical features safely",
    "section": "Conclusion",
    "text": "Conclusion\nWhen compared on data with outliers:\n\nStandardScaler compresses inliers due to large scaling factors\nRobustScaler preserves relative scales but allows extreme outlier values\nSquashingScaler keeps inliers in a reasonable range while smoothly bounding all values\n\nIf we plot the impact of each scaler on the result, this is what we can see:\n\nfrom helpers import scale_feature_and_plot\nscale_feature_and_plot(values)"
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html",
    "href": "chapters/05_feat_eng_numerical.html",
    "title": "Scaling numerical features safely",
    "section": "",
    "text": "Now that we can transform any column we want thanks to ApplyToCols, ApplyToFrame and the selectors, we can start covering the feature engineering part of our pipeline, beginning from numerical features.\nSpecifically, we will find out how to safely scale numerical features with the skrub SquashingScaler."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html#introduction",
    "href": "chapters/05_feat_eng_numerical.html#introduction",
    "title": "Scaling numerical features safely",
    "section": "",
    "text": "Now that we can transform any column we want thanks to ApplyToCols, ApplyToFrame and the selectors, we can start covering the feature engineering part of our pipeline, beginning from numerical features.\nSpecifically, we will find out how to safely scale numerical features with the skrub SquashingScaler."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "href": "chapters/05_feat_eng_numerical.html#numerical-features-with-outliers",
    "title": "Scaling numerical features safely",
    "section": "Numerical features with outliers",
    "text": "Numerical features with outliers\nWhen dealing with numerical features that contain outliers (including infinite values), standard scaling methods can be problematic. Outliers can dramatically affect the centering and scaling of the entire dataset, causing the scaled inliers to be compressed into a narrow range.\nConsider this example:\n\nfrom helpers import (\n    generate_data_with_outliers,\n    plot_feature_with_outliers\n)\n\nvalues = generate_data_with_outliers()\n\nplot_feature_with_outliers(values)\n\n\n\n\n\n\n\n\nIn this case, most of the values are in the range [-2, 2], but there are some large outliers in the range [-40, 40] that can cause issues when the feature needs to be scaled.\n\nRegular scalers and their limitations\nThe StandardScaler computes mean and standard deviation across all values. With outliers present, these statistics become unreliable, and the scaling factor can become too small, squashing inlier values.\nThe RobustScaler uses quantiles (typically the 25th and 75th percentiles) instead of mean/std, which makes it more resistant to outliers. However, it doesn’t bound the output values, so extreme outliers can still have very large scaled values."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html#squashingscaler-a-robust-solution",
    "href": "chapters/05_feat_eng_numerical.html#squashingscaler-a-robust-solution",
    "title": "Scaling numerical features safely",
    "section": "SquashingScaler: A robust solution",
    "text": "SquashingScaler: A robust solution\nThe SquashingScaler combines robust centering with smooth clipping to handle outliers effectively.\nIt works as following:\n\nIt centers the median to 0, then it scales values using quantile-based statistics.\nIt fills constant columns with 0s.\nIt applies a smooth squashing function: \\(x_{\\text{out}} = \\frac{z}{\\sqrt{1 + (z/B)^2}}\\)\nIt constrains all values to the range \\([-\\texttt{max\\_absolute\\_value}, \\texttt{max\\_absolute\\_value}]\\) (default: 3)\nInfinite values are mapped to the corresponding boundaries.\nNaN values are kept unchanged.\n\n\nAdvantages and disadvantages of SquashingScaler\nThe SquashingScaler has various advantages over traditional scalers:\n\nIt is outlier-resistant: Outliers don’t affect inlier scaling, unlike the StandardScaler.\nIt has bounded output: All values stay in a predictable range, ideal for neural networks and linear models.\nIt handles edge cases: The scaler works with infinite values and constant columns.\nIt preserves missing data: NaN values are kept unchanged.\n\nA disadvantage of the SquashingScaler is that it is non-invertible: The soft clipping function is smooth but cannot be exactly inverted."
  },
  {
    "objectID": "chapters/05_feat_eng_numerical.html#conclusion",
    "href": "chapters/05_feat_eng_numerical.html#conclusion",
    "title": "Scaling numerical features safely",
    "section": "Conclusion",
    "text": "Conclusion\nWhen compared on data with outliers:\n\nStandardScaler compresses inliers due to large scaling factors\nRobustScaler preserves relative scales but allows extreme outlier values\nSquashingScaler keeps inliers in a reasonable range while smoothly bounding all values\n\nIf we plot the impact of each scaler on the result, this is what we can see:\n\nfrom helpers import scale_feature_and_plot\nscale_feature_and_plot(values)"
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.slides.html#introduction",
    "href": "chapters/07_feat_eng_categorical.slides.html#introduction",
    "title": "Mixed data: dealing with categories",
    "section": "Introduction",
    "text": "Introduction\nReal-world datasets rarely contain only numeric values. We frequently encounter categorical features—values that belong to discrete categories, such as names, occupations, geographic locations, or clothing sizes. Text data also falls into this category, since each unique string can be considered a categorical value.\nThe challenge is that machine learning models require numeric input. How do we convert these categorical values into numeric features that preserve their information and enable our models to make good predictions?\nThis chapter explores the various strategies and tools available in skrub to encode categorical features, with an eye on choosing the best method for our specific use case."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.slides.html#why-categorical-encoders-matter",
    "href": "chapters/07_feat_eng_categorical.slides.html#why-categorical-encoders-matter",
    "title": "Mixed data: dealing with categories",
    "section": "Why categorical encoders matter",
    "text": "Why categorical encoders matter\nThe way we encode categorical features significantly impacts our machine learning pipeline:\n\nPerformance: The encoding choice directly affects how well our model learns from categorical information\nEfficiency: Some encodings create many features (potentially thousands), which increases computation time and memory usage\nInterpretability: Different encoders provide varying levels of transparency in what features represent\nScalability: Not all methods scale well to high-cardinality features (those with many unique values)\n\nUsing the appropriate encoder ensures we’re making the best use of categorical information while keeping our model efficient and interpretable."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.slides.html#categorical-encoders-pros-and-cons",
    "href": "chapters/07_feat_eng_categorical.slides.html#categorical-encoders-pros-and-cons",
    "title": "Mixed data: dealing with categories",
    "section": "Categorical encoders: pros and cons",
    "text": "Categorical encoders: pros and cons\nOne-Hot Encoding and Ordinal Encoding (scikit-learn)\nOneHotEncoder: Creates a binary indicator column for each unique category, where 1 denotes the presence of the category and 0 its absence.\nPros:\n\nStraightforward and intuitive\nWorks well for low-cardinality features (few unique values)\nProduces sparse matrices that can save memory\n\nCons:\n\nBecomes impractical with high-cardinality features (creates hundreds or thousands of columns)\nResults in mostly zero-valued sparse matrices when dense, which is the situation when working with dataframes\nIncreases overfitting risk and computational overhead\n\nThe OneHotEncoder is used by default by the skrub TableVectorizer for categorical features with fewer than 40 unique values.\nOrdinalEncoder: Assigns each category a numerical value (0, 1, 2, …).\nPros:\n\nVery memory-efficient\nCreates only one output column per input column\nFast to compute\n\nCons:\n\nIntroduces artificial ordering among categories that may not exist in reality\nCan mislead models into thinking some categories are “greater than” others\n\nCategorical encoders in skrub\nAll the categorical encoders in skrub are designed to encode any number of unique values using a fixed number of components: this number is controlled by the parameter n_components in each transformer.\nStringEncoder\nApproach: Applies term frequency-inverse document frequency (tf-idf) vectorization to character n-grams, followed by truncated singular value decomposition (SVD) for dimensionality reduction. This method is also known as Latent Semantic Analysis.\nPros:\n\nThe best all-rounder: Performs well on both categorical and text data\nFast training time\nRobust and generalizes well across different datasets\nNo artificial ordering introduced\n\nCons:\n\nLess interpretable than one-hot encoding or ordinal encoding\nMay not capture semantic relationships as well as language model-based approaches\nPerformance depends on the nature of the categorical data\n\nTextEncoder\nApproach: Uses pretrained language models from HuggingFace Hub to generate dense vector representations of text.\nPros:\n\nExceptional performance on free-flowing text and natural language\nCaptures semantic meaning and context\nLeverages knowledge from large-scale language model pretraining\nCan excel on datasets where domain-specific information aligns with pretraining data\n\nCons:\n\nVery computationally expensive: Significantly slower than other methods\nRequires heavy dependencies (PyTorch, transformers)\nModels are large and require downloading\nImpractical for CPU-only environments\nPerformance on traditional categorical data (non-text, such as IDs) is not much better than simpler methods\n\nMinHashEncoder\nApproach: Decomposes strings into n-grams and applies the MinHash algorithm for quick dimension reduction.\nPros:\n\nVery fast training time\nSimple and lightweight\nMinimal memory overhead\nGood for quick prototyping or very large-scale datasets\n\nCons:\n\nPerformance generally lags behind StringEncoder and TextEncoder\nLess nuanced feature representation\nLess robust across different types of data\n\nGapEncoder\nApproach: Estimates latent categories by finding common n-gram patterns across values, then encodes these patterns as numeric features.\nPros:\n\nInterpretable: Column names reflect the estimated categories\nCan group similar strings intelligently\nReasonable performance across datasets\n\nCons:\n\nSlower training time compared to StringEncoder and MinHashEncoder\nPerformance is on par with or slightly worse than the faster StringEncoder\nInterpretability comes at the cost of training speed\nMay require more computational resources for large datasets"
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.slides.html#conclusion",
    "href": "chapters/07_feat_eng_categorical.slides.html#conclusion",
    "title": "Mixed data: dealing with categories",
    "section": "Conclusion",
    "text": "Conclusion\nEncoding categorical features is a critical step in preparing data for machine learning. The skrub library provides multiple encoders to handle different scenarios:\n\nStart with StringEncoder as a default for high-cardinality categorical features. It offers the best balance of speed, performance, and robustness across diverse datasets.\nUse OneHotEncoder for low-cardinality features (&lt; 40 unique values) to keep the feature space manageable.\nChoose TextEncoder if you’re working with true textual data (reviews, comments, descriptions) and have sufficient computational resources.\nConsider GapEncoder when interpretability is important and the additional training time can be dealt with.\nUse MinHashEncoder when you need maximum speed and are working with very large datasets.\n\nThe TableVectorizer integrates these encoders automatically, dispatching columns to the appropriate encoder based on their data type and cardinality. This automation makes it easy to process mixed-type datasets efficiently while still allowing fine-grained control when needed. By default, the TableVectorizer uses the OneHotEncoder for categorical features with cardinality &lt;= 40, and StringEncoder for categorical features with cardinality &gt; 40.\nFor a comprehensive empirical comparison of these methods, refer to the categorical encoders benchmark.\nIn the next chapter we will put all the encoders explained so far in a single object, the TableVectorizer."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html",
    "href": "chapters/07_feat_eng_categorical.html",
    "title": "Mixed data: dealing with categories",
    "section": "",
    "text": "Real-world datasets rarely contain only numeric values. We frequently encounter categorical features—values that belong to discrete categories, such as names, occupations, geographic locations, or clothing sizes. Text data also falls into this category, since each unique string can be considered a categorical value.\nThe challenge is that machine learning models require numeric input. How do we convert these categorical values into numeric features that preserve their information and enable our models to make good predictions?\nThis chapter explores the various strategies and tools available in skrub to encode categorical features, with an eye on choosing the best method for our specific use case."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html#introduction",
    "href": "chapters/07_feat_eng_categorical.html#introduction",
    "title": "Mixed data: dealing with categories",
    "section": "",
    "text": "Real-world datasets rarely contain only numeric values. We frequently encounter categorical features—values that belong to discrete categories, such as names, occupations, geographic locations, or clothing sizes. Text data also falls into this category, since each unique string can be considered a categorical value.\nThe challenge is that machine learning models require numeric input. How do we convert these categorical values into numeric features that preserve their information and enable our models to make good predictions?\nThis chapter explores the various strategies and tools available in skrub to encode categorical features, with an eye on choosing the best method for our specific use case."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "href": "chapters/07_feat_eng_categorical.html#why-categorical-encoders-matter",
    "title": "Mixed data: dealing with categories",
    "section": "Why categorical encoders matter",
    "text": "Why categorical encoders matter\nThe way we encode categorical features significantly impacts our machine learning pipeline:\n\nPerformance: The encoding choice directly affects how well our model learns from categorical information\nEfficiency: Some encodings create many features (potentially thousands), which increases computation time and memory usage\nInterpretability: Different encoders provide varying levels of transparency in what features represent\nScalability: Not all methods scale well to high-cardinality features (those with many unique values)\n\nUsing the appropriate encoder ensures we’re making the best use of categorical information while keeping our model efficient and interpretable."
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "href": "chapters/07_feat_eng_categorical.html#categorical-encoders-pros-and-cons",
    "title": "Mixed data: dealing with categories",
    "section": "Categorical encoders: pros and cons",
    "text": "Categorical encoders: pros and cons\n\nOne-Hot Encoding and Ordinal Encoding (scikit-learn)\nOneHotEncoder: Creates a binary indicator column for each unique category, where 1 denotes the presence of the category and 0 its absence.\nPros:\n\nStraightforward and intuitive\nWorks well for low-cardinality features (few unique values)\nProduces sparse matrices that can save memory\n\nCons:\n\nBecomes impractical with high-cardinality features (creates hundreds or thousands of columns)\nResults in mostly zero-valued sparse matrices when dense, which is the situation when working with dataframes\nIncreases overfitting risk and computational overhead\n\nThe OneHotEncoder is used by default by the skrub TableVectorizer for categorical features with fewer than 40 unique values.\nOrdinalEncoder: Assigns each category a numerical value (0, 1, 2, …).\nPros:\n\nVery memory-efficient\nCreates only one output column per input column\nFast to compute\n\nCons:\n\nIntroduces artificial ordering among categories that may not exist in reality\nCan mislead models into thinking some categories are “greater than” others\n\n\n\nCategorical encoders in skrub\nAll the categorical encoders in skrub are designed to encode any number of unique values using a fixed number of components: this number is controlled by the parameter n_components in each transformer.\n\n\nStringEncoder\nApproach: Applies term frequency-inverse document frequency (tf-idf) vectorization to character n-grams, followed by truncated singular value decomposition (SVD) for dimensionality reduction. This method is also known as Latent Semantic Analysis.\nPros:\n\nThe best all-rounder: Performs well on both categorical and text data\nFast training time\nRobust and generalizes well across different datasets\nNo artificial ordering introduced\n\nCons:\n\nLess interpretable than one-hot encoding or ordinal encoding\nMay not capture semantic relationships as well as language model-based approaches\nPerformance depends on the nature of the categorical data\n\n\n\nTextEncoder\nApproach: Uses pretrained language models from HuggingFace Hub to generate dense vector representations of text.\nPros:\n\nExceptional performance on free-flowing text and natural language\nCaptures semantic meaning and context\nLeverages knowledge from large-scale language model pretraining\nCan excel on datasets where domain-specific information aligns with pretraining data\n\nCons:\n\nVery computationally expensive: Significantly slower than other methods\nRequires heavy dependencies (PyTorch, transformers)\nModels are large and require downloading\nImpractical for CPU-only environments\nPerformance on traditional categorical data (non-text, such as IDs) is not much better than simpler methods\n\n\n\nMinHashEncoder\nApproach: Decomposes strings into n-grams and applies the MinHash algorithm for quick dimension reduction.\nPros:\n\nVery fast training time\nSimple and lightweight\nMinimal memory overhead\nGood for quick prototyping or very large-scale datasets\n\nCons:\n\nPerformance generally lags behind StringEncoder and TextEncoder\nLess nuanced feature representation\nLess robust across different types of data\n\n\n\nGapEncoder\nApproach: Estimates latent categories by finding common n-gram patterns across values, then encodes these patterns as numeric features.\nPros:\n\nInterpretable: Column names reflect the estimated categories\nCan group similar strings intelligently\nReasonable performance across datasets\n\nCons:\n\nSlower training time compared to StringEncoder and MinHashEncoder\nPerformance is on par with or slightly worse than the faster StringEncoder\nInterpretability comes at the cost of training speed\nMay require more computational resources for large datasets"
  },
  {
    "objectID": "chapters/07_feat_eng_categorical.html#conclusion",
    "href": "chapters/07_feat_eng_categorical.html#conclusion",
    "title": "Mixed data: dealing with categories",
    "section": "Conclusion",
    "text": "Conclusion\nEncoding categorical features is a critical step in preparing data for machine learning. The skrub library provides multiple encoders to handle different scenarios:\n\nStart with StringEncoder as a default for high-cardinality categorical features. It offers the best balance of speed, performance, and robustness across diverse datasets.\nUse OneHotEncoder for low-cardinality features (&lt; 40 unique values) to keep the feature space manageable.\nChoose TextEncoder if you’re working with true textual data (reviews, comments, descriptions) and have sufficient computational resources.\nConsider GapEncoder when interpretability is important and the additional training time can be dealt with.\nUse MinHashEncoder when you need maximum speed and are working with very large datasets.\n\nThe TableVectorizer integrates these encoders automatically, dispatching columns to the appropriate encoder based on their data type and cardinality. This automation makes it easy to process mixed-type datasets efficiently while still allowing fine-grained control when needed. By default, the TableVectorizer uses the OneHotEncoder for categorical features with cardinality &lt;= 40, and StringEncoder for categorical features with cardinality &gt; 40.\nFor a comprehensive empirical comparison of these methods, refer to the categorical encoders benchmark.\nIn the next chapter we will put all the encoders explained so far in a single object, the TableVectorizer."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.slides.html#introduction",
    "href": "chapters/09_tabular_pipeline.slides.html#introduction",
    "title": "Building a tabular pipeline",
    "section": "Introduction",
    "text": "Introduction\nUp until now we have covered how to clean data with the Cleaner, extract features from different column types, and handle categorical features with specialized encoders. In this section we will show how we can combine all these preprocessing techniques into a complete machine learning pipeline.\nA pipeline ensures that:\n\nData transformations are applied consistently across training and test sets\nData leakage is avoided by fitting transformers only on training data\nThe workflow is reproducible and deployable\nPreprocessing steps are properly chained together\n\nIn this chapter, we explore two approaches: building custom pipelines with TableVectorizer, and using the tabular_pipeline function for quick, well-tuned baselines."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.slides.html#manual-pipeline-construction-with-tablevectorizer",
    "href": "chapters/09_tabular_pipeline.slides.html#manual-pipeline-construction-with-tablevectorizer",
    "title": "Building a tabular pipeline",
    "section": "Manual pipeline construction with TableVectorizer",
    "text": "Manual pipeline construction with TableVectorizer\nThe TableVectorizer can be the foundation of a custom scikit-learn pipeline, where cleaning and feature engineering are dealt with by a single object. Scaling and imputation are not required by all models, so they are not in the TableVectorizer’s scope.\nWe combine it with other preprocessing steps and a final estimator:\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skrub import TableVectorizer\n\nmodel = make_pipeline(\n    TableVectorizer(),           # Feature engineering\n    SimpleImputer(),             # Handle missing values\n    StandardScaler(),            # Normalize features\n    LogisticRegression()         # Final estimator\n)\nThis approach gives complete control over which preprocessing steps to use and in what order. We can customize the TableVectorizer parameters (cardinality threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\nIn the case of the example we used LogisticRegression as our estimator, but if we used a different estimator, such as the HistogramGradientBoostingClassifier, the scaling and imputation steps could have been avoided."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.slides.html#the-tabular_pipeline",
    "href": "chapters/09_tabular_pipeline.slides.html#the-tabular_pipeline",
    "title": "Building a tabular pipeline",
    "section": "The tabular_pipeline",
    "text": "The tabular_pipeline\nFor many common use cases, we can skip the manual pipeline construction and use the tabular_pipeline function. This function automatically creates an appropriate pipeline based on the estimator we provide:\nfrom skrub import tabular_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a complete pipeline for a specific estimator\nmodel = tabular_pipeline(LogisticRegression())\nOr, we can use a string to get a pre-configured pipeline with a default estimator:\n# Classification with HistGradientBoostingClassifier\nmodel = tabular_pipeline('classification')\n\n# Regression with HistGradientBoostingRegressor\nmodel = tabular_pipeline('regression')"
  },
  {
    "objectID": "chapters/09_tabular_pipeline.slides.html#how-tabular_pipeline-adapts-to-different-estimators",
    "href": "chapters/09_tabular_pipeline.slides.html#how-tabular_pipeline-adapts-to-different-estimators",
    "title": "Building a tabular pipeline",
    "section": "How tabular_pipeline adapts to different estimators",
    "text": "How tabular_pipeline adapts to different estimators\nThe tabular_pipeline function configures the preprocessing pipeline based on the estimator type:\nFor linear models (e.g., LogisticRegression, Ridge)\n\nTableVectorizer: Uses the default configuration, except for the addition of spline-encoded datetime features by the DatetimeEncoder\nSimpleImputer: Added because linear models cannot handle missing values\nSquashingScaler: Normalizes numeric features to improve convergence and performance\nEstimator: The provided linear model\n\nThis configuration ensures numeric features are properly scaled and missing values are handled appropriately.\nFor tree-based ensemble models (RandomForest, HistGradientBoosting)\n\nTableVectorizer: Configured specifically for tree models\n\nLow-cardinality categorical features: Either kept as categorical (HistGradientBoosting) or ordinal encoded (RandomForest)\nHigh-cardinality features: StringEncoder for robust feature extraction\nDatetime features: No spline encoding (unnecessary for trees)\n\nScaler: Not added (unnecessary for tree-based models)\nEstimator: The provided tree-based estimator\n\nThis configuration leverages the native capabilities of tree models while still providing effective feature engineering through the StringEncoder."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.slides.html#conclusions-why-the-tabular_pipeline-is-useful",
    "href": "chapters/09_tabular_pipeline.slides.html#conclusions-why-the-tabular_pipeline-is-useful",
    "title": "Building a tabular pipeline",
    "section": "Conclusions: why the tabular_pipeline is useful",
    "text": "Conclusions: why the tabular_pipeline is useful\n\nSmart Configuration: Automatically selects preprocessing parameters appropriate for the estimator\nSimplicity: One-line creation of a complete, well-tuned baseline\nRobustness: Handles edge cases like missing values and mixed data types automatically\n\n\nUse tabular_pipeline when you want a quick, well-tuned baseline to benchmark against or as a starting point\nBuild manual pipelines when you need fine-grained control over preprocessing steps or want to experiment with custom transformers\n\nBoth approaches produce scikit-learn compatible pipelines that can be used with cross-validation, hyperparameter tuning, and other standard workflows."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html",
    "href": "chapters/09_tabular_pipeline.html",
    "title": "Building a tabular pipeline",
    "section": "",
    "text": "Up until now we have covered how to clean data with the Cleaner, extract features from different column types, and handle categorical features with specialized encoders. In this section we will show how we can combine all these preprocessing techniques into a complete machine learning pipeline.\nA pipeline ensures that:\n\nData transformations are applied consistently across training and test sets\nData leakage is avoided by fitting transformers only on training data\nThe workflow is reproducible and deployable\nPreprocessing steps are properly chained together\n\nIn this chapter, we explore two approaches: building custom pipelines with TableVectorizer, and using the tabular_pipeline function for quick, well-tuned baselines."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html#introduction",
    "href": "chapters/09_tabular_pipeline.html#introduction",
    "title": "Building a tabular pipeline",
    "section": "",
    "text": "Up until now we have covered how to clean data with the Cleaner, extract features from different column types, and handle categorical features with specialized encoders. In this section we will show how we can combine all these preprocessing techniques into a complete machine learning pipeline.\nA pipeline ensures that:\n\nData transformations are applied consistently across training and test sets\nData leakage is avoided by fitting transformers only on training data\nThe workflow is reproducible and deployable\nPreprocessing steps are properly chained together\n\nIn this chapter, we explore two approaches: building custom pipelines with TableVectorizer, and using the tabular_pipeline function for quick, well-tuned baselines."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "href": "chapters/09_tabular_pipeline.html#manual-pipeline-construction-with-tablevectorizer",
    "title": "Building a tabular pipeline",
    "section": "Manual pipeline construction with TableVectorizer",
    "text": "Manual pipeline construction with TableVectorizer\nThe TableVectorizer can be the foundation of a custom scikit-learn pipeline, where cleaning and feature engineering are dealt with by a single object. Scaling and imputation are not required by all models, so they are not in the TableVectorizer’s scope.\nWe combine it with other preprocessing steps and a final estimator:\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skrub import TableVectorizer\n\nmodel = make_pipeline(\n    TableVectorizer(),           # Feature engineering\n    SimpleImputer(),             # Handle missing values\n    StandardScaler(),            # Normalize features\n    LogisticRegression()         # Final estimator\n)\nThis approach gives complete control over which preprocessing steps to use and in what order. We can customize the TableVectorizer parameters (cardinality threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\nIn the case of the example we used LogisticRegression as our estimator, but if we used a different estimator, such as the HistogramGradientBoostingClassifier, the scaling and imputation steps could have been avoided."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html#the-tabular_pipeline",
    "href": "chapters/09_tabular_pipeline.html#the-tabular_pipeline",
    "title": "Building a tabular pipeline",
    "section": "The tabular_pipeline",
    "text": "The tabular_pipeline\nFor many common use cases, we can skip the manual pipeline construction and use the tabular_pipeline function. This function automatically creates an appropriate pipeline based on the estimator we provide:\nfrom skrub import tabular_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a complete pipeline for a specific estimator\nmodel = tabular_pipeline(LogisticRegression())\nOr, we can use a string to get a pre-configured pipeline with a default estimator:\n# Classification with HistGradientBoostingClassifier\nmodel = tabular_pipeline('classification')\n\n# Regression with HistGradientBoostingRegressor\nmodel = tabular_pipeline('regression')"
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "href": "chapters/09_tabular_pipeline.html#how-tabular_pipeline-adapts-to-different-estimators",
    "title": "Building a tabular pipeline",
    "section": "How tabular_pipeline adapts to different estimators",
    "text": "How tabular_pipeline adapts to different estimators\nThe tabular_pipeline function configures the preprocessing pipeline based on the estimator type:\n\nFor linear models (e.g., LogisticRegression, Ridge)\n\nTableVectorizer: Uses the default configuration, except for the addition of spline-encoded datetime features by the DatetimeEncoder\nSimpleImputer: Added because linear models cannot handle missing values\nSquashingScaler: Normalizes numeric features to improve convergence and performance\nEstimator: The provided linear model\n\nThis configuration ensures numeric features are properly scaled and missing values are handled appropriately.\n\n\nFor tree-based ensemble models (RandomForest, HistGradientBoosting)\n\nTableVectorizer: Configured specifically for tree models\n\nLow-cardinality categorical features: Either kept as categorical (HistGradientBoosting) or ordinal encoded (RandomForest)\nHigh-cardinality features: StringEncoder for robust feature extraction\nDatetime features: No spline encoding (unnecessary for trees)\n\nScaler: Not added (unnecessary for tree-based models)\nEstimator: The provided tree-based estimator\n\nThis configuration leverages the native capabilities of tree models while still providing effective feature engineering through the StringEncoder."
  },
  {
    "objectID": "chapters/09_tabular_pipeline.html#conclusions-why-the-tabular_pipeline-is-useful",
    "href": "chapters/09_tabular_pipeline.html#conclusions-why-the-tabular_pipeline-is-useful",
    "title": "Building a tabular pipeline",
    "section": "Conclusions: why the tabular_pipeline is useful",
    "text": "Conclusions: why the tabular_pipeline is useful\n\nSmart Configuration: Automatically selects preprocessing parameters appropriate for the estimator\nSimplicity: One-line creation of a complete, well-tuned baseline\nRobustness: Handles edge cases like missing values and mixed data types automatically\n\n\nUse tabular_pipeline when you want a quick, well-tuned baseline to benchmark against or as a starting point\nBuild manual pipelines when you need fine-grained control over preprocessing steps or want to experiment with custom transformers\n\nBoth approaches produce scikit-learn compatible pipelines that can be used with cross-validation, hyperparameter tuning, and other standard workflows."
  },
  {
    "objectID": "chapters/quiz_01.html",
    "href": "chapters/quiz_01.html",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "",
    "text": "What do I need to open a TableReport saved with .write_html(\"report.html\")?\n\nA) A python console\nB) An internet browser\nC) A Jupyter notebook\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nAfter its generation, the TableReport can be persisted on disk as a HTML file. The file can be opened using a regular internet browswer.\nThe TableReport is not updated dynamically, and is not connected to python consoles or running kernels."
  },
  {
    "objectID": "chapters/quiz_01.html#question-1",
    "href": "chapters/quiz_01.html#question-1",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "",
    "text": "What do I need to open a TableReport saved with .write_html(\"report.html\")?\n\nA) A python console\nB) An internet browser\nC) A Jupyter notebook\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nAfter its generation, the TableReport can be persisted on disk as a HTML file. The file can be opened using a regular internet browswer.\nThe TableReport is not updated dynamically, and is not connected to python consoles or running kernels."
  },
  {
    "objectID": "chapters/quiz_01.html#question-2",
    "href": "chapters/quiz_01.html#question-2",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\n\n\nConsider this dataframe and TableReport, then answer the question.\n\nimport pandas as pd\nfrom skrub import TableReport\n\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n    'Salary': [70000, 80000, 90000, 100000, 110000],\n    'Department': ['HR', 'Finance', 'IT', 'Marketing', 'Sales']\n})\n\nTableReport(df, max_plot_columns=5, max_association_columns=3)\n\nProcessing column   1 / 5Processing column   2 / 5Processing column   3 / 5Processing column   4 / 5Processing column   5 / 5\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWhat does the “Distributions” tab show? What about the “Associations” tab?\n\nA) Both tabs work as normal.\nB) The “Distribution” tab shows the plots, “Associations” are not shown.\nC) Both tabs contain a message explaining their operation was skipped.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nThe “Distribution” contains the usual distribution plots, while the computation of the associations was skipped because the number of columns in the dataframe (5) was larger than max_association_columns (3)."
  },
  {
    "objectID": "chapters/quiz_01.html#question-3",
    "href": "chapters/quiz_01.html#question-3",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "Question 3",
    "text": "Question 3\n\n\n\n\n\n\nDoes the TableReport parse datetimes or other data types?\n\nYes, the TableReport automatically converts datetime strings to datetime objects and strings that contain numbers into floats.\nNo, the TableReport does not perform any conversion.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: No, the TableReport is generated on the basis of the datatypes found in the supplied dataframe. Any datatype parsing must be done before generating the report, e.g., by using the Cleaner."
  },
  {
    "objectID": "chapters/quiz_01.html#question-4",
    "href": "chapters/quiz_01.html#question-4",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nWhich of these transformations is executed by default when the Cleaner is fitted on a dataframe?\n\nA) Dropping constant columns\nB) Dropping columns that contain only missing values\nC) Dropping columns that contain more than 90% of missing values\nD) Dropping columns where all values are distinct\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: B)\nColumns that contain only missing values, i.e., where the fraction of missing values is 1.0, are dropped. This is controlled by the drop_null_fraction parameter."
  },
  {
    "objectID": "chapters/quiz_01.html#question-5",
    "href": "chapters/quiz_01.html#question-5",
    "title": "Quiz: Exploring and sanitizing dataframes with skrub",
    "section": "Question 5",
    "text": "Question 5\n\n\n\n\n\n\nConsider the following dataframe.\n\nimport pandas as pd\nmedical_df = pd.DataFrame({\n    'Patient_ID': ['P001', 'P002', 'P003', 'P004', 'P005'],\n    'Visit_Date': ['10 Jan 2023', '15 Feb 2023', '20 Mar 2023', '25 Apr 2023', None],\n    'Blood_Pressure': [120.5, 130.2, 125.8, 140.0, 135.6],\n    'Diagnosis': ['Hypertension', '?', '?', 'Hypertension', 'Diabetes'],\n})\n\nmedical_df\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\n130.2\n?\n\n\n2\nP003\n20 Mar 2023\n125.8\n?\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\nWhat is the output of this cleaner?\n\nfrom skrub import Cleaner\ncleaner = Cleaner()\ndf_clean = cleaner.fit_transform(medical_df)\n\n\nA)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n2023-01-10\n120.5\nHypertension\n\n\n1\nP002\n2023-02-15\n130.2\nNone\n\n\n2\nP003\n2023-03-20\n125.8\nNone\n\n\n3\nP004\n2023-04-25\n140.0\nHypertension\n\n\n4\nP005\nNaT\n135.6\nDiabetes\n\n\n\n\n\n\n\n\nB)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\nNaN\n?\n\n\n2\nP003\n20 Mar 2023\n125.8\n?\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\n\nC)\n\n\n\n\n\n\n\n\n\n\nPatient_ID\nVisit_Date\nBlood_Pressure\nDiagnosis\n\n\n\n\n0\nP001\n10 Jan 2023\n120.5\nHypertension\n\n\n1\nP002\n15 Feb 2023\nNaN\nNone\n\n\n2\nP003\n20 Mar 2023\n125.8\nNone\n\n\n3\nP004\n25 Apr 2023\n140.0\nHypertension\n\n\n4\nP005\nNone\n135.6\nDiabetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswer: A)\nThe Cleaner replaces strings that are commonly used to denote missing values (such as “?”), and guesses most common datetime formats from their strings.\nNo empty columns are present, so no further transformations are made."
  },
  {
    "objectID": "chapters/quiz_03.html",
    "href": "chapters/quiz_03.html",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider this data with outliers. \nIn this plot, which line shows numerical features scaled by the SquashingScaler? \n\nA) The solid blue line\nB) The dashed green line\nC) The dotted purple line\nD) The dash-dotted red line\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer A)\nThe solid blue line\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich of these is not a feature that can be added by the DatetimeEncoder?\n\nA) is_holiday: a boolean flag that checks whether a date is a holiday or not\nB) periodic_encoding: a flag that can be None, \"circular\" or \"spline\" and that adds the respective periodic encoding\nC) add_weekday: a boolean flag that indicates the day of the week\nD) add_total_seconds: a boolean flag that adds the number of seconds since epoch\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nAll other parameters are available to the DatetimeEncoder.\n\n\n\n\n\n\n\n\n\n\n\n\nWhich categorical encoder is the most balanced in both runtime and downstream performance?\n\nA) The StringEncoder\nB) The MinHashEncoder\nC) The TextEncoder\nD) The GapEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nThe StringEncoder is always one of the fastest encoders to fit, and produces high quality encodings. The MinHashEncoder is faster, but it produces worse encodings. The TextEncoder can produce the best encodings for certain types of categorical data (mainly text), but is very slow to run.\n\n\n\n\n\n\n\n\n\n\n\n\nYou need to do feature engineering on a dataset that includes user reviews as part of its features. You are working in an environment that has access to good computational resources, including a GPU. Which of the following encoders would be the best choice in such a scenario?\n\nA) OneHotEncoder\nB) TextEncoder\nC) OrdinalEncoder\nD) MinHashEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nThe TextEncoder works best when it is applied to free-flowing text, or columns whose content may find additional context in the training set of the language model used by the encoder. User reviews fall in this category.\nThe OneHotEncoder would likely generate a very large number of uninformative features, and the features prepared by the OrdinalEncoder are unlikely to be informative.\nThe MinHashEncoder would generate a fixed number of encoded features, but the quality of the embeddings is lower than that of the TextEncoder. In a situation where computational resources are limited, it may be beneficial, but even in that case the StringEncoder may produce better results."
  },
  {
    "objectID": "chapters/quiz_03.html#question-1",
    "href": "chapters/quiz_03.html#question-1",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Consider this data with outliers. \nIn this plot, which line shows numerical features scaled by the SquashingScaler? \n\nA) The solid blue line\nB) The dashed green line\nC) The dotted purple line\nD) The dash-dotted red line\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer A)\nThe solid blue line"
  },
  {
    "objectID": "chapters/quiz_03.html#question-2",
    "href": "chapters/quiz_03.html#question-2",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Which of these is not a feature that can be added by the DatetimeEncoder?\n\nA) is_holiday: a boolean flag that checks whether a date is a holiday or not\nB) periodic_encoding: a flag that can be None, \"circular\" or \"spline\" and that adds the respective periodic encoding\nC) add_weekday: a boolean flag that indicates the day of the week\nD) add_total_seconds: a boolean flag that adds the number of seconds since epoch\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nAll other parameters are available to the DatetimeEncoder."
  },
  {
    "objectID": "chapters/quiz_03.html#question-3",
    "href": "chapters/quiz_03.html#question-3",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "Which categorical encoder is the most balanced in both runtime and downstream performance?\n\nA) The StringEncoder\nB) The MinHashEncoder\nC) The TextEncoder\nD) The GapEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: A)\nThe StringEncoder is always one of the fastest encoders to fit, and produces high quality encodings. The MinHashEncoder is faster, but it produces worse encodings. The TextEncoder can produce the best encodings for certain types of categorical data (mainly text), but is very slow to run."
  },
  {
    "objectID": "chapters/quiz_03.html#question-4",
    "href": "chapters/quiz_03.html#question-4",
    "title": "Quiz: Column-level transformations",
    "section": "",
    "text": "You need to do feature engineering on a dataset that includes user reviews as part of its features. You are working in an environment that has access to good computational resources, including a GPU. Which of the following encoders would be the best choice in such a scenario?\n\nA) OneHotEncoder\nB) TextEncoder\nC) OrdinalEncoder\nD) MinHashEncoder\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnswer: B)\nThe TextEncoder works best when it is applied to free-flowing text, or columns whose content may find additional context in the training set of the language model used by the encoder. User reviews fall in this category.\nThe OneHotEncoder would likely generate a very large number of uninformative features, and the features prepared by the OrdinalEncoder are unlikely to be informative.\nThe MinHashEncoder would generate a fixed number of encoded features, but the quality of the embeddings is lower than that of the TextEncoder. In a situation where computational resources are limited, it may be beneficial, but even in that case the StringEncoder may produce better results."
  }
]