{
  "hash": "1a5dd7f3ddc1b042d3031f5fa2bb4dcf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Building a tabular pipeline\"\nformat:\n    html:\n        toc: true\n    revealjs:\n        slide-number: true\n        toc: false\n        code-fold: false\n        code-tools: true\n\n---\n\n## Introduction\n\nUp until now we have covered how to clean data with the `Cleaner`, extract features \nfrom different column types, and handle categorical features with specialized\nencoders. In this section we will show how we can combine all these preprocessing\ntechniques into a complete machine learning pipeline.\n\nA pipeline ensures that:\n\n- Data transformations are applied consistently across training and test sets\n- Data leakage is avoided by fitting transformers only on training data\n- The workflow is reproducible and deployable\n- Preprocessing steps are properly chained together\n\nIn this chapter, we explore two approaches: building custom pipelines with\n`TableVectorizer`, and using the `tabular_pipeline` function for quick,\nwell-tuned baselines.\n\n## Manual pipeline construction with `TableVectorizer`\n\nThe `TableVectorizer` can be the foundation of a custom scikit-learn pipeline, \nwhere cleaning and feature engineering are dealt with by a single object. \nScaling and imputation are not required by all models, so they are not in \nthe `TableVectorizer`'s scope. \n\nWe combine it with other preprocessing steps and a final estimator:\n\n```python\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skrub import TableVectorizer\n\nmodel = make_pipeline(\n    TableVectorizer(),           # Feature engineering\n    SimpleImputer(),             # Handle missing values\n    StandardScaler(),            # Normalize features\n    LogisticRegression()         # Final estimator\n)\n```\n\nThis approach gives complete control over which preprocessing steps to use and\nin what order. We can customize the `TableVectorizer` parameters (cardinality\nthreshold, custom encoders, etc.) and add additional preprocessing steps as needed.\n\nIn the case of the example we used `LogisticRegression` as our estimator, but if we\nused a different estimator, such as the `HistogramGradientBoostingClassifier`, \nthe scaling and imputation steps could have been avoided. \n\n## The `tabular_pipeline`\n\nFor many common use cases, we can skip the manual pipeline construction and use\nthe `tabular_pipeline` function. This function automatically creates an appropriate\npipeline based on the estimator we provide:\n\n```python\nfrom skrub import tabular_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a complete pipeline for a specific estimator\nmodel = tabular_pipeline(LogisticRegression())\n```\n\nOr, we can use a string to get a pre-configured pipeline with a default estimator:\n\n```python\n# Classification with HistGradientBoostingClassifier\nmodel = tabular_pipeline('classification')\n\n# Regression with HistGradientBoostingRegressor\nmodel = tabular_pipeline('regression')\n```\n\n## How `tabular_pipeline` adapts to different estimators\n\nThe `tabular_pipeline` function configures the preprocessing pipeline\nbased on the estimator type:\n\n### For linear models (e.g., LogisticRegression, Ridge)\n\n- **TableVectorizer**: Uses the default configuration, except for the addition of \nspline-encoded datetime features by the `DatetimeEncoder`\n- **SimpleImputer**: Added because linear models cannot handle missing values\n- **SquashingScaler**: Normalizes numeric features to improve convergence and\nperformance\n- **Estimator**: The provided linear model\n\nThis configuration ensures numeric features are properly scaled and missing\nvalues are handled appropriately.\n\n### For tree-based ensemble models (RandomForest, HistGradientBoosting)\n\n- **TableVectorizer**: Configured specifically for tree models\n  - Low-cardinality categorical features: Either kept as categorical\n  (HistGradientBoosting) or ordinal encoded (RandomForest)\n  - High-cardinality features: StringEncoder for robust feature extraction\n  - Datetime features: No spline encoding (unnecessary for trees)\n- **Scaler**: Not added (unnecessary for tree-based models)\n- **Estimator**: The provided tree-based estimator\n\nThis configuration leverages the native capabilities of tree models while still\nproviding effective feature engineering through the `StringEncoder`.\n\n## Conclusions: why the `tabular_pipeline` is useful\n\n1. **Smart Configuration**: Automatically selects preprocessing parameters\nappropriate for the estimator\n2. **Simplicity**: One-line creation of a complete, well-tuned baseline\n3. **Robustness**: Handles edge cases like missing values and mixed data types\nautomatically\n\n- **Use `tabular_pipeline`** when you want a quick, well-tuned baseline to\nbenchmark against or as a starting point\n- **Build manual pipelines** when you need fine-grained control over preprocessing\nsteps or want to experiment with custom transformers\n\nBoth approaches produce scikit-learn compatible pipelines that can be used with\ncross-validation, hyperparameter tuning, and other standard workflows.\n\n# Exercise \n\n**Path to the exercise**: `content/exercises/09_tabular_pipeline.ipynb`\n\nIn this exercise we're going to use the `TableVectorizer` and `tabular_pipeline` \nto replicate the behavior of a traditional scikit-learn pipeline. \n\nFirst, let's load the dataset: \n\n::: {#dad35ff7 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nX = pd.read_csv(\"../data/adult_census/data.csv\")\ny = pd.read_csv(\"../data/adult_census/target.csv\")\n```\n:::\n\n\nThis is the pipeline that needs to be replicated. \n\n- It uses `LogisticRegression` as the classifier, i.e., a linear model. \n- It scales numerical features using a `StandardScaler`.\n- Categorical features are one-hot-encoded.\n- Missing values are imputed using a `SimpleImputer`. \n\n::: {#631fa5fe .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=\"category\")(X)\nnumerical_columns = selector(dtype_include=\"number\")(X)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel_base = make_pipeline(ct, SimpleImputer(), LogisticRegression())\n# model_base\n```\n:::\n\n\nUse the `TableVectorizer` and `make_pipeline` to write a pipeline named \n`model_tv`, which includes all the steps necessary for the `LogisticRegression` \nto work (i.e., scaling and imputing missing values).\n\n::: {#49d8fad7 .cell execution_count=3}\n``` {.python .cell-code}\nfrom skrub import TableVectorizer\n# Write your code here\n# \n# \n# \n# \n# \n# \n# \n# \n# \n```\n:::\n\n\n::: {#260bacad .cell execution_count=4}\n``` {.python .cell-code}\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer()\n\nmodel_tv = make_pipeline(tv, SimpleImputer(), StandardScaler(), LogisticRegression())\n# model_tv\n```\n:::\n\n\nNow use the `tabular_pipeline` to get a new pipeline named `model_tp`. \n\n::: {#6f33ca56 .cell execution_count=5}\n``` {.python .cell-code}\nfrom skrub import tabular_pipeline\n# Write your code here\n# \n# \n# \n```\n:::\n\n\n::: {#e7094ba4 .cell execution_count=6}\n``` {.python .cell-code}\nfrom skrub import tabular_pipeline\n\nmodel_tp = tabular_pipeline(LogisticRegression())\n# model_tp\n```\n:::\n\n\nFor reference, let's also create a pipeline that uses \n`HistGradientBoostingClassifier`. This can be done by passing the string \n\"classification\" to `tabular_pipeline`. \n\n::: {#8d84a62b .cell execution_count=7}\n``` {.python .cell-code}\nmodel_hgb = tabular_pipeline(\"classification\")\n# model_hgb\n```\n:::\n\n\nFinally, let's evaluate the different models and see how they perform: \n\n::: {#946e8820 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\nresults_base = cross_val_score(model_base, X, y)\nprint(f\"Base model: {results_base.mean():.4f}\")\n\nresults_tv = cross_val_score(model_tv, X, y)\nprint(f\"TableVectorizer: {results_tv.mean():.4f}\")\n\nresults_tp = cross_val_score(model_tp, X, y)\nprint(f\"Tabular pipeline: {results_tp.mean():.4f}\")\n\nresults_hgb = cross_val_score(model_hgb, X, y)\nprint(f\"HGB model: {results_hgb.mean():.4f}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBase model: 0.8150\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTableVectorizer: 0.8523\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTabular pipeline: 0.8528\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHGB model: 0.8730\n```\n:::\n:::\n\n\nUnsurprisingly, the model that uses HGB outperforms the other models, while\nbeing much slower to train. The other pipelines have very similar performance, \nwhich is to be expected since they are very similar to each other. \n\n",
    "supporting": [
      "09_tabular_pipeline_files"
    ],
    "filters": [],
    "includes": {}
  }
}