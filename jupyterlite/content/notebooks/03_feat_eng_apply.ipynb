{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Applying transformers to columns\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "Often, transformers need to be applied only to a subset of columns, rather than \n",
    "the entire dataframe. \n",
    "\n",
    "As an example, it does not make sense to apply a `StandardScaler` to a column \n",
    "that contains strings, and indeed doing so would raise an exception. \n",
    "In other cases, specific columns may need particular treatment, and should therefore\n",
    "be ignored by the `Cleaner`. \n",
    "\n",
    "Scikit-learn provides the `ColumnTransformer` to deal with this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "import pandas as pd\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "df = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n",
    "\n",
    "categorical_columns = selector(dtype_include=object)(df)\n",
    "numerical_columns = selector(dtype_exclude=object)(df)\n",
    "\n",
    "ct = make_column_transformer(\n",
    "      (StandardScaler(),\n",
    "       numerical_columns),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "       categorical_columns))\n",
    "transformed = ct.fit_transform(df)\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_column_selector` allows to choose columns based on their datatype, or by \n",
    "using regex to filter column names. In some cases, this degree of control is \n",
    "not sufficient. \n",
    "\n",
    "To address such situations, skrub implements different transformers that allow \n",
    "to modify columns from within scikit-learn pipelines. Additionally, the selectors\n",
    "API allows to implement powerful, custom-made column selection filters. \n",
    "\n",
    "`SelectCols` and `DropCols` are transformers that can be used as part of a \n",
    "pipeline to filter columns according to the selectors API, while `ApplyToCols` and\n",
    "`ApplyToFrame` replicate the `ColumnTransformer` behavior with a different syntax\n",
    "and access to the selectors. \n",
    "\n",
    "## Selection operations in a scikit-learn pipeline\n",
    "`SelectCols` and `DropCols` allow selecting or removing specific columns in a \n",
    "dataframe according to user-provided rules. For example, to remove columns that \n",
    "include null values, or to select only columns that have a specific dtype. \n",
    "\n",
    "`SelectCols` and `DropCols` take a `cols` parameter to choose which columns to \n",
    "select or drop respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import ToDatetime\n",
    "df = pd.DataFrame({\n",
    "    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n",
    "    \"values\": [10, 20, 30]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can selectively choose or drop columns based on names, or more complex rules \n",
    "(see the next chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import SelectCols\n",
    "SelectCols(\"date\").fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import DropCols\n",
    "DropCols(\"date\").fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ApplyToCols` and `ApplyToFrame`\n",
    "Besides selecting and dropping columns, pre-processing pipelines are intended to \n",
    "_transform_ specific columns in specific ways. To make this process easier, skrub \n",
    "provides the `ApplyToCols` and `ApplyToFrame` transformers.\n",
    "\n",
    "### Applying a transformer to separate columns: `ApplyToCols`\n",
    "In many cases, `ApplyToCols` can be a direct replacememnt for the `ColumnTransformer`,\n",
    "like in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "import skrub.selectors as s\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skrub import ApplyToCols\n",
    "\n",
    "numeric = ApplyToCols(StandardScaler(), cols=s.numeric())\n",
    "string = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n",
    "\n",
    "transformed = make_pipeline(numeric, string).fit_transform(df)\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are applying the `StandardScaler` only to numeric features using \n",
    "`s.numeric()`, and `OneHotEncoder` with `s.string()`. \n",
    "\n",
    "Under the hood, `ApplyToCol` selects all columns that satisfy the condition specified\n",
    "in `cols` (in this case, that the dtype is numeric), then clones and applies the\n",
    "specified transformer (`StandardScaler`) to each column _separately_. \n",
    "\n",
    "::: {.callout-important}\n",
    "Columns that are not selected are passed through without any change, thus string\n",
    "columns are not touched by the `numeric` transformer. \n",
    ":::\n",
    "\n",
    "By passing through unselected columns without changes it is possible to chain \n",
    "several `ApplyToCols` together by putting them in a scikit-learn pipeline. \n",
    "\n",
    "::: {.callout-important}\n",
    "`ApplyToCols` is intended to work on dataframes, which are **dense**. As a result,\n",
    "transformers that produce sparse outputs (like the `OneHotEncoder`) must be set \n",
    "so that their output is dense. \n",
    ":::\n",
    "\n",
    "\n",
    "### Applying the same transformer to multiple columns at once: `ApplyToFrame`\n",
    "In some cases, there may be a need to apply the same transformer only to a subset \n",
    "of columns in a dataframe. \n",
    "\n",
    "Consider this example dataframe, which some patient information, and some metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_patients = 20\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n",
    "    \"age\": np.random.randint(18, 80, size=n_patients),\n",
    "    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n",
    "})\n",
    "\n",
    "for i in range(5):\n",
    "    df[f\"metric_{i}\"] = np.random.normal(loc=50, scale=10, size=n_patients)\n",
    "\n",
    "df[\"diagnosis\"] = np.random.choice([\"A\", \"B\", \"C\"], size=n_patients)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ApplyToFrame`, it is easy to apply a decomposition algorithm such as `PCA` \n",
    "to condense the `metric_*` columns into a smaller number of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import ApplyToFrame\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reduce = ApplyToFrame(PCA(n_components=2), cols=s.glob(\"metric_*\"))\n",
    "\n",
    "df_reduced = reduce.fit_transform(df)\n",
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `allow_reject` parameter\n",
    "When `ApplyToCols` or `ApplyToFrame` are using a skrub transformer, they can use\n",
    "the `allow_reject` parameter for more flexibility. By setting `allow_reject` to \n",
    "`True`, columns that cannot be treated by the current transformer will be ignored\n",
    "rather than raising an exception. \n",
    "\n",
    "Consider this example. By default, `ToDatetime` raises a `RejectColumn` exception\n",
    "when it finds a column it cannot convert to datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import ToDatetime\n",
    "df = pd.DataFrame({\n",
    "    \"date\": [\"03 January 2023\", \"04 February 2023\", \"05 March 2023\"],\n",
    "    \"values\": [10, 20, 30]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `allow_reject=True`, the datetime column is converted properly and \n",
    "the other column is passed through without issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_reject = ApplyToCols(ToDatetime(), allow_reject=True)\n",
    "with_reject.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the skrub column transformers\n",
    "Skrub column transformers can be concatenated by using scikit-learn pipelines.\n",
    "In the following example, we first select only the column `patient_id`, then encode\n",
    "it using `OneHotEncoder` and finally use `PCA` to reduce the number of dimensions.\n",
    "\n",
    "This is done by wrapping the latter two steps in `ApplyToCols` and `ApplyToFrame` \n",
    "respectively, and then putting all transformers in order in a scikit-learn pipeline\n",
    "using `make_pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from skrub import SelectCols\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"patient_id\": [f\"P{i:03d}\" for i in range(n_patients)],\n",
    "    \"age\": np.random.randint(18, 80, size=n_patients),\n",
    "    \"sex\": np.random.choice([\"M\", \"F\"], size=n_patients),\n",
    "})\n",
    "\n",
    "select = SelectCols(\"patient_id\")\n",
    "encode = ApplyToCols(OneHotEncoder(sparse_output=False))\n",
    "reduce = ApplyToFrame(PCA(n_components=2))\n",
    "\n",
    "transform = make_pipeline(select, encode, reduce)\n",
    "dft= transform.fit_transform(df)\n",
    "dft.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The order of column transformations is important\n",
    "Some care must be taken when concatenating columnn transformers, in particular\n",
    "when selection is done on datatypes. Consider this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n",
    "scale = ApplyToCols(StandardScaler(), cols=s.numeric())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, we encode and then scale, in the second case we instead \n",
    "scale first and then encode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1 = make_pipeline(encode, scale)\n",
    "dft = transform_1.fit_transform(df)\n",
    "dft.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_2 = make_pipeline(scale, encode)\n",
    "dft = transform_2.fit_transform(df)\n",
    "dft.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `transform_1` is that the features that have been generated by \n",
    "the `OneHotEncoder` are then scaled by the `StandardScaler`, because the new \n",
    "features are numeric and are therefore selected in the next step. \n",
    "\n",
    "In many cases, this behavior is not desired: while some model types may not be \n",
    "affected by the different ordering (such as tree-based models), linear models\n",
    "and NN-based models may produce worse results.\n",
    "\n",
    "## Conclusions\n",
    "In this chapter we explored how skrub helps with selecting and transforming \n",
    "specific columns using various transformers. While these transformers can take \n",
    "simple lists of columns to work, they become far more flexible and powerful when\n",
    "they are combined with the skrub selectors, which is the subject of the next \n",
    "chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
