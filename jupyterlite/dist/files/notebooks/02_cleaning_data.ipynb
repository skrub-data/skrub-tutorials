{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Preprocessing data with skrub\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "1. No cleaner\n",
    "2. Cleaner\n",
    "3. DropUninformative\n",
    "\n",
    "\n",
    "In this chapter, we will show how we can quickly pre-process and sanitize \n",
    "data using skrub's `Cleaner`, and compare it to traditional methods using pandas.\n",
    "\n",
    "\n",
    "## Cleaning data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=42074)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the skrub `Cleaner`\n",
    "\n",
    "\n",
    "The `Cleaner` is intended to be a first step in preparing tabular data for \n",
    "analysis or modeling, and can handle a variety of common data cleaning tasks\n",
    "automatically. It is designed to work out-of-the-box with minimal configuration,\n",
    "although it is also possible to customize its behavior if needed.\n",
    "\n",
    "Given a dataframe, the `Cleaner` applies a sequence of transformers to each column:\n",
    "\n",
    "1. It replaces common strings used to represent missing values (e.g., `NULL`, `?`)\n",
    "with NA markers. \n",
    "2. It uses the `DropUninformative` transformer to decide whether a column is \n",
    "\"uninformative\", that is, it is not likely to bring information useful to train\n",
    "a ML model. For example, empty columns are uninformative. \n",
    "3. It tries to parse datetime columns using common formats, or a user-provided\n",
    "`datetime_format`. \n",
    "4. It processes categorical columns to ensure consistent typing depending on the \n",
    "dataframe library in use. \n",
    "5. It converts columns to string, unless they have a data type that carries more \n",
    "information, such as numerical, datetime, and categorial columns.\n",
    "6. Finally, it can convert numerical columns to `np.float32` dtype. This ensures \n",
    "a consistent representation of numbers and missing values, and helps reducing \n",
    "the memory footprint. This is useful if the Cleaner is used as the first step in\n",
    "a machine learning pipeline. \n",
    "\n",
    "## Under the hood: `DropUninformative`\n",
    "When the cleaner is fitted on a dataframe, it checks whether the dataframe includes\n",
    "uninformative columns, that is columns that could be dropped as they do not bring\n",
    "useful information for training a ML model. \n",
    "\n",
    "This is done by the `DropUninformative` transformer, which is a standalone transformer\n",
    "that the `Cleaner` leverages to sanitize data. \n",
    "`DropUninformative` marks a columns as \"uninformative\" if it satisfies one of these \n",
    "conditions:\n",
    "\n",
    "- The fraction of missing values is larger than the threshold provided by the user\n",
    "with `drop_null_fraction`. By default, this threshold is 1.0, i.e., only columns\n",
    "that contain only missing values are dropped. \n",
    "- It contains only one value, and no missing values. This is controlled by the \n",
    "`drop_if_constant` flag, which is `False` by default.. \n",
    "- All values in the column are distinct. This may be the case if the column contains\n",
    "UIDs, but it can also happen when the column contains text. This check is off by\n",
    "default and can be turned on by setting `drop_if_unique` to `True`. \n",
    "\n",
    "\n",
    "## Exercise\n",
    "Given the following dataframe, use skrub's `Cleaner` to clean the data so that:\n",
    "\n",
    "- Constant columns are removed\n",
    "- All columns with more than 50% missing values are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first examine the dataset before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "TableReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `Cleaner` to clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import Cleaner\n",
    "\n",
    "# Configure the Cleaner to:\n",
    "# - Remove constant columns (drop_if_constant=True)\n",
    "# - Remove columns with more than 50% missing values (drop_null_fraction=0.5)\n",
    "cleaner = Cleaner(drop_if_constant=True, drop_null_fraction=0.5)\n",
    "\n",
    "# Apply the cleaning\n",
    "df_cleaned = cleaner.fit_transform(df)\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "TableReport(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect which columns were dropped and what transformations were applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "print(\n",
    "    f\"\\nColumns dropped: {[col for col in df.columns if col not in cleaner.all_outputs_]}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/skrub-tutorials/skrub-tutorials/.pixi/envs/doc/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
