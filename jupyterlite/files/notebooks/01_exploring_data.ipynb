{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Exploring dataframes with skrub\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this chapter, we will show how we use the skrub `TableReport` to explore\n",
    "tabular data. We will use the Adult Census dataset as our example table, and \n",
    "perform some exploratory analysis to learn about the characteristics of the data. \n",
    "\n",
    "First, let's import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the Adult Census dataset\n",
    "data =  pd.read_csv(\"../data/adult_census/data.csv\")\n",
    "target =  pd.read_csv(\"../data/adult_census/target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe we can work with, here is a list of features of the \n",
    "data we would like to find out:\n",
    "\n",
    "- The size of the dataset. \n",
    "- The data types and names of the columns. \n",
    "- The distribution of values in the columns. \n",
    "- Whether null values are present, in what measure and where. \n",
    "- Discrete/categorical features, and their cardinality.\n",
    "- Columns strongly correlated with each other. \n",
    "\n",
    "## Exploring data with Pandas tools\n",
    "Let's first explore the data using Pandas only.\n",
    "\n",
    "We can get an idea of the content of the table by printing the first few lines, \n",
    "which gives an idea of the datatypes and the columns we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a simpler view of the datatypes in the dataframe, we must \n",
    "use `data.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.info()` we can find out the shape of the dataframe (the number of rows \n",
    "and columns), the datatype and the number of non-null values for each column. \n",
    "\n",
    "We can also get a richer summary of the data with the `.describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us useful information about all the features in the dataset. Among \n",
    "others, we can find the number of unique values in each column, various statistics\n",
    "for the numerical columns and the number of null values.\n",
    "\n",
    "## Exploring data with the `TableReport`\n",
    "Now, let's create a TableReport to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "TableReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default view of the TableReport\n",
    "The `TableReport` gives us a comprehensive overview of the dataset. The default\n",
    "view shows all the columns in the dataset, and allows to select and copy the content\n",
    "of the cells shown in the preview. \n",
    "\n",
    "The `TableReport` is intended to show a preview of the data, so it does not \n",
    "contain all the rows in the dataset, rather it shows only the first and last\n",
    "few rows by default. Similarly, it stores only the top 10 most frequent values\n",
    "for each column, if column distributions are plotted.\n",
    "\n",
    "### The \"Stats\" tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(data, open_tab=\"stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Stats\" tab provides a variety of descriptive statistics for each column in\n",
    "the dataset.\n",
    "This includes:\n",
    "\n",
    "- The column name\n",
    "- The detected data type of the column\n",
    "- Whether the column is sorted or not \n",
    "- The number of null values in the column, as well as the percentage\n",
    "- The number of unique values in the column\n",
    "\n",
    "For numerical columns, additional statistics are provided:\n",
    "\n",
    "- Mean\n",
    "- Standard deviation\n",
    "- Minimum and maximum values\n",
    "- Median\n",
    "\n",
    "Stat columns can also be sorted, for example to quickly identify which columns \n",
    "contain the most nulls, or have the largest cardinality (number of unique values).\n",
    "\n",
    "::: {.callout}\n",
    "### Filters\n",
    "Pre-made column filters are also available, allowing to select columns by dtype \n",
    "or other characteristics. Filters are shared across tabs. \n",
    ":::\n",
    "\n",
    "### The \"Distributions\" tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(data, open_tab=\"distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Distributions\" tab provides visualizations of the distributions of values \n",
    "in each column. This includes histograms for numerical columns and bar plots for\n",
    "categorical columns.\n",
    "\n",
    "The \"Distributions\" tab helps with detecting potential issues in the data, such as:\n",
    "\n",
    "- Skewed distributions\n",
    "- Outliers\n",
    "- Unexpected value frequencies\n",
    "\n",
    "For example, in this dataset we can see that some columns are heavily \n",
    "skewed, such as \"workclass\", \"race\", and \"native-country\": this is important \n",
    "information to keep track of, because these columns may require special handling\n",
    "during data preprocessing or modeling.\n",
    "\n",
    "Additionally, the \"Distributions\" tab allows to select columns manually, so that\n",
    "they can be added to a script and selected for further analysis or modeling.\n",
    "\n",
    "::: {.callout-caution}\n",
    "#### Outlier detection\n",
    "The `TableReport` detects outliers using a simple interquartile test, marking \n",
    "as outliers all values that are beyond the IQR. This is a simple heuristic, and \n",
    "should not be treated as perfect. If your problem requires reliable outlier \n",
    "detection, you should not rely exclusively on what the `TableReport` shows. \n",
    ":::\n",
    "\n",
    "### The \"Associations\" tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(data, open_tab=\"associations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Associations\" tab provides insights into the relationships between different\n",
    "columns in the dataset.\n",
    "It shows [Pearson's correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) \n",
    "coefficient for numerical columns, as well as \n",
    "[Cramér's V](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V) for all columns. \n",
    "\n",
    "While this is a somewhat rough measure of association, it can help identify potential\n",
    "relationships worth exploring further during the analysis, and highlights \n",
    "highly correlated columns: depending on the modeling technique used, these may need \n",
    "to be handled specially to avoid issues with multicollinearity.\n",
    "\n",
    "In this example, we can see that \"education-num\" and \"education\" have perfect \n",
    "correlation, which means that one of the two columns can be dropped without losing\n",
    "information.\n",
    "\n",
    "## Exploring the target variable\n",
    "Besides dataframes, the `TableReport` handles series and mono- and bi-dimensional \n",
    "numpy arrays.\n",
    "\n",
    "So, let's take a closer look at the target variable, which indicates whether an\n",
    "individual's income exceeds $50K per year. We can create a separate `TableReport`\n",
    "for the target variable to explore its distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring and saving the `TableReport` \n",
    "The `TableReport` can be saved on disk as an HTML. \n",
    "```{.python}\n",
    "TableReport(data).write_html(\"report.html\")\n",
    "```\n",
    "\n",
    "Then, the report can be opened using any internet browser, with no need to run \n",
    "a Jupyter notebok or a python interactive console. \n",
    "\n",
    "It is possible to configure various parameters using the skrub global config. \n",
    "For example, it is possible to replace the default Pandas or Polars dataframe\n",
    "display with the TableReport by using `patch_display`  (and `unpatch_display`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import patch_display, unpatch_display\n",
    "\n",
    "# replace the default pandas repr \n",
    "patch_display()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To disable, use `unpatch_display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpatch_display()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done using the skrub global configuration and changing the \n",
    "`use_table_report` flag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import set_config\n",
    "\n",
    "# replace the default pandas repr \n",
    "set_config(use_table_report=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "set_config(use_table_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detail on the skrub configuration is reported in the \n",
    "[User Guide](https://skrub-data.org/dev/modules/configuration_and_utils/customizing_configuration.html).\n",
    "\n",
    "\n",
    "## Working with big tables\n",
    "Plotting and measuring the column correlations are expensive operations and may\n",
    "take a long time, so when the dataframe under study is large it may be more\n",
    "convenient to skip them for quicker development. \n",
    "\n",
    "The `max_plot_columns` and `max_association_columns` parameters allow to set a \n",
    "threshold on the number of columns: the `TableReport` will skip the respective\n",
    "task if the number of colums in the dataframe is larger than the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(\n",
    "    data, max_association_columns=3, max_plot_columns=3, open_tab=\"distributions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of columns is too large, an information message is shown in the \n",
    "respective tab instead of the plots or correlations. \n",
    "\n",
    "## Conclusions\n",
    "In this chapter we have learned how the `TableReport` can be used to speed up \n",
    "data exploration, allowing us to find possible criticalities in the data. In the\n",
    "next chapter, we will find out how to address some of the possible problems using \n",
    "the skrub `Cleaner`.\n",
    "\n",
    "\n",
    "# Exercise: exploring a new table\n",
    "\n",
    "**Path to the exercise**: `content/exercises/01_exploring_data.ipynb`\n",
    "\n",
    "For this exercise, we will use the `employee_salaries` dataframe to answer some \n",
    "questions. \n",
    "\n",
    "Run the following code to import the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/employee_salaries/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the skrub `TableReport` and answer the following questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- What's the size of the dataframe? (columns and rows)\n",
    "- How many columns have object/numerical/datetime\n",
    "- Are there columns with a large number of missing values?\n",
    "- Are there columns that have a high cardinality (>40 unique values)?\n",
    "- Were datetime columns parsed correctly?\n",
    "- Which columns have outliers?\n",
    "- Which columns have an imbalanced distribution?\n",
    "- Which columns are strongly correlated with each other?\n",
    "\n",
    "```{.python}\n",
    "# PLACEHOLDER\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "```\n",
    "\n",
    "## Answers\n",
    "- What's the size of the dataframe? (columns and rows)\n",
    "    - 9228 rows × 8 columns\n",
    "- How many columns have object/numerical/datetime\n",
    "    - No datetime columns, one integer column (`year_first_hired`), all other columns\n",
    "    are objects. \n",
    "- Are there columns with a large number of missing values?\n",
    "    - No, only the `gender` column contains a small fraction (0.2%) of missing\n",
    "    values.\n",
    "- Are there columns that have a high cardinality?\n",
    "    - Yes, `division`, `employee_position_title`, `date_first_hired` have a \n",
    "    cardinality larger than 40. \n",
    "- Were datetime columns parsed correctly?\n",
    "    - No, the `date_first_hired` column has dtype Object. \n",
    "- Which columns have outliers?\n",
    "    - No columns seem to include outliers. \n",
    "- Which columns have an imbalanced distribution?\n",
    "    - `assignment_category` has an unbalanced distribution. \n",
    "- Which columns are strongly correlated with each other?\n",
    "    - `department` and `department_name` have a Cramer's V of 1, so they are \n",
    "    very strongly correlated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
