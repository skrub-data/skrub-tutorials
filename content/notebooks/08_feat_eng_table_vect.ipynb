{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"All the pre-processing in one place: `TableVectorizer`\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "## What is the TableVectorizer?\n",
    "\n",
    "Machine learning models typically require numeric input features. When working\n",
    "with real-world datasets, we often have a mix of data types: numbers, text,\n",
    "dates, and categorical values. The `TableVectorizer` automates the entire process\n",
    "of converting a heterogeneous dataframe into a matrix of numeric features ready\n",
    "for machine learning.\n",
    "\n",
    "Instead of manually specifying how to handle each column, the `TableVectorizer`\n",
    "automatically detects the data type of each column and applies the appropriate\n",
    "transformation to encode the column using numerical features. \n",
    "\n",
    "## How does the TableVectorizer work?\n",
    "\n",
    "The `TableVectorizer` operates in two phases:\n",
    "\n",
    "### Phase 1: Data Cleaning and Type Detection\n",
    "\n",
    "First, it runs a `Cleaner` on the input data to:\n",
    "- Detect and parse datetime columns (possibly, with custom datetime formats)\n",
    "- Handle missing values represented as strings (e.g., \"N/A\")\n",
    "- Clean up categorical columns to have consistent typing\n",
    "- Remove uninformative columns (those with only nulls, constant values, or all\n",
    "unique values)\n",
    "- Finally, convert all numerical features to `float32` to reduce the computational\n",
    "cost. \n",
    "\n",
    "This ensures that each column has the correct data type before encoding.\n",
    "\n",
    "### Phase 2: Column Dispatch and Encoding\n",
    "\n",
    "After cleaning, the `TableVectorizer` categorizes columns and dispatches them\n",
    "to the appropriate transformer based on their data type and cardinality.\n",
    "\n",
    "The `TableVectorizer` uses the following default transformers for each column type:\n",
    "\n",
    "- **Numeric columns**: Left untouched (passthrough) - they're already in the\n",
    "right format\n",
    "- **Datetime columns**: Transformed by `DatetimeEncoder` to extract meaningful\n",
    "temporal features\n",
    "- **Low-cardinality categorical/string columns**: Transformed with `OneHotEncoder`\n",
    "to create binary indicator variables\n",
    "- **High-cardinality categorical/string columns**: Transformed with `StringEncoder`\n",
    "to create dense numeric representations\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "### Cardinality Threshold\n",
    "\n",
    "By default, columns with 40 or fewer unique values are considered \"low-cardinality\"\n",
    "and one-hot encoded, while those with more unique values are \"high-cardinality\"\n",
    "and encoded with `StringEncoder`. We can change this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "tv = TableVectorizer(cardinality_threshold=30)  # Adjust the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Parameters\n",
    "\n",
    "The `TableVectorizer` forwards several parameters to the internal `Cleaner`:\n",
    "\n",
    "- `drop_null_fraction`: Fraction of nulls above which a column is dropped (default: `1.0`)\n",
    "- `drop_if_constant`: Drop columns with only one unique value (default: `False`)\n",
    "- `drop_if_unique`: Drop string/categorical columns where all values are unique \n",
    "(default: `False`) \n",
    "- `datetime_format`: Format string for parsing dates\n",
    "\n",
    "Note that for `drop_if_constant` null values count as one additional distinct value.\n",
    "`drop_if_unique` should be used with care when working with free-flowing text,\n",
    "as in this case it is quite likely that all strings will be different, but the\n",
    "column is not uninformative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TableVectorizer(\n",
    "    drop_null_fraction=0.9,  # Drop columns that are 90% null\n",
    "    drop_if_constant=True,\n",
    "    datetime_format=\"%Y-%m-%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Custom Transformers\n",
    "\n",
    "The `TableVectorizer` applies whatever transformer is provided to each of the \n",
    "`numeric`, `datetime`, `high_cardinality`, and `low_cardinality` paramters. To\n",
    "tweak the default parameters of the transformers a new transformer should be \n",
    "provided: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer, DatetimeEncoder, StringEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create custom transformers\n",
    "datetime_enc = DatetimeEncoder(periodic_encoding=\"circular\")\n",
    "string_enc = StringEncoder(n_components=10)\n",
    "\n",
    "# Pass them to TableVectorizer\n",
    "tv = TableVectorizer(\n",
    "    datetime=datetime_enc,\n",
    "    high_cardinality=string_enc,\n",
    "    low_cardinality=OneHotEncoder(sparse_output=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows to, for example, change the neumber of parameters in the `StringEncoder`, \n",
    "or provide a custom datetime format for the `DatetimeEncoder`. \n",
    "\n",
    "\n",
    "## Using `specific_transformers` for Column-Specific Control\n",
    "\n",
    "For fine-grained control, we can specify transformers for specific columns using\n",
    "the `specific_transformers` parameter. This is useful when we want to override\n",
    "the default behavior for particular columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"occupation\": [\"engineer\", \"teacher\", \"doctor\"],\n",
    "    \"salary\": [100000, 50000, 150000]\n",
    "})\n",
    "\n",
    "# Create a custom transformer for the 'occupation' column\n",
    "specific_transformers = [(OrdinalEncoder(), [\"occupation\"])]\n",
    "\n",
    "tv = TableVectorizer(specific_transformers=specific_transformers)\n",
    "result = tv.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important notes about `specific_transformers`:\n",
    "\n",
    "- Columns specified here bypass the default categorization logic\n",
    "- The transformer receives the column as-is, without any preprocessing\n",
    "- The transformer must be able to handle the column's current data type and values\n",
    "- For more complex transformations, consider using `ApplyToCols` and the selectors API\n",
    "(explained in the previous chapters), or the skrub\n",
    "[Data Ops](https://skrub-data.org/stable/auto_examples/data_ops/11_data_ops_intro.html).\n",
    "\n",
    "# Exercise: implementing a `TableVectorizer` from its components\n",
    "Replicate the behavior of a `TableVectorizer` using `ApplyToCols`, the skrub \n",
    "selectors, and the given transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import Cleaner, ApplyToCols, StringEncoder, DatetimeEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import skrub.selectors as s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the implementation: \n",
    "\n",
    "- In the first step, the TableVectorizer cleans the data to parse datetimes and other\n",
    "dtypes.\n",
    "- Numeric features are left untouched, i.e., they use a Passthrough transformer. \n",
    "- String and categorical feature are split into high and low cardinality features. \n",
    "- For this exercise, set the the cardinality `threshold` to 4. \n",
    "- High cardinality features are transformed with a `StringEncoder`. In this exercise,\n",
    "set `n_components` to 2. \n",
    "- Low cardinality features are transformed with a `OneHotEncoder`, and the first \n",
    "category in binary features is dropped (hint: check the docs of the `OneHotEncoder`\n",
    "for the `drop` parameter). Set `sparse_output=True`.\n",
    "- Remember  `cardinality_below` is one of the skrub selectors. \n",
    "- Datetimes are transformed by a default `DatetimeEncoder`. \n",
    "- Everything should be wrapped in a scikit-learn `Pipeline`. \n",
    "\n",
    "\n",
    "Use the following dataframe to test the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "data = {\n",
    "    \"int\": [15, 56, 63, 12, 44],\n",
    "    \"float\": [5.2, 2.4, 6.2, 10.45, 9.0],\n",
    "    \"str1\": [\"public\", \"private\", \"private\", \"private\", \"public\"],\n",
    "    \"str2\": [\"officer\", \"manager\", \"lawyer\", \"chef\", \"teacher\"],\n",
    "    \"bool\": [True, False, True, False, True],\n",
    "    \"datetime-col\": [\n",
    "            \"2020-02-03T12:30:05\",\n",
    "            \"2021-03-15T00:37:15\",\n",
    "            \"2022-02-13T17:03:25\",\n",
    "            \"2023-05-22T08:45:55\",\n",
    "    ]\n",
    "    + [None],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following `PassThrough` transformer where needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub._apply_to_cols import SingleColumnTransformer\n",
    "class PassThrough(SingleColumnTransformer):\n",
    "    def fit_transform(self, column, y=None):\n",
    "        return column\n",
    "\n",
    "    def transform(self, column):\n",
    "        return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the correctness of your solution by comparing it with the equivalent\n",
    "`TableVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "tv = TableVectorizer(\n",
    "    high_cardinality=StringEncoder(n_components=2), cardinality_threshold=4\n",
    ")\n",
    "tv.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "cleaner = ApplyToCols(Cleaner())\n",
    "high_cardinality = ApplyToCols(\n",
    "    StringEncoder(n_components=2), cols=~s.cardinality_below(4) & (s.string())\n",
    ")\n",
    "low_cardinality = ApplyToCols(\n",
    "    OneHotEncoder(sparse_output=False, drop=\"if_binary\"),\n",
    "    cols=s.cardinality_below(4) & s.string(),\n",
    ")\n",
    "numeric = ApplyToCols(PassThrough(), cols=s.numeric())\n",
    "datetime = ApplyToCols(DatetimeEncoder(), cols=s.any_date())\n",
    "\n",
    "my_table_vectorizer = make_pipeline(\n",
    "    cleaner, numeric, high_cardinality, low_cardinality, datetime\n",
    ")\n",
    "\n",
    "my_table_vectorizer.fit_transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
