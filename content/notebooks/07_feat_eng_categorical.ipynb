{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Mixed data: dealing with categories\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: The Challenge of Categorical Features\n",
    "\n",
    "Real-world datasets rarely contain only numeric values. We frequently encounter\n",
    "categorical featuresâ€”values that belong to discrete categories, such as names,\n",
    "occupations, geographic locations, or clothing sizes. Text data also falls into\n",
    "this category, since each unique string can be considered a categorical value.\n",
    "\n",
    "The challenge is that machine learning models require numeric input. How do we\n",
    "convert these categorical values into numeric features that preserve their\n",
    "information and enable our models to make good predictions?\n",
    "\n",
    "This chapter explores the various strategies and tools available in skrub to\n",
    "encode categorical features, helping us choose the right approach for our specific\n",
    "use case.\n",
    "\n",
    "## Why Categorical Encoders Matter\n",
    "\n",
    "The way we encode categorical features significantly impacts our machine learning\n",
    "pipeline:\n",
    "\n",
    "- **Performance**: The encoding choice directly affects how well our model learns\n",
    "from categorical information\n",
    "- **Efficiency**: Some encodings create many features (potentially thousands), \n",
    "which increases computation time and memory usage\n",
    "- **Interpretability**: Different encoders provide varying levels of transparency\n",
    "in what features represent\n",
    "- **Scalability**: Not all methods scale well to high-cardinality features (those\n",
    "with many unique values)\n",
    "\n",
    "Using the appropriate encoder ensures we're making the best use of categorical\n",
    "information while keeping our model efficient and interpretable.\n",
    "\n",
    "## Categorical Encoders: Pros and Cons\n",
    "\n",
    "### One-Hot Encoding and Ordinal Encoding (scikit-learn)\n",
    "\n",
    "**`OneHotEncoder`**: Creates a binary indicator column for each unique category, \n",
    "where `1` denotes the presence of the category and `0` its absence.\n",
    "\n",
    "**Pros:**\n",
    "- Straightforward and intuitive\n",
    "- Works well for low-cardinality features (few unique values)\n",
    "- Produces sparse matrices that can save memory\n",
    "\n",
    "**Cons:**\n",
    "- Becomes impractical with high-cardinality features (creates hundreds or\n",
    "thousands of columns)\n",
    "- Results in mostly zero-valued sparse matrices when dense, which is the situation\n",
    "when working with dataframes\n",
    "- Increases overfitting risk and computational overhead\n",
    "\n",
    "The `OneHotEncoder` is used by default by the skrub `TableVectorizer` for categorical\n",
    "features with fewer than 40 unique values. \n",
    "\n",
    "**`OrdinalEncoder`**: Assigns each category a numerical value (0, 1, 2, ...).\n",
    "\n",
    "**Pros:**\n",
    "- Very memory-efficient\n",
    "- Creates only one output column per input column\n",
    "- Fast to compute\n",
    "\n",
    "**Cons:**\n",
    "- Introduces artificial ordering among categories that may not exist in reality\n",
    "- Can mislead models into thinking some categories are \"greater than\" others\n",
    "\n",
    "### Categorical encoders in skrub\n",
    "All the categorical encoders in skrub are designed to encode any number of unique\n",
    "values using a fixed number of components:  this number is controlled by the parameter\n",
    "`n_components` in each transformer. \n",
    "\n",
    "### StringEncoder\n",
    "\n",
    "**Approach**: Applies term frequency-inverse document frequency (tf-idf) vectorization\n",
    "to character n-grams, followed by truncated singular value decomposition (SVD)\n",
    "for dimensionality reduction. This method is also known as \n",
    "[Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis). \n",
    "\n",
    "**Pros:**\n",
    "- **The best all-rounder**: Performs well on both categorical and text data\n",
    "- Fast training time\n",
    "- Robust and generalizes well across different datasets\n",
    "- No artificial ordering introduced\n",
    "\n",
    "**Cons:**\n",
    "- Less interpretable than one-hot encoding or ordinal encoding\n",
    "- May not capture semantic relationships as well as language model-based approaches\n",
    "- Performance depends on the nature of the categorical data\n",
    "\n",
    "### TextEncoder\n",
    "\n",
    "**Approach**: Uses pretrained language models from HuggingFace Hub to generate\n",
    "dense vector representations of text.\n",
    "\n",
    "**Pros:**\n",
    "- Exceptional performance on free-flowing text and natural language\n",
    "- Captures semantic meaning and context\n",
    "- Leverages knowledge from large-scale language model pretraining\n",
    "- Can excel on datasets where domain-specific information aligns with pretraining data\n",
    "\n",
    "**Cons:**\n",
    "- **Very computationally expensive**: Significantly slower than other methods\n",
    "- Requires heavy dependencies (PyTorch, transformers)\n",
    "- Models are large and require downloading\n",
    "- Impractical for CPU-only environments\n",
    "- Performance on traditional categorical data (non-text, such as IDs) is not much\n",
    "better than simpler methods\n",
    "\n",
    "### MinHashEncoder\n",
    "\n",
    "**Approach**: Decomposes strings into n-grams and applies the MinHash algorithm\n",
    "for quick dimension reduction.\n",
    "\n",
    "**Pros:**\n",
    "- Very fast training time\n",
    "- Simple and lightweight\n",
    "- Minimal memory overhead\n",
    "- Good for quick prototyping or very large-scale datasets\n",
    "\n",
    "**Cons:**\n",
    "- Performance generally lags behind `StringEncoder` and `TextEncoder`\n",
    "- Less nuanced feature representation\n",
    "- Less robust across different types of data\n",
    "\n",
    "### GapEncoder\n",
    "\n",
    "**Approach**: Estimates latent categories by finding common n-gram patterns\n",
    "across values, then encodes these patterns as numeric features.\n",
    "\n",
    "**Pros:**\n",
    "- Interpretable: Column names reflect the estimated categories\n",
    "- Can group similar strings intelligently\n",
    "- Good for exploratory data analysis\n",
    "- Reasonable performance across datasets\n",
    "\n",
    "**Cons:**\n",
    "- Slower training time compared to `StringEncoder` and `MinHashEncoder`\n",
    "- Interpretability comes at the cost of training speed\n",
    "- May require more computational resources for large datasets\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Encoding categorical features is a critical step in preparing data for machine\n",
    "learning. The skrub library provides multiple encoders to handle different scenarios:\n",
    "\n",
    "- **Start with `StringEncoder`** as a default for high-cardinality categorical\n",
    "features. It offers the best balance of speed, performance, and robustness across\n",
    "diverse datasets.\n",
    "- **Use `OneHotEncoder`** for low-cardinality features (< 40 unique values) to\n",
    "keep the feature space manageable.\n",
    "- **Choose `TextEncoder`** if you're working with true textual data (reviews,\n",
    "comments, descriptions) and have sufficient computational resources.\n",
    "- **Consider `GapEncoder`** when interpretability is important and \n",
    "the additional training time can be dealt with. \n",
    "- **Use `MinHashEncoder`** when you need maximum speed and are working with very\n",
    "large datasets.\n",
    "\n",
    "The `TableVectorizer` integrates these encoders automatically, dispatching columns\n",
    "to the appropriate encoder based on their data type and cardinality. This automation\n",
    "makes it easy to process mixed-type datasets efficiently while still allowing\n",
    "fine-grained control when needed. By default, the `TableVectorizer` uses the \n",
    "`OneHotEncoder` for categorical features with cardinality <= 40, and `StringEncoder`\n",
    "for categorical features with cardinality > 40. \n",
    "\n",
    "For a comprehensive empirical comparison of these methods, refer to the\n",
    "[categorical encoders benchmark](https://skrub-data.org/skrub-materials/pages/notebooks/categorical-encoders/categorical-encoders.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
