{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building a tabular pipeline\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Up until now we have covered how to clean data with the `Cleaner`, extract features \n",
    "from different column types, and handle categorical features with specialized\n",
    "encoders. In this section we will show how we can combine all these preprocessing\n",
    "techniques into a complete machine learning pipeline.\n",
    "\n",
    "A pipeline ensures that:\n",
    "\n",
    "- Data transformations are applied consistently across training and test sets\n",
    "- Data leakage is avoided by fitting transformers only on training data\n",
    "- The workflow is reproducible and deployable\n",
    "- Preprocessing steps are properly chained together\n",
    "\n",
    "In this chapter, we explore two approaches: building custom pipelines with\n",
    "`TableVectorizer`, and using the `tabular_pipeline` function for quick,\n",
    "well-tuned baselines.\n",
    "\n",
    "## Manual pipeline construction with `TableVectorizer`\n",
    "\n",
    "The `TableVectorizer` can be the foundation of a custom scikit-learn pipeline, \n",
    "where cleaning and feature engineering are dealt with by a single object. \n",
    "Scaling and imputation are not required by all models, so they are not in \n",
    "the `TableVectorizer`'s scope. \n",
    "\n",
    "We combine it with other preprocessing steps and a final estimator:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "model = make_pipeline(\n",
    "    TableVectorizer(),           # Feature engineering\n",
    "    SimpleImputer(),             # Handle missing values\n",
    "    StandardScaler(),            # Normalize features\n",
    "    LogisticRegression()         # Final estimator\n",
    ")\n",
    "```\n",
    "\n",
    "This approach gives complete control over which preprocessing steps to use and\n",
    "in what order. We can customize the `TableVectorizer` parameters (cardinality\n",
    "threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\n",
    "\n",
    "In the case of the example we used `LogisticRegression` as our estimator, but if we\n",
    "used a different estimator, such as the `HistogramGradientBoostingClassifier`, \n",
    "the scaling and imputation steps could have been avoided. \n",
    "\n",
    "## The `tabular_pipeline`\n",
    "\n",
    "For many common use cases, we can skip the manual pipeline construction and use\n",
    "the `tabular_pipeline` function. This function automatically creates an appropriate\n",
    "pipeline based on the estimator we provide:\n",
    "\n",
    "```python\n",
    "from skrub import tabular_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a complete pipeline for a specific estimator\n",
    "model = tabular_pipeline(LogisticRegression())\n",
    "```\n",
    "\n",
    "Or, we can use a string to get a pre-configured pipeline with a default estimator:\n",
    "\n",
    "```python\n",
    "# Classification with HistGradientBoostingClassifier\n",
    "model = tabular_pipeline('classification')\n",
    "\n",
    "# Regression with HistGradientBoostingRegressor\n",
    "model = tabular_pipeline('regression')\n",
    "```\n",
    "\n",
    "## How `tabular_pipeline` adapts to different estimators\n",
    "\n",
    "The `tabular_pipeline` function intelligently configures the preprocessing pipeline\n",
    "based on the estimator type:\n",
    "\n",
    "### For linear models (e.g., LogisticRegression, Ridge)\n",
    "\n",
    "- **TableVectorizer**: Uses the default configuration, except for the addition of \n",
    "spline-encoded datetime features by the `DatetimeEncoder`\n",
    "- **SimpleImputer**: Added because linear models cannot handle missing values\n",
    "- **SquashingScaler**: Normalizes numeric features to improve convergence and\n",
    "performance\n",
    "- **Estimator**: The provided linear model\n",
    "\n",
    "This configuration ensures numeric features are properly scaled and missing\n",
    "values are handled appropriately.\n",
    "\n",
    "### For tree-based ensemble models (RandomForest, HistGradientBoosting)\n",
    "\n",
    "- **TableVectorizer**: Configured specifically for tree models\n",
    "  - Low-cardinality categorical features: Either kept as categorical\n",
    "  (HistGradientBoosting) or ordinal encoded (RandomForest)\n",
    "  - High-cardinality features: StringEncoder for robust feature extraction\n",
    "  - Datetime features: No spline encoding (unnecessary for trees)\n",
    "- **Scaler**: Not added (unnecessary for tree-based models)\n",
    "- **Estimator**: The provided tree-based estimator\n",
    "\n",
    "This configuration leverages the native capabilities of tree models while still\n",
    "providing effective feature engineering through the `StringEncoder`.\n",
    "\n",
    "## Conclusions: why the `tabular_pipeline` is useful\n",
    "\n",
    "1. **Smart Configuration**: Automatically selects preprocessing parameters\n",
    "appropriate for the estimator\n",
    "2. **Simplicity**: One-line creation of a complete, well-tuned baseline\n",
    "3. **Robustness**: Handles edge cases like missing values and mixed data types\n",
    "automatically\n",
    "\n",
    "- **Use `tabular_pipeline`** when you want a quick, well-tuned baseline to\n",
    "benchmark against or as a starting point\n",
    "- **Build manual pipelines** when you need fine-grained control over preprocessing\n",
    "steps or want to experiment with custom transformers\n",
    "\n",
    "Both approaches produce scikit-learn compatible pipelines that can be used with\n",
    "cross-validation, hyperparameter tuning, and other standard workflows.\n",
    "\n",
    "# Exercise \n",
    "\n",
    "**Path to the exercise**: `content/exercises/09_tabular_pipeline.ipynb`\n",
    "\n",
    "In this exercise we're going to use the `TableVectorizer` and `tabular_pipeline` \n",
    "to replicate the behavior of a traditional scikit-learn pipeline. \n",
    "\n",
    "First, let's load the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(\"../data/adult_census/data.csv\")\n",
    "y = pd.read_csv(\"../data/adult_census/target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pipeline that needs to be replicated. \n",
    "\n",
    "- It uses `LogisticRegression` as the classifier, i.e., a linear model. \n",
    "- It scales numerical features using a `StandardScaler`.\n",
    "- Categorical features are one-hot-encoded.\n",
    "- Missing values are imputed using a `SimpleImputer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "categorical_columns = selector(dtype_include=\"category\")(X)\n",
    "numerical_columns = selector(dtype_include=\"number\")(X)\n",
    "\n",
    "ct = make_column_transformer(\n",
    "      (StandardScaler(),\n",
    "       numerical_columns),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "       categorical_columns))\n",
    "\n",
    "model_base = make_pipeline(ct, SimpleImputer(), LogisticRegression())\n",
    "# model_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `TableVectorizer` and `make_pipeline` to write a pipeline named \n",
    "`model_tv`, which includes all the steps necessary for the `LogisticRegression` \n",
    "to work (i.e., scaling and imputing missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "# Write your code here\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "tv = TableVectorizer()\n",
    "\n",
    "model_tv = make_pipeline(tv, SimpleImputer(), StandardScaler(), LogisticRegression())\n",
    "# model_tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `tabular_pipeline` to get a new pipeline named `model_tp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "# Write your code here\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "\n",
    "model_tp = tabular_pipeline(LogisticRegression())\n",
    "# model_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, let's also create a pipeline that uses \n",
    "`HistGradientBoostingClassifier`. This can be done by passing the string \n",
    "\"classification\" to `tabular_pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hgb = tabular_pipeline(\"classification\")\n",
    "# model_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the different models and see how they perform: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results_base = cross_val_score(model_base, X, y)\n",
    "print(f\"Base model: {results_base.mean():.4f}\")\n",
    "\n",
    "results_tv = cross_val_score(model_tv, X, y)\n",
    "print(f\"TableVectorizer: {results_tv.mean():.4f}\")\n",
    "\n",
    "results_tp = cross_val_score(model_tp, X, y)\n",
    "print(f\"Tabular pipeline: {results_tp.mean():.4f}\")\n",
    "\n",
    "results_hgb = cross_val_score(model_hgb, X, y)\n",
    "print(f\"HGB model: {results_hgb.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the model that uses HGB outperforms the other models, while\n",
    "being much slower to train. The other pipelines have very similar performance, \n",
    "which is to be expected since they are very similar to each other. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
