{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building a tabular pipeline\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: From Raw Data to Predictions\n",
    "\n",
    "Up until now we have covered how to clean data with the `Cleaner`, extract features \n",
    "from different column types, and handle categorical features with specialized\n",
    "encoders. In this section we will show how we can combine all these preprocessing\n",
    "techniques into a complete machine learning pipeline.\n",
    "\n",
    "A pipeline ensures that:\n",
    "- Data transformations are applied consistently across training and test sets\n",
    "- Data leakage is avoided by fitting transformers only on training data\n",
    "- The workflow is reproducible and deployable\n",
    "- Preprocessing steps are properly chained together\n",
    "\n",
    "In this chapter, we explore two approaches: building custom pipelines with\n",
    "`TableVectorizer`, and using the `tabular_pipeline` function for quick,\n",
    "well-tuned baselines.\n",
    "\n",
    "## Manual Pipeline Construction with TableVectorizer\n",
    "\n",
    "The `TableVectorizer` can be the foundation of a custom scikit-learn pipeline.\n",
    "We combine it with other preprocessing steps and a final estimator:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "model = make_pipeline(\n",
    "    TableVectorizer(),           # Feature engineering\n",
    "    SimpleImputer(),             # Handle missing values\n",
    "    StandardScaler(),            # Normalize features\n",
    "    LogisticRegression()         # Final estimator\n",
    ")\n",
    "```\n",
    "\n",
    "This approach gives complete control over which preprocessing steps to use and\n",
    "in what order. We can customize the `TableVectorizer` parameters (cardinality\n",
    "threshold, custom encoders, etc.) and add additional preprocessing steps as needed.\n",
    "\n",
    "In the case of the example we used `LogisticRegression` as our estimator, but if we\n",
    "used a different estimator, such as the `HistogramGradientBoostingClassifier`, \n",
    "the scaling and imputation steps could have been avoided. \n",
    "\n",
    "## The `tabular_pipeline` function\n",
    "\n",
    "For most common use cases, we can skip the manual pipeline construction and use\n",
    "the `tabular_pipeline` function. This function automatically creates an appropriate\n",
    "pipeline based on the estimator we provide:\n",
    "\n",
    "```python\n",
    "from skrub import tabular_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a complete pipeline for a specific estimator\n",
    "model = tabular_pipeline(LogisticRegression())\n",
    "```\n",
    "\n",
    "Or, we can use a string to get a pre-configured pipeline with a default estimator:\n",
    "\n",
    "```python\n",
    "# Classification with HistGradientBoostingClassifier\n",
    "model = tabular_pipeline('classification')\n",
    "\n",
    "# Regression with HistGradientBoostingRegressor\n",
    "model = tabular_pipeline('regression')\n",
    "```\n",
    "\n",
    "## How tabular_pipeline Adapts to Different Estimators\n",
    "\n",
    "The `tabular_pipeline` function intelligently configures the preprocessing pipeline\n",
    "based on the estimator type:\n",
    "\n",
    "### For Linear Models (e.g., LogisticRegression, Ridge)\n",
    "\n",
    "- **TableVectorizer**: Uses the default configuration, except for the addition of \n",
    "spline-encoded datetime features by the `DatetimeEncoder`\n",
    "- **SimpleImputer**: Added because linear models cannot handle missing values\n",
    "- **SquashingScaler**: Normalizes numeric features to improve convergence and\n",
    "performance\n",
    "- **Estimator**: The provided linear model\n",
    "\n",
    "This configuration ensures numeric features are properly scaled and missing\n",
    "values are handled appropriately.\n",
    "\n",
    "### For Tree-Based Ensemble Models (RandomForest, HistGradientBoosting)\n",
    "\n",
    "- **TableVectorizer**: Configured specifically for tree models\n",
    "  - Low-cardinality categorical features: Either kept as categorical\n",
    "  (HistGradientBoosting) or ordinal encoded (RandomForest)\n",
    "  - High-cardinality features: StringEncoder for robust feature extraction\n",
    "  - Datetime features: No spline encoding (unnecessary for trees)\n",
    "- **SimpleImputer**: Only added for older scikit-learn versions (< 1.4) without native missing value support\n",
    "- **Scaler**: Not added (unnecessary for tree-based models)\n",
    "- **Estimator**: The provided tree-based estimator\n",
    "\n",
    "This configuration leverages the native capabilities of tree models while still\n",
    "providing effective feature engineering through the `StringEncoder`.\n",
    "\n",
    "## Key Advantages of tabular_pipeline\n",
    "\n",
    "1. **Smart Configuration**: Automatically selects preprocessing parameters appropriate for the estimator\n",
    "2. **Simplicity**: One-line creation of a complete, well-tuned baseline\n",
    "3. **Robustness**: Handles edge cases like missing values and mixed data types\n",
    "automatically\n",
    "\n",
    "- **Use `tabular_pipeline`** when you want a quick, well-tuned baseline to benchmark against or as a starting point\n",
    "- **Build manual pipelines** when you need fine-grained control over preprocessing steps or want to experiment with custom transformers\n",
    "\n",
    "Both approaches produce scikit-learn compatible pipelines that can be used with cross-validation, hyperparameter tuning, and other standard workflows.\n",
    "\n",
    "# Exercise \n",
    "In this exercise we're going to use the `TableVectorizer` and `tabular_pipeline` \n",
    "to replicate the behavior of a traditional scikit-learn pipeline. \n",
    "\n",
    "First, let's load the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub.datasets import fetch_employee_salaries\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "adult = fetch_openml(\"adult\", version=2)  \n",
    "X = adult.data\n",
    "y = adult.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pipeline that needs to be replicated. \n",
    "\n",
    "- It uses `LogisticRegression` as the classifier, i.e., a linear model. \n",
    "- It scales numerical features using a `StandardScaler`.\n",
    "- Categorical features are one-hot-encoded.\n",
    "- Missing values are imputed using a `SimpleImputer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "categorical_columns = selector(dtype_include=\"category\")(X)\n",
    "numerical_columns = selector(dtype_include=\"number\")(X)\n",
    "\n",
    "ct = make_column_transformer(\n",
    "      (StandardScaler(),\n",
    "       numerical_columns),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "       categorical_columns))\n",
    "\n",
    "model_base = make_pipeline(ct, SimpleImputer(), LogisticRegression())\n",
    "model_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `TableVectorizer` and `make_pipeline` to write a pipeline named \n",
    "`model_tv`, which includes all the steps necessary for the `LogisticRegression` \n",
    "to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "# Write your code here\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "tv = TableVectorizer()\n",
    "\n",
    "model_tv = make_pipeline(tv, SimpleImputer(), StandardScaler(), LogisticRegression())\n",
    "model_tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `tabular_pipeline` to get a new pipeline named `model_tp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "# Write your code here\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "\n",
    "model_tp = tabular_pipeline(LogisticRegression())\n",
    "model_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, let's also create a pipeline that uses \n",
    "`HistGradientBoostingClassifier`. This can be done by passing the string \n",
    "\"classification\" to `tabular_pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hgb = tabular_pipeline(\"classification\")\n",
    "model_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the different models and see how they perform: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results_base = cross_val_score(model_base, X, y)\n",
    "print(f\"Base model: {results_base.mean():.4f}\")\n",
    "\n",
    "results_tv = cross_val_score(model_tv, X, y)\n",
    "print(f\"TableVectorizer: {results_tv.mean():.4f}\")\n",
    "\n",
    "results_tp = cross_val_score(model_tp, X, y)\n",
    "print(f\"Tabular pipeline: {results_tp.mean():.4f}\")\n",
    "\n",
    "results_hgb = cross_val_score(model_hgb, X, y)\n",
    "print(f\"HGB model: {results_hgb.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisiingly, the model that uses HGB outperforms the other models, while\n",
    "being much slower to train. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
