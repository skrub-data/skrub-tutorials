{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A world without skrub\"\n",
    "format:\n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        toc: true\n",
    "        code-fold: false\n",
    "        code-tools: true\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin the lesson by imagining a world without skrub, where we can use \n",
    "only Pandas and scikit-learn to clean data and prepare a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv(\"../data/employee_salaries/data.csv\")\n",
    "y = pd.read_csv(\"../data/employee_salaries/target.csv\")\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the target::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a numerical column, and our task is predicting the value of `current_annual_salary`.\n",
    "\n",
    "## Strategizing\n",
    "We can begin by exploring the dataframe with `.describe`, and then think of a \n",
    "plan for pre-processing our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to:\n",
    "\n",
    "- Impute some missing values in the `gender` column.\n",
    "- Encode convert categorical features into numerical features. \n",
    "- Convert the column `date_first_hired` into numerical features.\n",
    "\n",
    "Once we have processed the data, we can train a machine learning model. For the sake\n",
    "of the example, we will use a linear model (`Ridge`), which means that we need to\n",
    "scale numerical features and impute missing values. \n",
    "\n",
    "Finally, we want to evaluate the performance of the method across multiple \n",
    "cross-validation splits.\n",
    "\n",
    "## Building a traditional pipeline\n",
    "Let's build a traditional predictive pipeline following the steps we just discussed. \n",
    "\n",
    "### Step 1: Convert date features to numerical\n",
    "\n",
    "Extract numerical features from the `date_first_hired` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to work with\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Parse the date column\n",
    "X_processed['date_first_hired'] = pd.to_datetime(X_processed['date_first_hired'])\n",
    "\n",
    "# Extract numerical features from date\n",
    "X_processed['hired_month'] = X_processed['date_first_hired'].dt.month\n",
    "X_processed['hired_year'] = X_processed['date_first_hired'].dt.year\n",
    "\n",
    "# Drop original date column\n",
    "X_processed = X_processed.drop('date_first_hired', axis=1)\n",
    "\n",
    "print(\"Features after date transformation:\")\n",
    "print(\"\\nShape:\", X_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Encode categorical features\n",
    "\n",
    "Encode only the non-numerical categorical features using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify only the non-numerical (truly categorical) columns\n",
    "categorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical columns to encode:\", categorical_cols)\n",
    "\n",
    "# Apply one-hot encoding only to categorical columns\n",
    "X_encoded = pd.get_dummies(X_processed, columns=categorical_cols)\n",
    "print(\"\\nShape after encoding:\", X_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Impute missing values\n",
    "\n",
    "We'll impute missing values in the `gender` column using the most frequent strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values with most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_encoded_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_encoded),\n",
    "    columns=X_encoded.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Scale numerical features\n",
    "\n",
    "Scale numerical features for the Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X_encoded_imputed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_encoded_imputed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train Ridge model with cross-validation\n",
    "\n",
    "Train a Ridge regression model and evaluate with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Ridge model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Perform cross-validation (5-fold)\n",
    "cv_results = cross_validate(\n",
    "    ridge,\n",
    "    X_scaled,\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring=[\"r2\", \"neg_mean_squared_error\"],\n",
    ")\n",
    "\n",
    "# Convert MSE to RMSE\n",
    "test_rmse = np.sqrt(-cv_results[\"test_neg_mean_squared_error\"])\n",
    "\n",
    "# Display results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\n",
    "    f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\"\n",
    ")\n",
    "print(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Just ask an agent to write the code\"\n",
    "It's what I did. Here are some of the issues I noticed: \n",
    "\n",
    "- Operations in the wrong order.\n",
    "- Trying to impute categorical features without encoding them as numerical values.\n",
    "- The datetime feature was encoded as a categorical (i.e, with dummmies).\n",
    "- Cells could not be executed in order without proper debugging and re-prompting.\n",
    "- `pd.get_dummies` was executed on the full dataframe, rather than only on the \n",
    "training split, leading to data leakage. \n",
    "\n",
    "This means that I had to spend time re-prompting the model to get it to run, and \n",
    "that's (intentionally) without removing the leakage. \n",
    "\n",
    "## Waking up from a nightmare\n",
    "Thankfully, we live in a world where we can `import skrub`. Let's see what we can\n",
    "get if we use `skrub.tabular_pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import tabular_pipeline\n",
    "\n",
    "# Perform cross-validation (5-fold)\n",
    "cv_results = cross_validate(tabular_pipeline(\"regression\"), X, y, cv=5, \n",
    "                            scoring=['r2', 'neg_mean_squared_error'],\n",
    "                            return_train_score=True)\n",
    "\n",
    "# Convert MSE to RMSE\n",
    "train_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "\n",
    "# Display results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Mean test R²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std():.4f})\")\n",
    "print(f\"Mean test RMSE: {test_rmse.mean():.4f} (+/- {test_rmse.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code from before, the tokens and the debugging are replaced by a single \n",
    "import that gives better results.\n",
    "\n",
    "Throughout the tutorial, we will see how each step can be simplified, replaced, or\n",
    "improved using skrub features, going through the various features until we get to\n",
    "the `tabular_pipeline`. \n",
    "\n",
    "## Roadmap for the course\n",
    "We are going to build what could be a typicial pre-processing pipeline: \n",
    "\n",
    "1. We will explore the data to identify possible problems and figure out what needs\n",
    "to be cleaned.\n",
    "2. We will then sanitize the data to address some common problems. \n",
    "3. There will be an intermission on various skrub features that simplify.\n",
    "4. Then, we will show how to perform feature engineering using various skrub encoders.\n",
    "5. Finally, we will show how we can put everything together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/rcap/work/skrub-tutorials/.pixi/envs/doc/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
